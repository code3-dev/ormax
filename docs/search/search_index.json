{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ormax ORM: Modern Async Python ORM","text":"<p>Ormax is a lightweight, high-performance asynchronous Object-Relational Mapping (ORM) library for Python that provides an intuitive interface for database operations. Designed for modern Python applications, Ormax supports multiple database backends while maintaining a clean, Pythonic API with full async/await support.</p>"},{"location":"#why-ormax","title":"Why Ormax?","text":"<p>Ormax bridges the gap between simplicity and power in Python ORM solutions. Unlike heavier alternatives, Ormax offers:</p> <ul> <li>\u2705 True async support - Built from the ground up for asyncio</li> <li>\u2705 Multiple database compatibility - Works with SQLite, PostgreSQL, MySQL, MSSQL, Oracle, and Aurora</li> <li>\u2705 Minimal overhead - Optimized for performance with <code>__slots__</code> and efficient query building</li> <li>\u2705 Intuitive API - Chainable QuerySet interface familiar to Django ORM users</li> <li>\u2705 Production-ready - Includes transaction management, connection pooling, and robust error handling</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install ormax\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>import ormax\nfrom ormax.fields import CharField, IntegerField\n\n# Connect to database\ndb = ormax.Database(\"sqlite:///mydatabase.db\")\n\n# Define a model\nclass Book(ormax.Model):\n    title = CharField(max_length=200)\n    pages = IntegerField()\n\n# Register model and create tables\ndb.register_model(Book)\nawait db.create_tables()\n\n# Create a book\nbook = await Book.create(title=\"\u0634\u06cc\u0631 \u0648 \u062e\u0648\u0631\u0634\u06cc\u062f\", pages=350)\n\n# Query books\nall_books = await Book.objects().all()\nahmad_books = await Book.objects().filter(title__contains=\"\u0627\u062d\u0645\u062f\").all()\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#asyncawait-support","title":"\ud83d\ude80 Async/Await Support","text":"<p>Fully asynchronous design for high-performance applications with proper connection pooling.</p>"},{"location":"#multiple-database-support","title":"\ud83d\uddc4\ufe0f Multiple Database Support","text":"<p>Works seamlessly with: - SQLite - PostgreSQL - MySQL - Microsoft SQL Server - Oracle - Amazon Aurora</p>"},{"location":"#relationship-management","title":"\ud83d\udd17 Relationship Management","text":"<p>Complete support for ForeignKey relationships with reverse lookups and efficient prefetching.</p>"},{"location":"#transactions","title":"\ud83d\udcb0 Transactions","text":"<p>Nested transactions with savepoint support for complex business logic.</p>"},{"location":"#query-optimization","title":"\ud83d\udcca Query Optimization","text":"<p>Chainable QuerySet API with: - <code>select_related</code> and <code>prefetch_related</code> to avoid N+1 queries - <code>defer</code> and <code>only</code> for field optimization - Index hinting with <code>using_index</code></p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to dive in? Start with our comprehensive guides:</p> <ul> <li>Installation Guide</li> <li>Basic Model Definition</li> <li>Querying Data</li> <li>Relationships</li> <li>Transactions</li> </ul>"},{"location":"#why-developers-choose-ormax","title":"Why Developers Choose Ormax","text":"<p>\"Ormax provided the perfect balance between simplicity and power for our async API. The transition from raw SQL queries was seamless, and the performance improvements were immediate.\" - Sarah Chen, Senior Backend Engineer</p> <p>\"The ability to use the same ORM across multiple database backends without changing our code has been invaluable for our microservices architecture.\" - Michael Rodriguez, CTO</p>"},{"location":"#community-support","title":"Community &amp; Support","text":"<p>Join our growing community: - GitHub Repository - Stack Overflow Tag</p> <p>Having issues? open an issue.</p>"},{"location":"advanced/custom-models/","title":"Custom Model Patterns in Ormax","text":"<p>While basic models cover many use cases, Ormax provides powerful tools for creating custom model patterns that solve complex business requirements. This guide covers advanced techniques for extending and customizing Ormax models.</p>"},{"location":"advanced/custom-models/#custom-model-methods","title":"Custom Model Methods","text":"<p>Adding custom methods to your models is the simplest way to encapsulate business logic.</p>"},{"location":"advanced/custom-models/#instance-methods","title":"Instance Methods","text":"<pre><code>class Book(ormax.Model):\n    title = CharField(max_length=200)\n    pages = IntegerField()\n    published_date = DateTimeField(null=True)\n\n    def is_long(self) -&gt; bool:\n        \"\"\"Check if book is considered long\"\"\"\n        return self.pages &gt; 300\n\n    def days_since_publication(self) -&gt; int:\n        \"\"\"Calculate days since publication\"\"\"\n        if not self.published_date:\n            return 0\n        return (datetime.now() - self.published_date).days\n</code></pre>"},{"location":"advanced/custom-models/#class-methods","title":"Class Methods","text":"<pre><code>class Author(ormax.Model):\n    name = CharField(max_length=100)\n    bio = CharField(max_length=500, null=True)\n\n    @classmethod\n    async def get_top_authors(cls, min_books=5, limit=10):\n        \"\"\"Get authors with at least min_books published\"\"\"\n        return await cls.objects() \\\n            .annotate(book_count=\"COUNT(books)\") \\\n            .filter(book_count__gte=min_books) \\\n            .order_by(\"-book_count\") \\\n            .limit(limit) \\\n            .all()\n</code></pre>"},{"location":"advanced/custom-models/#async-methods","title":"Async Methods","text":"<pre><code>class Product(ormax.Model):\n    name = CharField(max_length=200)\n    price = DecimalField(max_digits=10, decimal_places=2)\n    stock = IntegerField(default=0)\n\n    async def check_availability(self, quantity: int) -&gt; bool:\n        \"\"\"Check if product is available in requested quantity\"\"\"\n        return self.stock &gt;= quantity\n\n    async def reserve_stock(self, quantity: int) -&gt; bool:\n        \"\"\"Reserve stock for an order\"\"\"\n        if await self.check_availability(quantity):\n            self.stock -= quantity\n            await self.save()\n            return True\n        return False\n</code></pre>"},{"location":"advanced/custom-models/#model-inheritance","title":"Model Inheritance","text":"<p>Ormax supports several inheritance patterns for models.</p>"},{"location":"advanced/custom-models/#abstract-base-classes","title":"Abstract Base Classes","text":"<p>Abstract models let you define common fields without creating a database table:</p> <pre><code>class TimestampedModel(ormax.Model):\n    created_at = DateTimeField(auto_now_add=True)\n    updated_at = DateTimeField(auto_now=True)\n\n    class Meta:\n        abstract = True\n\nclass Book(TimestampedModel):\n    title = CharField(max_length=200)\n    pages = IntegerField()\n\n    # Inherits created_at and updated_at fields\n</code></pre>"},{"location":"advanced/custom-models/#multi-table-inheritance","title":"Multi-table Inheritance","text":"<p>Create separate tables for parent and child models:</p> <pre><code>class Person(ormax.Model):\n    name = CharField(max_length=100)\n    email = EmailField()\n\nclass Author(Person):\n    bio = CharField(max_length=500, null=True)\n\n    class Meta:\n        # This enables multi-table inheritance\n        proxy = False\n\nclass Editor(Person):\n    department = CharField(max_length=100)\n\n    class Meta:\n        proxy = False\n</code></pre> <p>Note: Multi-table inheritance requires additional queries to retrieve all fields.</p>"},{"location":"advanced/custom-models/#proxy-models","title":"Proxy Models","text":"<p>Create different Python interfaces to the same database table:</p> <pre><code>class Book(ormax.Model):\n    title = CharField(max_length=200)\n    pages = IntegerField()\n    published_date = DateTimeField(null=True)\n\nclass PublishedBook(Book):\n    class Meta:\n        proxy = True\n\n    @classmethod\n    async def get_queryset(cls):\n        \"\"\"Only return published books\"\"\"\n        return super().get_queryset().filter(published_date__isnull=False)\n</code></pre>"},{"location":"advanced/custom-models/#custom-managers","title":"Custom Managers","text":"<p>Custom managers let you encapsulate query logic and provide model-specific query methods.</p>"},{"location":"advanced/custom-models/#basic-custom-manager","title":"Basic Custom Manager","text":"<pre><code>class PublishedBookManager(ormax.Manager):\n    def get_queryset(self):\n        \"\"\"Override to filter only published books\"\"\"\n        return super().get_queryset().filter(published_date__isnull=False)\n\n    async def recently_published(self, days=7):\n        \"\"\"Get books published in the last N days\"\"\"\n        from_date = datetime.now() - timedelta(days=days)\n        return await self.get_queryset().filter(published_date__gte=from_date).all()\n\nclass Book(ormax.Model):\n    title = CharField(max_length=200)\n    pages = IntegerField()\n    published_date = DateTimeField(null=True)\n\n    objects = ormax.Manager()  # Default manager\n    published = PublishedBookManager()  # Custom manager\n</code></pre>"},{"location":"advanced/custom-models/#usage","title":"Usage","text":"<pre><code># Get all books (including unpublished)\nall_books = await Book.objects().all()\n\n# Get only published books\npublished_books = await Book.published.all()\n\n# Get recently published books\nrecent_books = await Book.published.recently_published(days=14)\n</code></pre>"},{"location":"advanced/custom-models/#manager-with-relationship-support","title":"Manager with Relationship Support","text":"<p>Managers can also handle relationship operations:</p> <pre><code>class BookManager(ormax.Manager):\n    async def create_with_author(self, title, pages, author_name):\n        \"\"\"Create a book with a new or existing author\"\"\"\n        # Check if author exists\n        try:\n            author = await Author.objects().get(name=author_name)\n        except Author.DoesNotExist:\n            author = await Author.create(name=author_name)\n\n        # Create the book\n        return await self.model.create(\n            title=title,\n            pages=pages,\n            author=author\n        )\n\nclass Book(ormax.Model):\n    title = CharField(max_length=200)\n    pages = IntegerField()\n    author = ForeignKeyField(Author, related_name=\"books\")\n\n    objects = BookManager()\n</code></pre>"},{"location":"advanced/custom-models/#advanced-field-customization","title":"Advanced Field Customization","text":""},{"location":"advanced/custom-models/#custom-field-types","title":"Custom Field Types","text":"<p>Create your own field types by subclassing <code>Field</code>:</p> <pre><code>class PercentField(ormax.fields.FloatField):\n    def __init__(self, **kwargs):\n        # Ensure min/max constraints\n        kwargs.setdefault(\"min_value\", 0.0)\n        kwargs.setdefault(\"max_value\", 100.0)\n        super().__init__(**kwargs)\n\n    def validate(self, value):\n        value = super().validate(value)\n        if value is not None and (value &lt; 0 or value &gt; 100):\n            raise ValidationError(\"Percent value must be between 0 and 100\")\n        return value\n\nclass Product(ormax.Model):\n    name = CharField(max_length=200)\n    discount = PercentField()\n</code></pre>"},{"location":"advanced/custom-models/#field-with-custom-conversion","title":"Field with Custom Conversion","text":"<pre><code>class CommaSeparatedListField(ormax.fields.TextField):\n    def to_python(self, value):\n        if isinstance(value, str):\n            return [item.strip() for item in value.split(\",\")]\n        return value\n\n    def to_database(self, value):\n        if isinstance(value, list):\n            return \", \".join(str(item) for item in value)\n        return value\n\nclass Tag(ormax.Model):\n    name = CharField(max_length=50)\n\nclass Article(ormax.Model):\n    title = CharField(max_length=200)\n    tags = CommaSeparatedListField()\n</code></pre>"},{"location":"advanced/custom-models/#encrypted-fields","title":"Encrypted Fields","text":"<pre><code>from ormax.utils import hash_password, verify_password\n\nclass EncryptedCharField(ormax.fields.CharField):\n    def __init__(self, **kwargs):\n        self.salt = kwargs.pop(\"salt\", None) or ormax.utils.generate_secure_token(16)\n        super().__init__(**kwargs)\n\n    def to_database(self, value):\n        if value is not None:\n            hashed, _ = hash_password(value, self.salt)\n            return hashed\n        return None\n\n    def to_python(self, value):\n        # We can't decrypt, so we just return the hash\n        return value\n\nclass User(ormax.Model):\n    username = CharField(max_length=50)\n    password = EncryptedCharField(max_length=100)\n</code></pre>"},{"location":"advanced/custom-models/#model-validation","title":"Model Validation","text":"<p>Ormax provides multiple ways to validate model data.</p>"},{"location":"advanced/custom-models/#field-level-validation","title":"Field-level Validation","text":"<p>Already built into field types (handled by <code>Field.validate()</code>).</p>"},{"location":"advanced/custom-models/#model-level-validation","title":"Model-level Validation","text":"<p>Override <code>clean()</code> for model-level validation:</p> <pre><code>class Book(ormax.Model):\n    title = CharField(max_length=200)\n    pages = IntegerField()\n    published_date = DateTimeField(null=True)\n\n    async def clean(self):\n        \"\"\"Validate model-level constraints\"\"\"\n        await super().clean()\n\n        # Ensure published books have a publication date\n        if self.published_date is None and self.pages &gt; 0:\n            raise ValidationError(\"Published books must have a publication date\")\n\n        # Ensure page count is reasonable\n        if self.pages &gt; 10000:\n            raise ValidationError(\"Page count seems unrealistically high\")\n</code></pre>"},{"location":"advanced/custom-models/#validation-on-save","title":"Validation on Save","text":"<p>Override <code>save()</code> for validation that requires database access:</p> <pre><code>class Book(ormax.Model):\n    # fields...\n\n    async def save(self, *args, **kwargs):\n        # Check for duplicate titles\n        if await Book.objects().filter(title=self.title).exclude(id=self.id).exists():\n            raise ValidationError(\"Book with this title already exists\")\n\n        await super().save(*args, **kwargs)\n</code></pre>"},{"location":"advanced/custom-models/#signals-and-events","title":"Signals and Events","text":"<p>Ormax supports signals for reacting to model events.</p>"},{"location":"advanced/custom-models/#built-in-signals","title":"Built-in Signals","text":"<p>Ormax provides signals for common events:</p> <pre><code>from ormax.signals import pre_save, post_save, pre_delete, post_delete\n\n# Connect to pre_save signal\n@pre_save.connect\nasync def book_pre_save(sender, instance, **kwargs):\n    \"\"\"Called before a Book is saved\"\"\"\n    if isinstance(instance, Book) and not instance.published_date and instance.pages &gt; 0:\n        instance.published_date = datetime.now()\n\n# Connect to post_save signal\n@post_save.connect\nasync def book_post_save(sender, instance, created, **kwargs):\n    \"\"\"Called after a Book is saved\"\"\"\n    if isinstance(instance, Book) and created:\n        # Send notification for new books\n        await send_new_book_notification(instance)\n</code></pre>"},{"location":"advanced/custom-models/#custom-signals","title":"Custom Signals","text":"<p>Create your own signals for domain-specific events:</p> <pre><code>from ormax.signals import Signal\n\n# Define a custom signal\nbook_published = Signal(providing_args=[\"instance\"])\n\nclass Book(ormax.Model):\n    # fields...\n\n    async def publish(self):\n        \"\"\"Publish the book\"\"\"\n        self.published_date = datetime.now()\n        await self.save()\n        # Send the signal\n        await book_published.send(sender=self.__class__, instance=self)\n\n# Connect to the custom signal\n@book_published.connect\nasync def handle_book_published(sender, instance, **kwargs):\n    \"\"\"Handle book publication\"\"\"\n    # Send email to subscribers\n    await notify_subscribers(instance)\n    # Update search index\n    await update_search_index(instance)\n</code></pre>"},{"location":"advanced/custom-models/#custom-querysets","title":"Custom QuerySets","text":"<p>Create specialized QuerySet classes for complex query patterns.</p>"},{"location":"advanced/custom-models/#basic-custom-queryset","title":"Basic Custom QuerySet","text":"<pre><code>class BookQuerySet(ormax.QuerySet):\n    def published(self):\n        \"\"\"Filter for published books\"\"\"\n        return self.filter(published_date__isnull=False)\n\n    def recent(self, days=7):\n        \"\"\"Filter for recently published books\"\"\"\n        from_date = datetime.now() - timedelta(days=days)\n        return self.published().filter(published_date__gte=from_date)\n\n    async def top_rated(self, limit=10):\n        \"\"\"Get top rated books\"\"\"\n        return await self.annotate(\n            avg_rating=\"AVG(reviews__rating)\"\n        ).order_by(\"-avg_rating\").limit(limit).all()\n\nclass Book(ormax.Model):\n    # fields...\n\n    objects = BookQuerySet.as_manager()\n</code></pre>"},{"location":"advanced/custom-models/#usage_1","title":"Usage","text":"<pre><code># Get recently published books\nrecent_books = await Book.objects().recent(days=14).all()\n\n# Get top rated books\ntop_books = await Book.objects().top_rated(limit=5)\n</code></pre>"},{"location":"advanced/custom-models/#queryset-with-relationship-support","title":"QuerySet with Relationship Support","text":"<pre><code>class AuthorQuerySet(ormax.QuerySet):\n    def with_book_count(self):\n        \"\"\"Annotate authors with their book count\"\"\"\n        return self.annotate(book_count=\"COUNT(books)\")\n\n    def prolific_authors(self, min_books=5):\n        \"\"\"Filter for authors with at least min_books\"\"\"\n        return self.with_book_count().filter(book_count__gte=min_books)\n\n    async def get_with_books(self, author_id):\n        \"\"\"Get author with their books in one query\"\"\"\n        author = await self.filter(id=author_id).select_related(\"books\").first()\n        if not author:\n            raise Author.DoesNotExist()\n        return author\n\nclass Author(ormax.Model):\n    # fields...\n\n    objects = AuthorQuerySet.as_manager()\n</code></pre>"},{"location":"advanced/custom-models/#advanced-model-patterns","title":"Advanced Model Patterns","text":""},{"location":"advanced/custom-models/#soft-delete-pattern","title":"Soft Delete Pattern","text":"<p>Implement soft delete instead of actual deletion:</p> <pre><code>class SoftDeleteModel(ormax.Model):\n    deleted_at = DateTimeField(null=True)\n\n    class Meta:\n        abstract = True\n\n    async def delete(self, hard=False):\n        \"\"\"Soft delete by default, hard delete with hard=True\"\"\"\n        if hard:\n            await super().delete()\n        else:\n            self.deleted_at = datetime.now()\n            await self.save()\n\n    @classmethod\n    def objects(cls):\n        \"\"\"Override to exclude soft-deleted objects\"\"\"\n        return super().objects().filter(deleted_at__isnull=True)\n\nclass Book(SoftDeleteModel):\n    title = CharField(max_length=200)\n    pages = IntegerField()\n\n    # All queries will automatically exclude soft-deleted books\n</code></pre>"},{"location":"advanced/custom-models/#versioned-models","title":"Versioned Models","text":"<p>Track changes to model instances:</p> <pre><code>class VersionedModel(ormax.Model):\n    current_version = IntegerField(default=1)\n\n    class Meta:\n        abstract = True\n\n    async def save(self, *args, **kwargs):\n        \"\"\"Create a new version on save\"\"\"\n        is_new = self._is_new\n        if not is_new:\n            # Create version before updating\n            await self.create_version()\n        await super().save(*args, **kwargs)\n\n    async def create_version(self):\n        \"\"\"Create a new version of the model\"\"\"\n        version_data = self.to_dict()\n        version_data.pop(\"id\", None)  # Remove ID for new version\n        version_data[\"original_id\"] = self.id\n        version_data[\"version\"] = self.current_version + 1\n\n        # Create version record\n        await VersionedBookVersion.create(**version_data)\n\n        # Increment version counter\n        self.current_version += 1\n\nclass BookVersion(ormax.Model):\n    original_id = IntegerField()\n    version = IntegerField()\n    title = CharField(max_length=200)\n    pages = IntegerField()\n    created_at = DateTimeField(auto_now_add=True)\n\nclass Book(VersionedModel):\n    title = CharField(max_length=200)\n    pages = IntegerField()\n</code></pre>"},{"location":"advanced/custom-models/#tree-structures","title":"Tree Structures","text":"<p>Model hierarchical data with self-referential relationships:</p> <pre><code>class Category(ormax.Model):\n    name = CharField(max_length=100)\n    parent = ForeignKeyField(\"Category\", related_name=\"children\", null=True)\n\n    async def get_ancestors(self, include_self=False):\n        \"\"\"Get all ancestors of this category\"\"\"\n        ancestors = []\n        current = self if include_self else await self.parent\n        while current:\n            ancestors.append(current)\n            current = await current.parent\n        return ancestors\n\n    async def get_descendants(self, include_self=False):\n        \"\"\"Get all descendants of this category (breadth-first)\"\"\"\n        descendants = []\n        queue = [self] if include_self else await self.children.all()\n\n        while queue:\n            node = queue.pop(0)\n            descendants.append(node)\n            children = await node.children.all()\n            queue.extend(children)\n\n        return descendants\n\n    async def move_to(self, new_parent=None):\n        \"\"\"Move category to a new parent\"\"\"\n        # Prevent circular references\n        if new_parent == self or self in await new_parent.get_descendants():\n            raise ValueError(\"Cannot move category to itself or its descendants\")\n\n        self.parent = new_parent\n        await self.save()\n</code></pre>"},{"location":"advanced/custom-models/#performance-considerations-for-custom-models","title":"Performance Considerations for Custom Models","text":""},{"location":"advanced/custom-models/#avoid-heavy-operations-in-init","title":"Avoid Heavy Operations in init","text":"<pre><code># BAD: Heavy operation in __init__\nclass Book(ormax.Model):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        # This will run for EVERY instance, even in QuerySet\n        self.related_books = self.get_related_books()  # Expensive operation\n\n# GOOD: Use a method or property instead\nclass Book(ormax.Model):\n    @property\n    async def related_books(self):\n        if not hasattr(self, \"_related_books\"):\n            self._related_books = await self.get_related_books()\n        return self._related_books\n</code></pre>"},{"location":"advanced/custom-models/#use-caching-for-expensive-methods","title":"Use Caching for Expensive Methods","text":"<pre><code>from ormax.utils import cached_property, memoize_async\n\nclass Book(ormax.Model):\n    # fields...\n\n    @cached_property\n    def normalized_title(self):\n        \"\"\"Cached property - computed once per instance\"\"\"\n        return self.title.lower().replace(\" \", \"-\")\n\n    @memoize_async(maxsize=100)\n    async def get_recommendations(self, limit=5):\n        \"\"\"Cached async method - computed once per argument combination\"\"\"\n        # Expensive recommendation algorithm\n        return await calculate_recommendations(self, limit)\n</code></pre>"},{"location":"advanced/custom-models/#optimize-relationship-loading","title":"Optimize Relationship Loading","text":"<pre><code>class Author(ormax.Model):\n    # fields...\n\n    async def get_books_with_reviews(self):\n        \"\"\"Optimized method to get books with reviews in minimal queries\"\"\"\n        # Get author's books in one query\n        books = await Book.objects().filter(author=self).select_related(\"reviews\").all()\n\n        # Process books and reviews\n        result = []\n        for book in books:\n            # Reviews are already loaded due to select_related\n            book_reviews = await book.reviews.all()\n            result.append({\n                \"book\": book,\n                \"review_count\": len(book_reviews),\n                \"average_rating\": sum(r.rating for r in book_reviews) / len(book_reviews) if book_reviews else 0\n            })\n\n        return result\n</code></pre>"},{"location":"advanced/custom-models/#real-world-examples","title":"Real-World Examples","text":""},{"location":"advanced/custom-models/#e-commerce-product-model","title":"E-commerce Product Model","text":"<pre><code>class Product(ormax.Model):\n    name = CharField(max_length=200)\n    description = TextField(null=True)\n    price = DecimalField(max_digits=10, decimal_places=2)\n    stock = IntegerField(default=0)\n    sku = CharField(max_length=50, unique=True)\n    category = ForeignKeyField(\"Category\", related_name=\"products\")\n    created_at = DateTimeField(auto_now_add=True)\n    updated_at = DateTimeField(auto_now=True)\n    is_active = BooleanField(default=True)\n\n    class Meta:\n        indexes = [\n            {\"fields\": [\"name\"], \"type\": \"FULLTEXT\"}\n        ]\n\n    def get_formatted_price(self) -&gt; str:\n        \"\"\"Format price for display\"\"\"\n        return f\"${self.price:.2f}\"\n\n    async def is_in_stock(self, quantity=1) -&gt; bool:\n        \"\"\"Check if product is in stock\"\"\"\n        return self.stock &gt;= quantity\n\n    async def reduce_stock(self, quantity: int) -&gt; bool:\n        \"\"\"Reduce stock after purchase\"\"\"\n        if not await self.is_in_stock(quantity):\n            return False\n        self.stock -= quantity\n        await self.save()\n        return True\n\n    async def get_similar_products(self, limit=5):\n        \"\"\"Get similar products based on category\"\"\"\n        return await Product.objects() \\\n            .filter(category=self.category, is_active=True) \\\n            .exclude(id=self.id) \\\n            .order_by(\"-created_at\") \\\n            .limit(limit) \\\n            .all()\n\nclass ProductQuerySet(ormax.QuerySet):\n    def active(self):\n        \"\"\"Filter for active products\"\"\"\n        return self.filter(is_active=True)\n\n    def in_stock(self):\n        \"\"\"Filter for products in stock\"\"\"\n        return self.active().filter(stock__gt=0)\n\n    async def search(self, query):\n        \"\"\"Search products by name and description\"\"\"\n        if \"postgresql\" in self._database.connection_string:\n            # Use PostgreSQL full-text search\n            return await self.filter(\n                name__icontains=query\n            ) | await self.filter(description__icontains=query)\n        else:\n            # Fallback for other databases\n            return await self.filter(\n                name__icontains=query\n            ).filter(description__icontains=query)\n\nclass ProductManager(ormax.Manager):\n    def get_queryset(self):\n        return ProductQuerySet(self.model)\n\nProduct.objects = ProductManager()\n</code></pre>"},{"location":"advanced/custom-models/#blog-system-with-nested-comments","title":"Blog System with Nested Comments","text":"<pre><code>class User(ormax.Model):\n    username = CharField(max_length=50, unique=True)\n    email = EmailField(unique=True)\n    joined_at = DateTimeField(auto_now_add=True)\n\nclass Post(ormax.Model):\n    title = CharField(max_length=200)\n    content = TextField()\n    author = ForeignKeyField(User, related_name=\"posts\")\n    created_at = DateTimeField(auto_now_add=True)\n    updated_at = DateTimeField(auto_now=True)\n    is_published = BooleanField(default=False)\n\n    def excerpt(self, length=100) -&gt; str:\n        \"\"\"Get a shortened excerpt of the post\"\"\"\n        return self.content[:length] + \"...\" if len(self.content) &gt; length else self.content\n\n    async def get_comment_count(self) -&gt; int:\n        \"\"\"Get total comment count including nested comments\"\"\"\n        return await Comment.objects().filter(post=self).count()\n\n    async def get_top_level_comments(self):\n        \"\"\"Get top-level comments (no parent)\"\"\"\n        return await Comment.objects().filter(post=self, parent=None).all()\n\nclass CommentQuerySet(ormax.QuerySet):\n    def for_post(self, post_id):\n        \"\"\"Filter comments for a specific post\"\"\"\n        return self.filter(post_id=post_id)\n\n    def top_level(self):\n        \"\"\"Filter for top-level comments\"\"\"\n        return self.filter(parent=None)\n\n    async def with_nested_replies(self, comment_id):\n        \"\"\"Get a comment with all nested replies\"\"\"\n        comment = await self.filter(id=comment_id).first()\n        if not comment:\n            return None\n        comment.replies = await self.filter(parent_id=comment_id).all()\n        for reply in comment.replies:\n            reply.replies = await self.with_nested_replies(reply.id)\n        return comment\n\nclass Comment(ormax.Model):\n    post = ForeignKeyField(Post, related_name=\"comments\")\n    author = ForeignKeyField(User, related_name=\"comments\")\n    content = TextField()\n    parent = ForeignKeyField(\"Comment\", related_name=\"replies\", null=True)\n    created_at = DateTimeField(auto_now_add=True)\n\n    objects = CommentQuerySet.as_manager()\n\n    async def get_ancestors(self):\n        \"\"\"Get all ancestor comments\"\"\"\n        ancestors = []\n        current = await self.parent\n        while current:\n            ancestors.append(current)\n            current = await current.parent\n        return ancestors\n\n    async def get_descendants(self):\n        \"\"\"Get all descendant comments (breadth-first)\"\"\"\n        descendants = []\n        queue = await self.replies.all()\n\n        while queue:\n            comment = queue.pop(0)\n            descendants.append(comment)\n            replies = await comment.replies.all()\n            queue.extend(replies)\n\n        return descendants\n</code></pre>"},{"location":"advanced/custom-models/#best-practices-for-custom-models","title":"Best Practices for Custom Models","text":""},{"location":"advanced/custom-models/#keep-business-logic-in-models","title":"Keep Business Logic in Models","text":"<p>Models are the right place for business logic related to the data:</p> <pre><code># GOOD: Business logic in model\nclass Order(ormax.Model):\n    # fields...\n\n    async def calculate_total(self) -&gt; Decimal:\n        \"\"\"Calculate order total with tax and discounts\"\"\"\n        # Business logic here\n        return total\n\n# BAD: Business logic outside model\ndef calculate_order_total(order):\n    # Business logic here\n    return total\n</code></pre>"},{"location":"advanced/custom-models/#use-properties-for-simple-calculations","title":"Use Properties for Simple Calculations","text":"<pre><code>class Book(ormax.Model):\n    pages = IntegerField()\n    price = DecimalField(max_digits=10, decimal_places=2)\n\n    @property\n    def price_per_page(self) -&gt; Decimal:\n        \"\"\"Calculate price per page\"\"\"\n        return self.price / self.pages if self.pages &gt; 0 else Decimal(0)\n</code></pre>"},{"location":"advanced/custom-models/#use-async-methods-for-database-operations","title":"Use Async Methods for Database Operations","text":"<pre><code>class User(ormax.Model):\n    # fields...\n\n    # GOOD: Async method for database operations\n    async def get_purchased_books(self):\n        return await Book.objects().filter(purchase__user=self).all()\n\n    # BAD: Synchronous method that hides async operations\n    def get_purchased_books(self):\n        # This would require some complex async-to-sync conversion\n        # and is generally a bad practice\n        pass\n</code></pre>"},{"location":"advanced/custom-models/#document-custom-methods","title":"Document Custom Methods","text":"<pre><code>class Product(ormax.Model):\n    # fields...\n\n    async def is_low_stock(self, threshold=10) -&gt; bool:\n        \"\"\"\n        Check if product is low in stock.\n\n        Args:\n            threshold: The stock level considered \"low\" (default: 10)\n\n        Returns:\n            bool: True if stock is below threshold, False otherwise\n        \"\"\"\n        return self.stock &lt; threshold\n</code></pre>"},{"location":"advanced/custom-models/#testing-custom-models","title":"Testing Custom Models","text":""},{"location":"advanced/custom-models/#unit-testing-model-methods","title":"Unit Testing Model Methods","text":"<pre><code>import pytest\nfrom unittest.mock import patch\n\n@pytest.mark.asyncio\nasync def test_book_is_long():\n    # Create a book with 400 pages\n    book = Book(title=\"Long Book\", pages=400)\n    assert book.is_long() is True\n\n    # Create a book with 200 pages\n    book = Book(title=\"Short Book\", pages=200)\n    assert book.is_long() is False\n\n@pytest.mark.asyncio\nasync def test_product_reserve_stock():\n    # Setup\n    product = await Product.create(name=\"Test Product\", price=10.0, stock=10)\n\n    # Test successful reservation\n    result = await product.reserve_stock(5)\n    assert result is True\n    assert product.stock == 5\n\n    # Test failed reservation\n    result = await product.reserve_stock(10)\n    assert result is False\n    assert product.stock == 5  # No change\n</code></pre>"},{"location":"advanced/custom-models/#testing-custom-managers","title":"Testing Custom Managers","text":"<pre><code>@pytest.mark.asyncio\nasync def test_published_book_manager():\n    # Create published and unpublished books\n    await Book.create(title=\"Published Book\", pages=200, published_date=datetime.now())\n    await Book.create(title=\"Unpublished Book\", pages=100, published_date=None)\n\n    # Test published manager\n    published_books = await Book.published.all()\n    assert len(published_books) == 1\n    assert published_books[0].title == \"Published Book\"\n\n    # Test recently published\n    recent_books = await Book.published.recently_published(days=1)\n    assert len(recent_books) == 1\n</code></pre>"},{"location":"advanced/custom-models/#testing-signals","title":"Testing Signals","text":"<pre><code>@pytest.mark.asyncio\nasync def test_book_published_signal():\n    # Create a mock signal handler\n    handler_called = False\n    result_instance = None\n\n    async def handler(sender, instance, **kwargs):\n        nonlocal handler_called, result_instance\n        handler_called = True\n        result_instance = instance\n\n    # Connect the handler\n    book_published.connect(handler)\n\n    # Create a book (should trigger the signal)\n    book = await Book.create(title=\"Signal Test\", pages=100)\n    await book.publish()\n\n    # Check if handler was called\n    assert handler_called is True\n    assert result_instance.id == book.id\n\n    # Disconnect the handler\n    book_published.disconnect(handler)\n</code></pre>"},{"location":"advanced/custom-models/#troubleshooting-custom-models","title":"Troubleshooting Custom Models","text":""},{"location":"advanced/custom-models/#method-not-found","title":"Method Not Found","text":"<p>Issue: <code>AttributeError: 'Book' object has no attribute 'is_long'</code></p> <p>Solution: - Verify the method is defined in the model class - Check for typos in the method name - Ensure you're using the correct model instance</p>"},{"location":"advanced/custom-models/#unexpected-query-behavior","title":"Unexpected Query Behavior","text":"<p>Issue: Custom QuerySet methods not working as expected</p> <p>Solution: - Verify the QuerySet methods are returning new QuerySet instances - Check if you're properly chaining methods - Use <code>print(queryset._build_select_query())</code> to see the generated SQL</p>"},{"location":"advanced/custom-models/#signal-not-firing","title":"Signal Not Firing","text":"<p>Issue: Signal handlers not being called</p> <p>Solution: - Verify the handler is properly connected to the signal - Check if the signal is being sent from the correct location - Ensure async handlers are awaited properly</p>"},{"location":"advanced/custom-models/#performance-issues-with-custom-methods","title":"Performance Issues with Custom Methods","text":"<p>Issue: Slow performance with custom model methods</p> <p>Solution: - Avoid database operations in <code>__init__</code> or properties - Use <code>select_related</code> and <code>prefetch_related</code> to optimize relationship access - Add appropriate indexes to support your query patterns</p>"},{"location":"advanced/debugging/","title":"Debugging Techniques for Ormax Applications","text":"<p>Debugging database-driven applications requires specialized techniques to understand what's happening under the hood. This guide covers comprehensive debugging strategies specifically designed for Ormax-based applications.</p>"},{"location":"advanced/debugging/#understanding-ormax-error-types","title":"Understanding Ormax Error Types","text":"<p>Before debugging, it's important to understand the different error types Ormax can raise:</p>"},{"location":"advanced/debugging/#common-error-categories","title":"Common Error Categories","text":"Error Type When It Occurs Typical Causes <code>DatabaseError</code> General database issues Connection problems, syntax errors <code>ValidationError</code> Data validation fails Invalid field values, constraint violations <code>DoesNotExist</code> Record not found <code>get()</code> called with no matching records <code>MultipleObjectsReturned</code> Multiple records found <code>get()</code> called with multiple matches <code>IntegrityError</code> Database integrity violation Foreign key constraints, unique constraints <code>TransactionRollbackError</code> Transaction failure Deadlocks, serialization failures"},{"location":"advanced/debugging/#decoding-error-messages","title":"Decoding Error Messages","text":"<p>Ormax error messages contain valuable information:</p> <pre><code>DatabaseError: (sqlite3.IntegrityError) UNIQUE constraint failed: books.title\n[SQL: INSERT INTO books (title, pages) VALUES (?, ?)]\n[parameters: ('Duplicate Title', 200)]\n</code></pre> <p>This tells us: - The error type (<code>DatabaseError</code>) - The underlying database error (<code>sqlite3.IntegrityError</code>) - The specific constraint that failed (<code>UNIQUE constraint failed: books.title</code>) - The SQL query that caused the error - The parameters used in the query</p>"},{"location":"advanced/debugging/#debugging-query-issues","title":"Debugging Query Issues","text":""},{"location":"advanced/debugging/#viewing-executed-queries","title":"Viewing Executed Queries","text":"<p>The most fundamental debugging technique is seeing what SQL is being executed:</p>"},{"location":"advanced/debugging/#method-1-enable-debug-logging","title":"Method 1: Enable Debug Logging","text":"<pre><code>import logging\n\n# Configure logging to show queries\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(\"ormax\")\nlogger.setLevel(logging.DEBUG)\n</code></pre> <p>This will log all queries to the console:</p> <pre><code>DEBUG:ormax:Executing query: SELECT * FROM books WHERE pages &gt; ?\nDEBUG:ormax:Parameters: (300,)\n</code></pre>"},{"location":"advanced/debugging/#method-2-using-the-explain-method","title":"Method 2: Using the <code>explain()</code> Method","text":"<p>Get database-specific execution plans:</p> <pre><code># Get query execution plan\nplan = await Book.objects().filter(pages__gt=300).explain()\nprint(plan)\n\n# For more detailed analysis (PostgreSQL)\ndetailed_plan = await Book.objects().filter(pages__gt=300).explain(analyze=True)\n</code></pre>"},{"location":"advanced/debugging/#method-3-manual-query-inspection","title":"Method 3: Manual Query Inspection","text":"<pre><code># Get the raw SQL query\nquery, params = Book.objects().filter(pages__gt=300)._build_select_query()\nprint(f\"SQL: {query}\")\nprint(f\"Parameters: {params}\")\n</code></pre>"},{"location":"advanced/debugging/#common-query-issues-and-solutions","title":"Common Query Issues and Solutions","text":""},{"location":"advanced/debugging/#issue-incorrect-where-clause","title":"Issue: Incorrect WHERE Clause","text":"<p>Symptoms: - Unexpected results - Missing records - Extra records</p> <p>Diagnosis: - Check the generated WHERE clause - Verify parameter values</p> <p>Solution: <pre><code># Check the query\nqs = Book.objects().filter(title__contains=\"sun\", pages__gt=300)\nprint(qs._build_select_query())\n\n# Fix by using correct field lookups\nqs = Book.objects().filter(title__icontains=\"sun\", pages__gt=300)\n</code></pre></p>"},{"location":"advanced/debugging/#issue-n1-query-problem","title":"Issue: N+1 Query Problem","text":"<p>Symptoms: - Many similar queries with different parameters - Performance degrades with larger datasets</p> <p>Diagnosis: - Look for repeated queries in logs - Check if relationship fields are being accessed in loops</p> <p>Solution: <pre><code># BAD: N+1 queries\nbooks = await Book.objects().all()\nfor book in books:\n    print(book.author.name)  # Triggers a query per book\n\n# GOOD: Use select_related\nbooks = await Book.objects().select_related(\"author\").all()\nfor book in books:\n    print(book.author.name)  # No additional queries\n</code></pre></p>"},{"location":"advanced/debugging/#issue-incorrect-joins","title":"Issue: Incorrect JOINs","text":"<p>Symptoms: - Missing related data - Duplicate records - Incorrect results with <code>prefetch_related</code></p> <p>Diagnosis: - Check the generated JOIN clauses - Verify relationship definitions</p> <p>Solution: <pre><code># Check the query\nqs = Book.objects().select_related(\"author\")\nprint(qs._build_select_query())\n\n# Verify relationship definition\nclass Book(Model):\n    author = ForeignKeyField(Author, related_name=\"books\")\n</code></pre></p>"},{"location":"advanced/debugging/#debugging-connection-issues","title":"Debugging Connection Issues","text":""},{"location":"advanced/debugging/#diagnosing-connection-problems","title":"Diagnosing Connection Problems","text":""},{"location":"advanced/debugging/#step-1-verify-connection-string","title":"Step 1: Verify Connection String","text":"<pre><code># Print connection string (with passwords masked)\ndef mask_password(url):\n    return re.sub(r':(.*?)(@|/)', r':***\\2', url)\n\nprint(f\"Connection URL: {mask_password(db.connection_string)}\")\n</code></pre>"},{"location":"advanced/debugging/#step-2-test-basic-connection","title":"Step 2: Test Basic Connection","text":"<pre><code>async def test_connection():\n    try:\n        await db.connect()\n        print(\"Connection successful\")\n        # Test a simple query\n        await db.connection.execute(\"SELECT 1\")\n        print(\"Simple query successful\")\n    except Exception as e:\n        print(f\"Connection failed: {e}\")\n</code></pre>"},{"location":"advanced/debugging/#step-3-check-connection-pool-status","title":"Step 3: Check Connection Pool Status","text":"<pre><code>def print_connection_pool_status():\n    \"\"\"Print current connection pool status\"\"\"\n    if hasattr(db, 'connection') and hasattr(db.connection, 'pool'):\n        if hasattr(db.connection.pool, 'size'):\n            print(\n                f\"Connection pool: {db.connection.pool.size()} active, \"\n                f\"{db.connection.pool.freesize()} free, \"\n                f\"{db.connection.pool.size() - db.connection.pool.freesize()} used\"\n            )\n        elif hasattr(db.connection.pool, '_maxsize'):\n            print(\n                f\"Connection pool: max={db.connection.pool._maxsize}, \"\n                f\"current={len(db.connection.pool._queue._get_heap())}\"\n            )\n</code></pre>"},{"location":"advanced/debugging/#common-connection-issues-and-solutions","title":"Common Connection Issues and Solutions","text":""},{"location":"advanced/debugging/#issue-connection-timeout","title":"Issue: Connection Timeout","text":"<p>Symptoms: - <code>OperationalError: Connection timeout</code> - Application hangs during database operations</p> <p>Solutions: - Increase connection timeout: <code>Database(url, connect_timeout=15.0)</code> - Check network connectivity - Verify database server is running - Reduce application load</p>"},{"location":"advanced/debugging/#issue-too-many-connections","title":"Issue: Too Many Connections","text":"<p>Symptoms: - <code>OperationalError: Too many connections</code> - Application fails to connect during peak times</p> <p>Solutions: - Increase database connection limit - Optimize connection pool settings:   <pre><code>db = Database(url, min_size=5, max_size=25)\n</code></pre> - Ensure connections are properly closed - Implement connection recycling</p>"},{"location":"advanced/debugging/#issue-authentication-failure","title":"Issue: Authentication Failure","text":"<p>Symptoms: - <code>OperationalError: Authentication failed</code> - <code>Access denied for user</code></p> <p>Solutions: - Verify username and password - Check database user permissions - Ensure correct database name - Test credentials with database client</p>"},{"location":"advanced/debugging/#debugging-relationship-issues","title":"Debugging Relationship Issues","text":""},{"location":"advanced/debugging/#diagnosing-relationship-problems","title":"Diagnosing Relationship Problems","text":""},{"location":"advanced/debugging/#step-1-verify-relationship-definitions","title":"Step 1: Verify Relationship Definitions","text":"<pre><code># Check forward relationship\nprint(Book._fields['author'])  # Should be a ForeignKeyField\n\n# Check reverse relationship\nprint(Author._relationships.get_reverse_manager('books'))\n</code></pre>"},{"location":"advanced/debugging/#step-2-inspect-relationship-data","title":"Step 2: Inspect Relationship Data","text":"<pre><code># Get book with author\nbook = await Book.objects().get(id=1)\n\n# Check raw foreign key value\nprint(f\"Author ID: {book.author_id}\")\n\n# Attempt to access related object\ntry:\n    author = await book.author\n    print(f\"Author name: {author.name}\")\nexcept Exception as e:\n    print(f\"Error accessing author: {e}\")\n</code></pre>"},{"location":"advanced/debugging/#common-relationship-issues-and-solutions","title":"Common Relationship Issues and Solutions","text":""},{"location":"advanced/debugging/#issue-foreign-key-constraint-violation","title":"Issue: Foreign Key Constraint Violation","text":"<p>Symptoms: - <code>IntegrityError: FOREIGN KEY constraint failed</code> - Cannot save model with relationship</p> <p>Diagnosis: - Check if related object exists - Verify primary key values - Check database schema</p> <p>Solution: <pre><code># Verify related object exists\nauthor = await Author.get(id=author_id)\nif not author:\n    print(f\"Author with ID {author_id} does not exist\")\n\n# Ensure related object is saved before assignment\nnew_author = Author(name=\"New Author\")\nawait new_author.save()  # Save first\nbook.author = new_author  # Then assign\n</code></pre></p>"},{"location":"advanced/debugging/#issue-accessing-relationship-returns-none","title":"Issue: Accessing Relationship Returns None","text":"<p>Symptoms: - <code>await book.author</code> returns None when it shouldn't - Relationship appears broken</p> <p>Diagnosis: - Check if foreign key field is None - Verify relationship was properly assigned</p> <p>Solution: <pre><code># Check foreign key value\nprint(book.author_id)  # If None, relationship isn't set\n\n# Properly assign relationship\nauthor = await Author.get(id=1)\nbook.author = author\nawait book.save()\n</code></pre></p>"},{"location":"advanced/debugging/#issue-reverse-relationship-not-working","title":"Issue: Reverse Relationship Not Working","text":"<p>Symptoms: - <code>await author.books.all()</code> returns empty list - Reverse relationship seems broken</p> <p>Diagnosis: - Check relationship name - Verify related_name is correct</p> <p>Solution: <pre><code># Check how the relationship is defined\nclass Book(Model):\n    author = ForeignKeyField(Author, related_name=\"books\")\n\n# Correct usage\nbooks = await author.books.all()\n\n# If related_name wasn't specified\nbooks = await author.book_set.all()\n</code></pre></p>"},{"location":"advanced/debugging/#debugging-transactions","title":"Debugging Transactions","text":""},{"location":"advanced/debugging/#diagnosing-transaction-issues","title":"Diagnosing Transaction Issues","text":""},{"location":"advanced/debugging/#step-1-verify-transaction-state","title":"Step 1: Verify Transaction State","text":"<pre><code>def print_transaction_status():\n    \"\"\"Print current transaction status\"\"\"\n    print(f\"Current transaction: {db._current_transaction}\")\n    print(f\"Transaction level: {getattr(db, '_transaction_level', 0)}\")\n</code></pre>"},{"location":"advanced/debugging/#step-2-monitor-transaction-execution","title":"Step 2: Monitor Transaction Execution","text":"<pre><code>async def monitored_transaction():\n    print(\"Starting transaction...\")\n    try:\n        async with db.transaction():\n            print(\"Transaction active\")\n            # Perform operations\n            await Book.create(title=\"Transaction Book\", pages=200)\n            print(\"Book created\")\n        print(\"Transaction committed\")\n    except Exception as e:\n        print(f\"Transaction rolled back: {e}\")\n        raise\n</code></pre>"},{"location":"advanced/debugging/#common-transaction-issues-and-solutions","title":"Common Transaction Issues and Solutions","text":""},{"location":"advanced/debugging/#issue-transaction-not-rolling-back","title":"Issue: Transaction Not Rolling Back","text":"<p>Symptoms: - Changes persist after exception - Data inconsistency</p> <p>Diagnosis: - Verify exception is not being caught and ignored - Check if transaction context is properly exited</p> <p>Solution: <pre><code># CORRECT: Transaction rolls back on exception\nasync with db.transaction():\n    await Book.create(title=\"Test\", pages=100)\n    raise ValueError(\"Force rollback\")\n\n# INCORRECT: Exception caught, transaction not rolled back\ntry:\n    async with db.transaction():\n        await Book.create(title=\"Test\", pages=100)\n        raise ValueError(\"Force rollback\")\nexcept ValueError:\n    pass  # Transaction still active!\n</code></pre></p>"},{"location":"advanced/debugging/#issue-deadlocks","title":"Issue: Deadlocks","text":"<p>Symptoms: - <code>TransactionRollbackError: deadlock detected</code> - Intermittent failures</p> <p>Diagnosis: - Check query execution order - Monitor lock contention</p> <p>Solution: <pre><code># Always access tables in consistent order\nasync with db.transaction():\n    # Always access authors before books\n    author = await Author.get(id=1)\n    book = await Book.get(id=1)\n\n    # Update in consistent order\n    await author.update(...)\n    await book.update(...)\n</code></pre></p>"},{"location":"advanced/debugging/#issue-nested-transaction-problems","title":"Issue: Nested Transaction Problems","text":"<p>Symptoms: - Unexpected rollbacks - Data not committed</p> <p>Diagnosis: - Check savepoint usage - Verify database supports nested transactions</p> <p>Solution: <pre><code>async with db.transaction():\n    await Book.create(title=\"Outer\", pages=100)\n\n    try:\n        async with db.transaction():\n            await Book.create(title=\"Inner\", pages=200)\n            raise ValueError(\"Force inner rollback\")\n    except ValueError:\n        pass  # Inner transaction rolled back, outer continues\n\n    await Book.create(title=\"Another Outer\", pages=300)\n</code></pre></p>"},{"location":"advanced/debugging/#advanced-debugging-techniques","title":"Advanced Debugging Techniques","text":""},{"location":"advanced/debugging/#using-a-debugger-with-async-code","title":"Using a Debugger with Async Code","text":"<p>Debugging async code requires special considerations:</p>"},{"location":"advanced/debugging/#vs-code-configuration","title":"VS Code Configuration","text":"<pre><code>{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: Ormax Debug\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"module\": \"pytest\",\n            \"args\": [\n                \"tests/test_debug.py\",\n                \"-s\"\n            ],\n            \"env\": {\n                \"PYTHONASYNCIODEBUG\": \"1\"\n            },\n            \"justMyCode\": false\n        }\n    ]\n}\n</code></pre>"},{"location":"advanced/debugging/#common-debugger-commands-for-async","title":"Common Debugger Commands for Async","text":"<ul> <li><code>asyncio.coroutines.coro_scheduling</code> - Show coroutine scheduling</li> <li><code>asyncio.tasks.all_tasks()</code> - List all running tasks</li> <li><code>asyncio.current_task()</code> - Get current task</li> </ul>"},{"location":"advanced/debugging/#debugging-tips","title":"Debugging Tips","text":"<ol> <li>Set breakpoints before async operations</li> <li>Use \"Step Over\" carefully with async/await</li> <li>Inspect task states when paused</li> <li>Watch for pending futures</li> </ol>"},{"location":"advanced/debugging/#database-specific-debugging","title":"Database-Specific Debugging","text":""},{"location":"advanced/debugging/#postgresql-debugging","title":"PostgreSQL Debugging","text":"<pre><code>async def debug_postgresql():\n    \"\"\"Get PostgreSQL-specific debug information\"\"\"\n    # Active queries\n    active_queries = await db.connection.fetch_all(\"\"\"\n        SELECT pid, now() - query_start AS duration, query \n        FROM pg_stat_activity \n        WHERE state = 'active'\n    \"\"\")\n\n    # Locks\n    locks = await db.connection.fetch_all(\"\"\"\n        SELECT * FROM pg_locks WHERE granted = false\n    \"\"\")\n\n    # Replication status\n    replication = await db.connection.fetch_all(\"\"\"\n        SELECT * FROM pg_stat_replication\n    \"\"\")\n\n    return {\n        \"active_queries\": active_queries,\n        \"locks\": locks,\n        \"replication\": replication\n    }\n</code></pre>"},{"location":"advanced/debugging/#mysql-debugging","title":"MySQL Debugging","text":"<pre><code>async def debug_mysql():\n    \"\"\"Get MySQL-specific debug information\"\"\"\n    # Process list\n    process_list = await db.connection.fetch_all(\"SHOW FULL PROCESSLIST\")\n\n    # InnoDB status\n    innodb_status = await db.connection.fetch_val(\"SHOW ENGINE INNODB STATUS\")\n\n    # Connection statistics\n    connection_stats = await db.connection.fetch_all(\"\"\"\n        SHOW STATUS LIKE 'Threads_%'\n    \"\"\")\n\n    return {\n        \"process_list\": process_list,\n        \"innodb_status\": innodb_status,\n        \"connection_stats\": connection_stats\n    }\n</code></pre>"},{"location":"advanced/debugging/#sqlite-debugging","title":"SQLite Debugging","text":"<pre><code>async def debug_sqlite():\n    \"\"\"Get SQLite-specific debug information\"\"\"\n    # Database statistics\n    stats = await db.connection.fetch_all(\"PRAGMA stats\")\n\n    # Table information\n    table_info = await db.connection.fetch_all(\"PRAGMA table_info(books)\")\n\n    # Index list\n    index_list = await db.connection.fetch_all(\"PRAGMA index_list(books)\")\n\n    return {\n        \"stats\": stats,\n        \"table_info\": table_info,\n        \"index_list\": index_list\n    }\n</code></pre>"},{"location":"advanced/debugging/#debugging-performance-issues","title":"Debugging Performance Issues","text":""},{"location":"advanced/debugging/#identifying-slow-queries","title":"Identifying Slow Queries","text":""},{"location":"advanced/debugging/#method-1-query-timing","title":"Method 1: Query Timing","text":"<pre><code>from datetime import datetime\n\nasync def time_query():\n    start = datetime.now()\n    books = await Book.objects().filter(pages__gt=300).all()\n    duration = (datetime.now() - start).total_seconds()\n    print(f\"Query took {duration:.4f} seconds\")\n    return books\n</code></pre>"},{"location":"advanced/debugging/#method-2-query-profiling","title":"Method 2: Query Profiling","text":"<pre><code>async def profile_query():\n    \"\"\"Profile a query for detailed timing information\"\"\"\n    # Enable profiling\n    await db.connection.execute(\"SET SESSION PROFILING = 1\")\n\n    # Run query\n    books = await Book.objects().filter(pages__gt=300).all()\n\n    # Get profiling data\n    profiles = await db.connection.fetch_all(\"SHOW PROFILES\")\n    profile_data = await db.connection.fetch_all(\n        f\"SHOW PROFILE FOR QUERY {profiles[0]['Query_ID']}\"\n    )\n\n    # Disable profiling\n    await db.connection.execute(\"SET SESSION PROFILING = 0\")\n\n    return {\n        \"books\": books,\n        \"profile_data\": profile_data\n    }\n</code></pre>"},{"location":"advanced/debugging/#memory-usage-debugging","title":"Memory Usage Debugging","text":""},{"location":"advanced/debugging/#tracking-memory-usage","title":"Tracking Memory Usage","text":"<pre><code>import tracemalloc\nfrom ormax.utils import Timer\n\nasync def debug_memory_usage():\n    \"\"\"Track memory usage during database operations\"\"\"\n    tracemalloc.start()\n\n    # Take initial snapshot\n    initial_snapshot = tracemalloc.take_snapshot()\n\n    # Run operation to monitor\n    with Timer(\"Memory Test\"):\n        books = await Book.objects().all()\n        # Process books to simulate real usage\n        processed = [book.title.upper() for book in books]\n\n    # Take final snapshot\n    final_snapshot = tracemalloc.take_snapshot()\n\n    # Compare snapshots\n    top_stats = final_snapshot.compare_to(initial_snapshot, 'lineno')\n\n    # Print top memory consumers\n    print(\"[ Top 10 memory consumers ]\")\n    for stat in top_stats[:10]:\n        print(stat)\n\n    tracemalloc.stop()\n</code></pre>"},{"location":"advanced/debugging/#identifying-memory-leaks","title":"Identifying Memory Leaks","text":"<pre><code>async def check_for_memory_leaks():\n    \"\"\"Check for memory leaks in database operations\"\"\"\n    import gc\n\n    # Run garbage collection\n    gc.collect()\n\n    # Get initial memory usage\n    initial = tracemalloc.take_snapshot()\n\n    # Perform operations multiple times\n    for _ in range(10):\n        books = await Book.objects().all()\n        # Process books\n        [book.title for book in books]\n\n    # Run garbage collection again\n    gc.collect()\n\n    # Get final memory usage\n    final = tracemalloc.take_snapshot()\n\n    # Compare snapshots\n    stats = final.compare_to(initial, 'lineno')\n\n    # Check for significant memory growth\n    if stats and stats[0].size_diff &gt; 1024 * 1024:  # 1MB\n        print(\"Potential memory leak detected:\")\n        for stat in stats[:5]:\n            print(f\"  {stat}\")\n    else:\n        print(\"No significant memory leak detected\")\n</code></pre>"},{"location":"advanced/debugging/#debugging-migration-issues","title":"Debugging Migration Issues","text":""},{"location":"advanced/debugging/#diagnosing-migration-problems","title":"Diagnosing Migration Problems","text":""},{"location":"advanced/debugging/#step-1-verify-migration-status","title":"Step 1: Verify Migration Status","text":"<pre><code>async def check_migration_status():\n    \"\"\"Check current migration status\"\"\"\n    # Check if migrations table exists\n    exists = await db.connection.fetch_val(\n        \"SELECT 1 FROM sqlite_master WHERE type='table' AND name='migrations'\"\n    )\n\n    if not exists:\n        print(\"Migrations table does not exist\")\n        return\n\n    # Get applied migrations\n    applied = await db.connection.fetch_all(\"SELECT * FROM migrations\")\n    print(f\"Applied migrations ({len(applied)}):\")\n    for migration in applied:\n        print(f\"  - {migration['name']}\")\n</code></pre>"},{"location":"advanced/debugging/#step-2-inspect-migration-scripts","title":"Step 2: Inspect Migration Scripts","text":"<pre><code>def inspect_migration_script(migration_name):\n    \"\"\"Inspect a migration script without executing it\"\"\"\n    import importlib.util\n    import os\n\n    migration_path = f\"migrations/{migration_name}.py\"\n    if not os.path.exists(migration_path):\n        print(f\"Migration {migration_name} not found\")\n        return\n\n    # Load the module without executing\n    spec = importlib.util.spec_from_file_location(\n        migration_name, migration_path\n    )\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n\n    # Print up and down functions\n    print(f\"\\nMigration: {migration_name}\")\n    print(\"Up function:\")\n    print(inspect.getsource(module.up))\n    print(\"\\nDown function:\")\n    print(inspect.getsource(module.down))\n</code></pre>"},{"location":"advanced/debugging/#common-migration-issues-and-solutions","title":"Common Migration Issues and Solutions","text":""},{"location":"advanced/debugging/#issue-migration-already-applied","title":"Issue: Migration Already Applied","text":"<p>Symptoms: - <code>DatabaseError: Migration already applied</code> - Duplicate entry errors</p> <p>Diagnosis: - Check migrations table - Verify migration script idempotency</p> <p>Solution: <pre><code># Ensure migration is idempotent\nasync def up(db):\n    # Check if column already exists\n    columns = await db.connection.fetch_all(\n        \"PRAGMA table_info(books)\"\n    )\n    if \"pages\" not in [col[\"name\"] for col in columns]:\n        await db.connection.execute(\n            \"ALTER TABLE books ADD COLUMN pages INTEGER NOT NULL DEFAULT 100\"\n        )\n</code></pre></p>"},{"location":"advanced/debugging/#issue-foreign-key-constraint-violation-during-migration","title":"Issue: Foreign Key Constraint Violation During Migration","text":"<p>Symptoms: - <code>IntegrityError: FOREIGN KEY constraint failed</code> - Migration fails when adding foreign key</p> <p>Diagnosis: - Check existing data - Verify referenced values exist</p> <p>Solution: <pre><code>async def up(db):\n    # 1. Add column without constraint\n    await db.connection.execute(\n        \"ALTER TABLE books ADD COLUMN author_id INTEGER\"\n    )\n\n    # 2. Update existing data\n    await db.connection.execute(\n        \"UPDATE books SET author_id = 1 WHERE author_id IS NULL\"\n    )\n\n    # 3. Add foreign key constraint\n    await db.connection.execute(\n        \"ALTER TABLE books ADD CONSTRAINT fk_author \"\n        \"FOREIGN KEY (author_id) REFERENCES authors(id)\"\n    )\n</code></pre></p>"},{"location":"advanced/debugging/#issue-migration-rollback-failure","title":"Issue: Migration Rollback Failure","text":"<p>Symptoms: - <code>DatabaseError: Cannot drop column used in foreign key constraint</code> - Rollback fails after partial migration</p> <p>Diagnosis: - Check database-specific limitations - Verify rollback script</p> <p>Solution: <pre><code>async def down(db):\n    # For PostgreSQL, need to drop constraints first\n    if \"postgresql\" in db.connection_string:\n        await db.connection.execute(\n            \"ALTER TABLE books DROP CONSTRAINT IF EXISTS fk_author\"\n        )\n\n    # Now drop the column\n    await db.connection.execute(\n        \"ALTER TABLE books DROP COLUMN author_id\"\n    )\n</code></pre></p>"},{"location":"advanced/debugging/#debugging-with-real-world-examples","title":"Debugging with Real-World Examples","text":""},{"location":"advanced/debugging/#debugging-an-n1-query-problem","title":"Debugging an N+1 Query Problem","text":"<pre><code>import logging\nfrom io import StringIO\n\ndef detect_n_plus_one():\n    \"\"\"Detect potential N+1 query patterns in logs\"\"\"\n    # Configure logger to capture queries\n    log_stream = StringIO()\n    handler = logging.StreamHandler(log_stream)\n    logger = logging.getLogger(\"ormax\")\n    logger.addHandler(handler)\n\n    try:\n        # Run the code that might have N+1 issues\n        books = Book.objects().all()\n        for book in books:\n            await book.author  # This would trigger N+1 queries\n\n        # Analyze logs\n        log_content = log_stream.getvalue()\n        queries = [line for line in log_content.split('\\n') \n                  if \"Executing query\" in line]\n\n        # Look for similar queries with different parameters\n        pattern_counts = {}\n        for query in queries:\n            # Extract the base query without parameters\n            base_query = re.sub(r'\\?.*?FROM', ' ? FROM', query)\n            base_query = re.sub(r'WHERE .*? = ', 'WHERE ? = ', base_query)\n\n            pattern_counts[base_query] = pattern_counts.get(base_query, 0) + 1\n\n        # Flag potential N+1 queries\n        n_plus_one_queries = [\n            (pattern, count) for pattern, count in pattern_counts.items() \n            if count &gt; 5  # Threshold for potential N+1\n        ]\n\n        if n_plus_one_queries:\n            print(\"Potential N+1 query patterns detected:\")\n            for pattern, count in n_plus_one_queries:\n                print(f\"  {count} occurrences of: {pattern}\")\n            return True\n        return False\n    finally:\n        logger.removeHandler(handler)\n\n# Usage\nif detect_n_plus_one():\n    print(\"Consider using select_related or prefetch_related\")\n</code></pre>"},{"location":"advanced/debugging/#debugging-a-transaction-deadlock","title":"Debugging a Transaction Deadlock","text":"<pre><code>async def debug_deadlock():\n    \"\"\"Debug a transaction deadlock scenario\"\"\"\n    # Setup test data\n    author1 = await Author.create(name=\"Author 1\")\n    author2 = await Author.create(name=\"Author 2\")\n    book1 = await Book.create(title=\"Book 1\", author=author1)\n    book2 = await Book.create(title=\"Book 2\", author=author2)\n\n    # Create two tasks that might deadlock\n    async def update_task1():\n        async with db.transaction():\n            # Update author1\n            await Author.get(id=author1.id).update(name=\"Updated 1\")\n            # Try to update book2\n            await Book.get(id=book2.id).update(title=\"Updated Book 2\")\n\n    async def update_task2():\n        async with db.transaction():\n            # Update author2\n            await Author.get(id=author2.id).update(name=\"Updated 2\")\n            # Try to update book1\n            await Book.get(id=book1.id).update(title=\"Updated Book 1\")\n\n    # Run tasks concurrently\n    try:\n        await asyncio.gather(update_task1(), update_task2())\n    except Exception as e:\n        print(f\"Transaction failed: {e}\")\n\n        # Get database-specific deadlock information\n        if \"postgresql\" in db.connection_string:\n            locks = await db.connection.fetch_all(\"\"\"\n                SELECT \n                    blocked_locks.pid AS blocked_pid,\n                    blocking_locks.pid AS blocking_pid,\n                    blocked_activity.query AS blocked_query,\n                    blocking_activity.query AS blocking_query\n                FROM pg_catalog.pg_locks blocked_locks\n                JOIN pg_catalog.pg_stat_activity blocked_activity \n                    ON blocked_activity.pid = blocked_locks.pid\n                JOIN pg_catalog.pg_locks blocking_locks \n                    ON blocking_locks.locktype = blocked_locks.locktype\n                    AND blocking_locks.DATABASE IS NOT DISTINCT FROM blocked_locks.DATABASE\n                    AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation\n                    AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page\n                    AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple\n                    AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid\n                    AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid\n                    AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid\n                    AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid\n                    AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid\n                    AND blocking_locks.pid != blocked_locks.pid\n                JOIN pg_catalog.pg_stat_activity blocking_activity \n                    ON blocking_activity.pid = blocking_locks.pid\n                WHERE NOT blocked_locks.GRANTED\n            \"\"\")\n            if locks:\n                print(\"\\nDeadlock details:\")\n                for lock in locks:\n                    print(f\"Blocked PID {lock['blocked_pid']} by PID {lock['blocking_pid']}\")\n                    print(f\"Blocked query: {lock['blocked_query']}\")\n                    print(f\"Blocking query: {lock['blocking_query']}\")\n</code></pre>"},{"location":"advanced/debugging/#best-practices-for-debugging-ormax-applications","title":"Best Practices for Debugging Ormax Applications","text":""},{"location":"advanced/debugging/#use-structured-logging","title":"Use Structured Logging","text":"<pre><code>import logging\nimport json\n\n# Configure structured logging\nclass JsonFormatter(logging.Formatter):\n    def format(self, record):\n        log_data = {\n            \"timestamp\": self.formatTime(record),\n            \"level\": record.levelname,\n            \"message\": record.getMessage(),\n            \"logger\": record.name,\n        }\n        if hasattr(record, \"query\"):\n            log_data[\"query\"] = record.query\n        if hasattr(record, \"params\"):\n            log_data[\"params\"] = record.params\n        return json.dumps(log_data)\n\n# Setup logger\nlogger = logging.getLogger(\"ormax\")\nhandler = logging.StreamHandler()\nhandler.setFormatter(JsonFormatter())\nlogger.addHandler(handler)\nlogger.setLevel(logging.INFO)\n\n# Log queries with parameters\ndef log_query(query, params=None):\n    extra = {\"query\": query}\n    if params:\n        extra[\"params\"] = params\n    logger.info(\"Database query executed\", extra=extra)\n</code></pre>"},{"location":"advanced/debugging/#implement-debug-endpoints","title":"Implement Debug Endpoints","text":"<pre><code>from fastapi import APIRouter, Depends\nfrom ormax.utils import get_system_info\n\nrouter = APIRouter()\n\n@router.get(\"/debug/db\")\nasync def debug_db():\n    \"\"\"Get database debug information\"\"\"\n    # Connection status\n    connection_status = \"connected\" if db._connected else \"disconnected\"\n\n    # Query statistics\n    query_stats = {\n        \"total\": len(logger.handlers[0].logs),\n        \"slow\": sum(1 for log in logger.handlers[0].logs if \"Slow query\" in log.getMessage())\n    }\n\n    # System information\n    system_info = get_system_info()\n\n    return {\n        \"connection_status\": connection_status,\n        \"query_stats\": query_stats,\n        \"system_info\": system_info,\n        \"timestamp\": datetime.now().isoformat()\n    }\n</code></pre>"},{"location":"advanced/debugging/#create-debugging-utilities","title":"Create Debugging Utilities","text":"<pre><code>def print_model_schema(model_class):\n    \"\"\"Print detailed schema information for a model\"\"\"\n    print(f\"\\nModel: {model_class.__name__}\")\n    print(f\"Table: {model_class.get_table_name()}\")\n    print(\"\\nFields:\")\n\n    for field_name, field in model_class._fields.items():\n        field_type = field.get_sql_type()\n        constraints = []\n\n        if field.primary_key:\n            constraints.append(\"PRIMARY KEY\")\n        if field.auto_increment:\n            constraints.append(\"AUTOINCREMENT\")\n        if not field.nullable:\n            constraints.append(\"NOT NULL\")\n        if field.unique:\n            constraints.append(\"UNIQUE\")\n        if field.default is not None:\n            constraints.append(f\"DEFAULT {field.get_default_sql()}\")\n\n        print(f\"  {field_name}: {field_type}\")\n        if constraints:\n            print(f\"    {' | '.join(constraints)}\")\n\n    # Relationships\n    relationships = []\n    for name, manager in model_class._relationships.get_all_related_managers().items():\n        relationships.append(f\"  {name} -&gt; {manager.related_model_class.__name__}\")\n\n    for name, manager in model_class._relationships.get_all_reverse_managers().items():\n        relationships.append(f\"  {name} &lt;- {manager.related_model_class.__name__}\")\n\n    if relationships:\n        print(\"\\nRelationships:\")\n        for rel in relationships:\n            print(rel)\n</code></pre>"},{"location":"advanced/debugging/#troubleshooting-common-debugging-issues","title":"Troubleshooting Common Debugging Issues","text":""},{"location":"advanced/debugging/#issue-debugger-doesnt-stop-at-breakpoints","title":"Issue: Debugger Doesn't Stop at Breakpoints","text":"<p>Symptoms: - Breakpoints are ignored - Debugger skips over async code</p> <p>Solutions: - Ensure \"Asyncio Support\" is enabled in your IDE - Use <code>asyncio.run()</code> for top-level async code - Set breakpoints before async operations - Check if code is being executed in a different event loop</p>"},{"location":"advanced/debugging/#issue-logging-doesnt-show-queries","title":"Issue: Logging Doesn't Show Queries","text":"<p>Symptoms: - No query logs appear - Debug logs are missing</p> <p>Solutions: - Verify logger level is set to DEBUG - Check if logger is properly configured - Ensure you're using the correct logger name (\"ormax\") - Check if logs are being captured by another handler</p>"},{"location":"advanced/debugging/#issue-memory-profiling-shows-false-positives","title":"Issue: Memory Profiling Shows False Positives","text":"<p>Symptoms: - Memory usage appears to grow but isn't a real leak - Profiling shows unexpected allocations</p> <p>Solutions: - Run garbage collection before taking snapshots - Take multiple snapshots to identify real trends - Focus on long-term growth rather than short-term fluctuations - Use production-like data volumes for profiling</p>"},{"location":"advanced/debugging/#issue-transaction-state-is-confusing","title":"Issue: Transaction State is Confusing","text":"<p>Symptoms: - Can't tell if in a transaction - Unexpected rollbacks or commits</p> <p>Solutions: - Add transaction state logging - Create a transaction context inspector - Use database-specific transaction monitoring queries - Implement transaction nesting counters</p>"},{"location":"advanced/debugging/#debugging-checklist","title":"Debugging Checklist","text":"<p>Use this checklist when debugging Ormax issues:</p>"},{"location":"advanced/debugging/#initial-diagnosis","title":"Initial Diagnosis","text":"<ul> <li>[ ] Reproduce the issue consistently</li> <li>[ ] Identify the error type and message</li> <li>[ ] Check logs for relevant information</li> <li>[ ] Determine if the issue is in the ORM or database</li> </ul>"},{"location":"advanced/debugging/#query-issues","title":"Query Issues","text":"<ul> <li>[ ] View the actual SQL being executed</li> <li>[ ] Check parameter values</li> <li>[ ] Verify the query execution plan</li> <li>[ ] Test the query directly in the database client</li> </ul>"},{"location":"advanced/debugging/#connection-issues","title":"Connection Issues","text":"<ul> <li>[ ] Verify connection string format</li> <li>[ ] Test basic connectivity</li> <li>[ ] Check connection pool status</li> <li>[ ] Review connection timeout settings</li> </ul>"},{"location":"advanced/debugging/#relationship-issues","title":"Relationship Issues","text":"<ul> <li>[ ] Verify relationship definitions</li> <li>[ ] Check foreign key values</li> <li>[ ] Inspect related objects directly</li> <li>[ ] Test relationships with minimal data</li> </ul>"},{"location":"advanced/debugging/#transaction-issues","title":"Transaction Issues","text":"<ul> <li>[ ] Check current transaction state</li> <li>[ ] Monitor lock contention</li> <li>[ ] Verify transaction boundaries</li> <li>[ ] Test with simplified transaction flow</li> </ul>"},{"location":"advanced/debugging/#performance-issues","title":"Performance Issues","text":"<ul> <li>[ ] Time the problematic operation</li> <li>[ ] Profile the database query</li> <li>[ ] Check for N+1 query problems</li> <li>[ ] Analyze memory usage patterns</li> </ul>"},{"location":"advanced/debugging/#advanced-debugging-tools","title":"Advanced Debugging Tools","text":""},{"location":"advanced/debugging/#database-monitoring-tools","title":"Database Monitoring Tools","text":""},{"location":"advanced/debugging/#postgresql","title":"PostgreSQL","text":"<ul> <li><code>pg_stat_statements</code>: Track query execution statistics</li> <li><code>pg_locks</code>: Monitor lock contention</li> <li><code>pg_blocking_pids</code>: Identify blocking processes</li> </ul>"},{"location":"advanced/debugging/#mysql","title":"MySQL","text":"<ul> <li><code>SHOW PROCESSLIST</code>: View active queries</li> <li><code>SHOW ENGINE INNODB STATUS</code>: InnoDB engine status</li> <li><code>performance_schema</code>: Detailed performance metrics</li> </ul>"},{"location":"advanced/debugging/#sqlite","title":"SQLite","text":"<ul> <li><code>PRAGMA stats</code>: Database statistics</li> <li><code>PRAGMA table_info</code>: Table schema information</li> <li><code>PRAGMA index_list</code>: Index information</li> </ul>"},{"location":"advanced/debugging/#python-debugging-tools","title":"Python Debugging Tools","text":""},{"location":"advanced/debugging/#asyncio-debugging","title":"Asyncio Debugging","text":"<ul> <li><code>PYTHONASYNCIODEBUG=1</code>: Enable asyncio debug mode</li> <li><code>asyncio.current_task()</code>: Get current task</li> <li><code>asyncio.all_tasks()</code>: List all tasks</li> </ul>"},{"location":"advanced/debugging/#memory-profiling","title":"Memory Profiling","text":"<ul> <li><code>tracemalloc</code>: Track memory allocations</li> <li><code>objgraph</code>: Visualize object references</li> <li><code>memory_profiler</code>: Line-by-line memory usage</li> </ul>"},{"location":"advanced/debugging/#performance-profiling","title":"Performance Profiling","text":"<ul> <li><code>cProfile</code>: CPU profiling</li> <li><code>line_profiler</code>: Line-by-line CPU profiling</li> <li><code>py-spy</code>: Sampling profiler for running processes</li> </ul>"},{"location":"advanced/debugging/#real-world-debugging-scenarios","title":"Real-World Debugging Scenarios","text":""},{"location":"advanced/debugging/#scenario-intermittent-unique-constraint-violation","title":"Scenario: Intermittent Unique Constraint Violation","text":"<p>Problem: Users occasionally see \"UNIQUE constraint failed\" errors when creating accounts, but the username appears to be available.</p> <p>Debugging Steps: 1. Add detailed logging for username checks 2. Capture timestamp of each operation 3. Check for concurrent requests 4. Review transaction isolation level</p> <p>Solution: <pre><code>async def create_account(username, email, password):\n    # Use SERIALIZABLE isolation to prevent race conditions\n    async with db.transaction(isolation_level=\"SERIALIZABLE\"):\n        # Check if username exists\n        exists = await User.objects().filter(username=username).exists()\n        if exists:\n            raise ValueError(\"Username already taken\")\n\n        # Create account\n        return await User.create(\n            username=username,\n            email=email,\n            password=password\n        )\n</code></pre></p>"},{"location":"advanced/debugging/#scenario-slow-dashboard-queries","title":"Scenario: Slow Dashboard Queries","text":"<p>Problem: Dashboard queries that were fast during development are slow in production.</p> <p>Debugging Steps: 1. Capture EXPLAIN output for production queries 2. Compare execution plans between dev and prod 3. Check index usage in production 4. Profile query execution time</p> <p>Solution: <pre><code>async def get_dashboard_data():\n    # Optimized query with explicit indexes\n    books = await Book.objects() \\\n        .select_related(\"author\") \\\n        .only(\"id\", \"title\", \"pages\", \"author_id\") \\\n        .order_by(\"-published_date\") \\\n        .limit(10) \\\n        .all()\n\n    # Batch author loading\n    author_ids = list(set(book.author_id for book in books))\n    authors = await Author.objects().filter(id__in=author_ids).all()\n    author_map = {author.id: author for author in authors}\n\n    # Process results without additional queries\n    for book in books:\n        book.author = author_map.get(book.author_id)\n\n    return {\n        \"books\": books,\n        \"stats\": await calculate_stats()\n    }\n</code></pre></p>"},{"location":"advanced/deployment/","title":"Deployment Strategies for Ormax Applications","text":"<p>Proper deployment is crucial for ensuring your Ormax-based application runs smoothly in production. This guide covers best practices, strategies, and considerations for deploying applications that use Ormax ORM across various environments.</p>"},{"location":"advanced/deployment/#deployment-considerations","title":"Deployment Considerations","text":"<p>Before deploying, consider these key factors:</p>"},{"location":"advanced/deployment/#database-compatibility","title":"Database Compatibility","text":"<p>Ormax supports multiple database backends, but each has specific deployment requirements:</p> Database Production Readiness Connection Stability Scaling Characteristics PostgreSQL \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2605 Excellent horizontal scaling MySQL \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2606 Good scaling with proper configuration SQLite \u2605\u2605\u2606\u2606\u2606 \u2605\u2605\u2606\u2606\u2606 Limited to single-server deployments MSSQL \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2606 Enterprise scaling capabilities Oracle \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2605 Enterprise-grade scaling Aurora \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2605 Excellent cloud scaling"},{"location":"advanced/deployment/#environment-configuration","title":"Environment Configuration","text":"<p>Ensure proper configuration for each environment:</p> <pre><code># Environment-specific configuration\nimport os\n\nDB_URL = os.getenv(\"DATABASE_URL\")\nDEBUG = os.getenv(\"DEBUG\", \"false\").lower() == \"true\"\n\n# Configure database based on environment\nif \"production\" in os.getenv(\"ENV\", \"\"):\n    db = Database(\n        DB_URL,\n        min_size=10,\n        max_size=50,\n        command_timeout=30.0\n    )\nelif \"staging\" in os.getenv(\"ENV\", \"\"):\n    db = Database(\n        DB_URL,\n        min_size=5,\n        max_size=20,\n        command_timeout=60.0\n    )\nelse:\n    db = Database(\"sqlite:///:memory:\")\n</code></pre>"},{"location":"advanced/deployment/#deployment-patterns","title":"Deployment Patterns","text":""},{"location":"advanced/deployment/#monolithic-application-deployment","title":"Monolithic Application Deployment","text":"<p>For traditional monolithic applications:</p>"},{"location":"advanced/deployment/#directory-structure","title":"Directory Structure","text":"<pre><code>myapp/\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 book.py\n\u2502   \u2502   \u2514\u2500\u2500 author.py\n\u2502   \u251c\u2500\u2500 services/\n\u2502   \u2514\u2500\u2500 views/\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 development.py\n\u2502   \u251c\u2500\u2500 staging.py\n\u2502   \u2514\u2500\u2500 production.py\n\u251c\u2500\u2500 migrations/\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 run.py\n</code></pre>"},{"location":"advanced/deployment/#deployment-steps","title":"Deployment Steps","text":"<ol> <li>Install dependencies: <code>pip install -r requirements.txt</code></li> <li>Apply database migrations: <code>python manage.py migrate</code></li> <li>Run the application: <code>gunicorn app.wsgi:application --bind 0.0.0.0:8000</code></li> </ol>"},{"location":"advanced/deployment/#microservices-deployment","title":"Microservices Deployment","text":"<p>For microservices architecture using Ormax:</p>"},{"location":"advanced/deployment/#service-structure","title":"Service Structure","text":"<pre><code>book-service/\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 service.py\n\u2514\u2500\u2500 models/\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 book.py\n\nauthor-service/\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 service.py\n\u2514\u2500\u2500 models/\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 author.py\n</code></pre>"},{"location":"advanced/deployment/#database-per-service-pattern","title":"Database per Service Pattern","text":"<p>Each service has its own database:</p> <pre><code># book-service/models/book.py\nfrom ormax import Model, Database\nfrom ormax.fields import CharField, IntegerField, ForeignKeyField\n\ndb = Database(os.getenv(\"BOOK_DB_URL\"))\n\nclass Book(Model):\n    title = CharField(max_length=200)\n    pages = IntegerField()\n\ndb.register_model(Book)\n</code></pre> <pre><code># author-service/models/author.py\nfrom ormax import Model, Database\nfrom ormax.fields import CharField\n\ndb = Database(os.getenv(\"AUTHOR_DB_URL\"))\n\nclass Author(Model):\n    name = CharField(max_length=100)\n\ndb.register_model(Author)\n</code></pre>"},{"location":"advanced/deployment/#deployment-with-kubernetes","title":"Deployment with Kubernetes","text":"<pre><code># book-service-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: book-service\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: book-service\n  template:\n    metadata:\n      labels:\n        app: book-service\n    spec:\n      containers:\n      - name: book-service\n        image: my-registry/book-service:latest\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: db-secrets\n              key: book-db-url\n        ports:\n        - containerPort: 8000\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"1000m\"\n</code></pre>"},{"location":"advanced/deployment/#database-migration-strategies","title":"Database Migration Strategies","text":""},{"location":"advanced/deployment/#schema-migration-with-ormax","title":"Schema Migration with Ormax","text":"<p>Ormax doesn't include built-in migrations, but here's a recommended approach:</p>"},{"location":"advanced/deployment/#migration-file-structure","title":"Migration File Structure","text":"<pre><code>migrations/\n\u251c\u2500\u2500 0001_initial.py\n\u251c\u2500\u2500 0002_add_pages_field.py\n\u251c\u2500\u2500 0003_create_authors_table.py\n\u2514\u2500\u2500 __init__.py\n</code></pre>"},{"location":"advanced/deployment/#sample-migration","title":"Sample Migration","text":"<pre><code># migrations/0002_add_pages_field.py\nasync def up(db):\n    \"\"\"Add pages field to books table\"\"\"\n    await db.connection.execute(\n        \"ALTER TABLE books ADD COLUMN pages INTEGER NOT NULL DEFAULT 100\"\n    )\n\nasync def down(db):\n    \"\"\"Remove pages field from books table\"\"\"\n    await db.connection.execute(\n        \"ALTER TABLE books DROP COLUMN pages\"\n    )\n</code></pre>"},{"location":"advanced/deployment/#migration-runner","title":"Migration Runner","text":"<pre><code>import os\nimport importlib\nfrom ormax import Database\n\nasync def run_migrations():\n    db = Database(os.getenv(\"DATABASE_URL\"))\n    await db.connect()\n\n    # Get applied migrations\n    try:\n        await db.connection.execute(\n            \"CREATE TABLE IF NOT EXISTS migrations (name TEXT PRIMARY KEY)\"\n        )\n        applied = await db.connection.fetch_all(\n            \"SELECT name FROM migrations\"\n        )\n        applied = {m[\"name\"] for m in applied}\n    except Exception as e:\n        print(f\"Error checking migrations table: {e}\")\n        applied = set()\n\n    # Run pending migrations\n    migration_dir = \"migrations\"\n    for filename in sorted(os.listdir(migration_dir)):\n        if filename.endswith(\".py\") and not filename.startswith(\"__\"):\n            migration_name = filename[:-3]\n            if migration_name in applied:\n                continue\n\n            print(f\"Running migration: {migration_name}\")\n            try:\n                # Import and run migration\n                module = importlib.import_module(f\"migrations.{migration_name}\")\n                async with db.transaction():\n                    await module.up(db)\n                    await db.connection.execute(\n                        \"INSERT INTO migrations (name) VALUES (?)\", \n                        (migration_name,)\n                    )\n                print(f\"Migration {migration_name} applied successfully\")\n            except Exception as e:\n                print(f\"Migration {migration_name} failed: {e}\")\n                raise\n\n    await db.disconnect()\n</code></pre>"},{"location":"advanced/deployment/#zero-downtime-deployments","title":"Zero-Downtime Deployments","text":"<p>For applications requiring zero downtime during deployments:</p>"},{"location":"advanced/deployment/#dual-write-pattern","title":"Dual-Write Pattern","text":"<pre><code>async def dual_write_data(new_data):\n    \"\"\"Write to both old and new schema during transition period\"\"\"\n    # Write to current schema\n    await save_to_current_schema(new_data)\n\n    # Write to new schema\n    await save_to_new_schema(new_data)\n\n    # Track dual-write status\n    await track_dual_write(new_data[\"id\"])\n</code></pre>"},{"location":"advanced/deployment/#blue-green-deployment-with-database","title":"Blue-Green Deployment with Database","text":"<ol> <li>Deploy new application version (green) alongside existing (blue)</li> <li>Configure green to write to both old and new database schemas</li> <li>Run data migration in background</li> <li>Switch traffic to green once migration is complete</li> <li>Decommission blue environment</li> </ol>"},{"location":"advanced/deployment/#canary-releases-with-database-routing","title":"Canary Releases with Database Routing","text":"<pre><code>class DatabaseRouter:\n    \"\"\"Route database operations based on traffic percentage\"\"\"\n    def __init__(self, primary_db, secondary_db, canary_percentage=5):\n        self.primary_db = primary_db\n        self.secondary_db = secondary_db\n        self.canary_percentage = canary_percentage\n\n    async def get_db(self, user_id=None):\n        \"\"\"Determine which database to use\"\"\"\n        if user_id and self._is_canary_user(user_id):\n            return self.secondary_db\n        return self.primary_db\n\n    def _is_canary_user(self, user_id):\n        \"\"\"Determine if user should use canary deployment\"\"\"\n        return hash(user_id) % 100 &lt; self.canary_percentage\n</code></pre>"},{"location":"advanced/deployment/#production-configuration-best-practices","title":"Production Configuration Best Practices","text":""},{"location":"advanced/deployment/#connection-pool-configuration","title":"Connection Pool Configuration","text":"<p>Optimize connection pool settings for production:</p> <pre><code>db = Database(\n    os.getenv(\"DATABASE_URL\"),\n    min_size=10,          # Minimum connections to keep open\n    max_size=50,          # Maximum connections in pool\n    pool_recycle=1800,    # Recycle connections after 30 minutes\n    command_timeout=30.0, # Query timeout in seconds\n    connect_timeout=10.0  # Connection timeout in seconds\n)\n</code></pre>"},{"location":"advanced/deployment/#connection-pool-sizing-formula","title":"Connection Pool Sizing Formula","text":"<pre><code>max_connections = (number_of_instances * max_size_per_instance) * safety_factor\n\n# Where safety_factor is typically 1.2-1.5 to account for spikes\n</code></pre>"},{"location":"advanced/deployment/#environment-specific-settings","title":"Environment-Specific Settings","text":""},{"location":"advanced/deployment/#production-settings","title":"Production Settings","text":"<pre><code># production.py\nDATABASE_URL = os.getenv(\"DATABASE_URL\")\nDEBUG = False\nLOG_LEVEL = \"INFO\"\nCONNECTION_MIN_SIZE = 10\nCONNECTION_MAX_SIZE = 50\nQUERY_TIMEOUT = 30.0\n</code></pre>"},{"location":"advanced/deployment/#staging-settings","title":"Staging Settings","text":"<pre><code># staging.py\nDATABASE_URL = os.getenv(\"DATABASE_URL\")\nDEBUG = True\nLOG_LEVEL = \"DEBUG\"\nCONNECTION_MIN_SIZE = 5\nCONNECTION_MAX_SIZE = 20\nQUERY_TIMEOUT = 60.0\n</code></pre>"},{"location":"advanced/deployment/#database-specific-production-tuning","title":"Database-Specific Production Tuning","text":""},{"location":"advanced/deployment/#postgresql-production-tuning","title":"PostgreSQL Production Tuning","text":"<pre><code># PostgreSQL-specific optimizations\ndb = Database(\n    os.getenv(\"DATABASE_URL\"),\n    min_size=15,\n    max_size=60,\n    command_timeout=20.0,\n    init_command=\"SET work_mem TO '64MB'; SET maintenance_work_mem TO '256MB'\"\n)\n</code></pre>"},{"location":"advanced/deployment/#mysql-production-tuning","title":"MySQL Production Tuning","text":"<pre><code># MySQL-specific optimizations\ndb = Database(\n    os.getenv(\"DATABASE_URL\"),\n    min_size=12,\n    max_size=48,\n    command_timeout=25.0,\n    init_command=\"SET SESSION wait_timeout=28800; SET SESSION innodb_lock_wait_timeout=50\"\n)\n</code></pre>"},{"location":"advanced/deployment/#monitoring-and-alerting-in-production","title":"Monitoring and Alerting in Production","text":""},{"location":"advanced/deployment/#critical-metrics-to-monitor","title":"Critical Metrics to Monitor","text":"Metric Warning Threshold Critical Threshold Monitoring Strategy Connection Usage 75% 90% Real-time monitoring with alerts Query Time (P95) 200ms 500ms Histogram tracking Error Rate 0.5% 2% Per-endpoint tracking Transaction Duration 500ms 2s Trace-based monitoring"},{"location":"advanced/deployment/#setting-up-production-monitoring","title":"Setting Up Production Monitoring","text":""},{"location":"advanced/deployment/#basic-monitoring-setup","title":"Basic Monitoring Setup","text":"<pre><code>from ormax.utils import performance_monitor\n\ndef setup_production_monitoring():\n    \"\"\"Configure monitoring for production environment\"\"\"\n    # Set up logging\n    import logging\n    logger = logging.getLogger(\"ormax\")\n    logger.setLevel(logging.INFO)\n    logger.addHandler(logging.FileHandler(\"/var/log/ormax.log\"))\n\n    # Set up performance monitoring\n    performance_monitor.clear()\n\n    # Set up alerts\n    from .alerts import setup_alerts\n    setup_alerts()\n\n    # Start monitoring tasks\n    from .monitoring import start_background_monitoring\n    start_background_monitoring()\n</code></pre>"},{"location":"advanced/deployment/#alert-configuration","title":"Alert Configuration","text":"<pre><code># alerts.py\nfrom .notification import send_alert\n\ndef setup_alerts():\n    \"\"\"Configure alert thresholds and handlers\"\"\"\n    from ormax.utils import performance_monitor\n\n    # Query time alerts\n    performance_monitor.add_alert(\n        \"query_time\",\n        lambda stats: stats[\"max\"] &gt; 0.5,  # 500ms\n        lambda stats: send_alert(\n            \"High Query Time\",\n            f\"Maximum query time: {stats['max']:.2f}s\"\n        )\n    )\n\n    # Connection pool alerts\n    performance_monitor.add_alert(\n        \"connection_usage\",\n        lambda stats: stats[\"used\"] / stats[\"max\"] &gt; 0.85,  # 85% usage\n        lambda stats: send_alert(\n            \"High Connection Usage\",\n            f\"Connection pool usage: {stats['used']}/{stats['max']} ({stats['used']/stats['max']:.1%})\"\n        )\n    )\n</code></pre>"},{"location":"advanced/deployment/#security-considerations","title":"Security Considerations","text":""},{"location":"advanced/deployment/#database-credentials-management","title":"Database Credentials Management","text":"<p>Never hardcode credentials in your application:</p> <pre><code># BAD: Hardcoded credentials\ndb = Database(\"postgresql://user:password@localhost:5432/mydb\")\n\n# GOOD: Use environment variables\nimport os\ndb = Database(os.getenv(\"DATABASE_URL\"))\n</code></pre>"},{"location":"advanced/deployment/#recommended-credential-management","title":"Recommended Credential Management","text":"<ul> <li>Use environment variables for containerized applications</li> <li>Use secret management services (AWS Secrets Manager, HashiCorp Vault)</li> <li>Rotate credentials regularly</li> <li>Use IAM roles where possible (AWS RDS IAM authentication)</li> </ul>"},{"location":"advanced/deployment/#sql-injection-prevention","title":"SQL Injection Prevention","text":"<p>Ormax automatically parameterizes queries, but be careful with raw queries:</p> <pre><code># BAD: Potential SQL injection\nquery = f\"SELECT * FROM books WHERE title = '{user_input}'\"\nresults = await db.connection.fetch_all(query)\n\n# GOOD: Proper parameterization\nquery = \"SELECT * FROM books WHERE title = ?\"\nresults = await db.connection.fetch_all(query, (user_input,))\n</code></pre>"},{"location":"advanced/deployment/#connection-security","title":"Connection Security","text":"<p>Ensure secure database connections:</p> <pre><code># PostgreSQL with SSL\ndb = Database(\n    \"postgresql://user:password@db.example.com:5432/mydb?\"\n    \"sslmode=require&amp;\"\n    \"sslrootcert=/path/to/ca.pem\"\n)\n\n# MySQL with SSL\ndb = Database(\n    \"mysql://user:password@db.example.com:3306/mydb?\"\n    \"ssl_ca=/path/to/ca.pem&amp;\"\n    \"ssl_verify_cert=true\"\n)\n</code></pre>"},{"location":"advanced/deployment/#scaling-strategies","title":"Scaling Strategies","text":""},{"location":"advanced/deployment/#vertical-scaling","title":"Vertical Scaling","text":"<p>Increase database server resources:</p> <ul> <li>Add more CPU/memory to database server</li> <li>Use faster storage (SSD/NVMe)</li> <li>Increase connection limits</li> </ul>"},{"location":"advanced/deployment/#when-to-use","title":"When to Use","text":"<ul> <li>For moderate traffic growth</li> <li>When application is not easily sharded</li> <li>When budget constraints limit horizontal scaling</li> </ul>"},{"location":"advanced/deployment/#horizontal-scaling","title":"Horizontal Scaling","text":""},{"location":"advanced/deployment/#read-replicas","title":"Read Replicas","text":"<pre><code>class ReadWriteRouter:\n    \"\"\"Route reads to replicas and writes to primary\"\"\"\n    def __init__(self, primary_db, replica_dbs):\n        self.primary_db = primary_db\n        self.replica_dbs = replica_dbs\n        self.current_replica = 0\n\n    async def get_read_db(self):\n        \"\"\"Get a read replica database connection\"\"\"\n        db = self.replica_dbs[self.current_replica]\n        self.current_replica = (self.current_replica + 1) % len(self.replica_dbs)\n        return db\n\n    async def get_write_db(self):\n        \"\"\"Get the primary database connection\"\"\"\n        return self.primary_db\n\n# Usage\nrouter = ReadWriteRouter(primary_db, [replica1, replica2, replica3])\n\nasync def get_book(book_id):\n    db = await router.get_read_db()\n    return await Book.objects(db=db).get(id=book_id)\n\nasync def create_book(**kwargs):\n    db = await router.get_write_db()\n    return await Book.create(db=db, **kwargs)\n</code></pre>"},{"location":"advanced/deployment/#database-sharding","title":"Database Sharding","text":"<p>Implement sharding for very large datasets:</p> <pre><code>class ShardingRouter:\n    \"\"\"Route requests to appropriate shard\"\"\"\n    def __init__(self, shards):\n        self.shards = shards  # {shard_id: Database}\n\n    def get_shard(self, user_id):\n        \"\"\"Determine which shard to use based on user_id\"\"\"\n        shard_id = user_id % len(self.shards)\n        return self.shards[shard_id]\n\n# Usage\nshards = {\n    0: Database(os.getenv(\"SHARD_0_URL\")),\n    1: Database(os.getenv(\"SHARD_1_URL\")),\n    2: Database(os.getenv(\"SHARD_2_URL\")),\n}\nrouter = ShardingRouter(shards)\n\nasync def get_user_books(user_id):\n    db = router.get_shard(user_id)\n    return await Book.objects(db=db).filter(user_id=user_id).all()\n</code></pre>"},{"location":"advanced/deployment/#cloud-deployment-strategies","title":"Cloud Deployment Strategies","text":""},{"location":"advanced/deployment/#aws-deployment","title":"AWS Deployment","text":""},{"location":"advanced/deployment/#rds-configuration","title":"RDS Configuration","text":"<pre><code># AWS RDS configuration\nimport os\n\ndb = Database(\n    f\"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}\"\n    f\"@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}\"\n    f\"/{os.getenv('DB_NAME')}?sslmode=require\"\n)\n</code></pre>"},{"location":"advanced/deployment/#lambda-with-rds-proxy","title":"Lambda with RDS Proxy","text":"<pre><code># Lambda function with RDS Proxy\nimport os\nimport ormax\nfrom ormax.utils import cached_property\n\nclass DatabaseManager:\n    \"\"\"Manage database connection in Lambda environment\"\"\"\n    _instance = None\n\n    @classmethod\n    def get_instance(cls):\n        if cls._instance is None:\n            cls._instance = cls()\n        return cls._instance\n\n    @cached_property\n    def db(self):\n        \"\"\"Get database connection with RDS Proxy\"\"\"\n        return ormax.Database(os.getenv(\"RDS_PROXY_ENDPOINT\"))\n\n    async def get_connection(self):\n        \"\"\"Get a connection from the pool\"\"\"\n        if not hasattr(self, \"_connection\") or self._connection._current_transaction is None:\n            self._connection = await self.db.transaction().__aenter__()\n        return self._connection\n\n    async def release_connection(self):\n        \"\"\"Release the connection back to the pool\"\"\"\n        if hasattr(self, \"_connection\") and self._connection._current_transaction:\n            await self._connection.__aexit__(None, None, None)\n            del self._connection\n\n# Usage in Lambda handler\nasync def lambda_handler(event, context):\n    db_manager = DatabaseManager.get_instance()\n    try:\n        db = await db_manager.get_connection()\n        # Process request\n        books = await Book.objects(db=db).all()\n        return {\"books\": [b.to_dict() for b in books]}\n    finally:\n        await db_manager.release_connection()\n</code></pre>"},{"location":"advanced/deployment/#google-cloud-deployment","title":"Google Cloud Deployment","text":""},{"location":"advanced/deployment/#cloud-sql-configuration","title":"Cloud SQL Configuration","text":"<pre><code># Google Cloud SQL configuration\nimport os\nfrom google.cloud.sql.connector import Connector\n\nasync def get_cloud_sql_connection():\n    \"\"\"Get Cloud SQL connection using Connector\"\"\"\n    connector = Connector()\n    conn = await connector.connect_async(\n        os.getenv(\"CLOUD_SQL_CONNECTION_NAME\"),\n        \"asyncpg\",\n        user=os.getenv(\"DB_USER\"),\n        password=os.getenv(\"DB_PASSWORD\"),\n        db=os.getenv(\"DB_NAME\")\n    )\n    return conn\n\n# Custom connection class for Ormax\nclass CloudSQLConnection(ormax.DatabaseConnection):\n    async def connect(self):\n        self.connection = await get_cloud_sql_connection()\n\n    # Implement other required methods...\n\n# Usage\ndb = ormax.Database(\"cloudsql://\", connection_class=CloudSQLConnection)\n</code></pre>"},{"location":"advanced/deployment/#azure-deployment","title":"Azure Deployment","text":""},{"location":"advanced/deployment/#azure-sql-database-configuration","title":"Azure SQL Database Configuration","text":"<pre><code># Azure SQL Database configuration\nimport os\nimport pyodbc\n\ndb = ormax.Database(\n    f\"mssql://\"\n    f\"{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}\"\n    f\"@{os.getenv('DB_HOST')}:1433/\"\n    f\"{os.getenv('DB_NAME')}\"\n    \"?driver=ODBC+Driver+17+for+SQL+Server\"\n    \"&amp;Encrypt=yes\"\n    \"&amp;TrustServerCertificate=no\"\n    \"&amp;Connection Timeout=30\"\n)\n</code></pre>"},{"location":"advanced/deployment/#handling-database-failures","title":"Handling Database Failures","text":""},{"location":"advanced/deployment/#retry-logic-for-transient-errors","title":"Retry Logic for Transient Errors","text":"<p>Implement retry logic for common database issues:</p> <pre><code>from ormax.utils import retry_async\n\n@retry_async(\n    max_attempts=3,\n    delay=0.1,\n    backoff=2.0,\n    exceptions=(asyncpg.exceptions.ConnectionDoesNotExistError,)\n)\nasync def get_books():\n    return await Book.objects().all()\n</code></pre>"},{"location":"advanced/deployment/#circuit-breaker-pattern","title":"Circuit Breaker Pattern","text":"<p>Prevent cascading failures with a circuit breaker:</p> <pre><code>class CircuitBreaker:\n    \"\"\"Circuit breaker for database operations\"\"\"\n    def __init__(self, failure_threshold=5, recovery_timeout=60):\n        self.failure_count = 0\n        self.failure_threshold = failure_threshold\n        self.recovery_timeout = recovery_timeout\n        self.last_failure_time = 0\n        self.open = False\n\n    def call(self, func, *args, **kwargs):\n        \"\"\"Call a function with circuit breaker protection\"\"\"\n        if self.open:\n            # Check if it's time to try again\n            if time.time() - self.last_failure_time &gt; self.recovery_timeout:\n                self.open = False\n            else:\n                raise Exception(\"Database circuit is open\")\n\n        try:\n            result = func(*args, **kwargs)\n            self.failure_count = 0\n            return result\n        except Exception as e:\n            self.failure_count += 1\n            self.last_failure_time = time.time()\n            if self.failure_count &gt;= self.failure_threshold:\n                self.open = True\n            raise\n\n# Usage\ncircuit_breaker = CircuitBreaker()\n\nasync def safe_get_books():\n    return await circuit_breaker.call(lambda: Book.objects().all())\n</code></pre>"},{"location":"advanced/deployment/#fallback-strategies","title":"Fallback Strategies","text":"<p>Implement fallback strategies for critical operations:</p> <pre><code>async def get_books_with_fallback():\n    \"\"\"Get books with fallback to cached data\"\"\"\n    try:\n        # Try to get fresh data\n        return await Book.objects().all()\n    except Exception as e:\n        # Log the error but continue with fallback\n        logger.error(f\"Database error: {e}\")\n        try:\n            # Try to get cached data\n            return await get_cached_books()\n        except:\n            # Final fallback: return empty list\n            return []\n\nasync def get_cached_books():\n    \"\"\"Get books from cache\"\"\"\n    # Implementation would use Redis or another cache\n    cached_books = await cache.get(\"books:all\")\n    if cached_books:\n        return [Book.from_dict(book) for book in cached_books]\n    return []\n</code></pre>"},{"location":"advanced/deployment/#deployment-checklists","title":"Deployment Checklists","text":""},{"location":"advanced/deployment/#pre-deployment-checklist","title":"Pre-Deployment Checklist","text":"<ul> <li>[ ] Verify database connection settings</li> <li>[ ] Check migration status</li> <li>[ ] Validate connection pool settings</li> <li>[ ] Ensure proper logging configuration</li> <li>[ ] Verify backup strategy is in place</li> <li>[ ] Test failover procedures</li> <li>[ ] Confirm monitoring is configured</li> </ul>"},{"location":"advanced/deployment/#post-deployment-checklist","title":"Post-Deployment Checklist","text":"<ul> <li>[ ] Verify application is responding</li> <li>[ ] Check database connection metrics</li> <li>[ ] Monitor error rates</li> <li>[ ] Verify slow query monitoring is working</li> <li>[ ] Confirm all features work as expected</li> <li>[ ] Test backup restoration process</li> <li>[ ] Document deployment process</li> </ul>"},{"location":"advanced/deployment/#common-deployment-issues-and-solutions","title":"Common Deployment Issues and Solutions","text":""},{"location":"advanced/deployment/#issue-connection-leaks","title":"Issue: Connection Leaks","text":"<p>Symptoms: - Gradually increasing connection count - Application becomes unresponsive - \"Too many connections\" errors</p> <p>Solutions: - Ensure all transactions are properly closed - Use context managers for database operations - Implement connection monitoring - Set appropriate <code>pool_recycle</code> value</p>"},{"location":"advanced/deployment/#issue-slow-deployments","title":"Issue: Slow Deployments","text":"<p>Symptoms: - Long deployment times - Extended downtime during releases - Failed migrations</p> <p>Solutions: - Optimize migration scripts - Use zero-downtime deployment patterns - Pre-warm database connections - Run migrations during off-peak hours</p>"},{"location":"advanced/deployment/#issue-data-inconsistency-after-deployment","title":"Issue: Data Inconsistency After Deployment","text":"<p>Symptoms: - Missing data - Inconsistent application behavior - Foreign key constraint violations</p> <p>Solutions: - Implement proper migration strategies - Use transactions for data migrations - Validate data consistency after deployment - Implement data reconciliation processes</p>"},{"location":"advanced/deployment/#real-world-deployment-examples","title":"Real-World Deployment Examples","text":""},{"location":"advanced/deployment/#e-commerce-platform-deployment","title":"E-commerce Platform Deployment","text":"<pre><code># ecom/deployment.py\nimport os\nimport asyncio\nfrom ormax import Database\nfrom .models import Product, Category, Order\n\nasync def deploy_ecommerce():\n    \"\"\"Deploy e-commerce platform with proper sequencing\"\"\"\n    # 1. Configure database based on environment\n    db = Database(os.getenv(\"DATABASE_URL\"))\n\n    # 2. Connect to database\n    await db.connect()\n\n    # 3. Register models\n    db.register_model(Product)\n    db.register_model(Category)\n    db.register_model(Order)\n\n    # 4. Run migrations\n    await run_migrations()\n\n    # 5. Verify database health\n    if not await verify_database_health():\n        raise Exception(\"Database health check failed\")\n\n    # 6. Pre-warm critical queries\n    await prewarm_queries()\n\n    # 7. Start background monitoring\n    asyncio.create_task(monitor_database_performance())\n\n    return db\n\nasync def verify_database_health():\n    \"\"\"Verify database is healthy and responsive\"\"\"\n    try:\n        # Check basic query\n        await Product.objects().limit(1).all()\n\n        # Check connection pool status\n        if hasattr(db.connection, 'pool') and hasattr(db.connection.pool, 'freesize'):\n            if db.connection.pool.freesize() &lt; 5:\n                logger.warning(\"Low connection pool availability\")\n\n        # Check for slow queries\n        slow_queries = await db.connection.fetch_all(\n            \"SELECT * FROM pg_stat_statements WHERE mean_time &gt; 100\"\n        )\n        if slow_queries:\n            logger.warning(f\"Found {len(slow_queries)} slow queries\")\n\n        return True\n    except Exception as e:\n        logger.error(f\"Database health check failed: {e}\")\n        return False\n\nasync def prewarm_queries():\n    \"\"\"Pre-warm critical queries to reduce first-user latency\"\"\"\n    # Warm up product listing\n    asyncio.create_task(Product.objects().limit(50).all())\n\n    # Warm up category tree\n    asyncio.create_task(Category.objects().all())\n\n    # Warm up popular products\n    asyncio.create_task(Product.objects().order_by(\"-popularity\").limit(20).all())\n</code></pre>"},{"location":"advanced/deployment/#high-traffic-blog-platform-deployment","title":"High-Traffic Blog Platform Deployment","text":"<pre><code># blog/deployment.py\nimport os\nimport asyncio\nfrom ormax import Database\nfrom .models import Post, Comment, User\n\nasync def deploy_blog_platform():\n    \"\"\"Deploy blog platform with read replicas and caching\"\"\"\n    # 1. Configure primary database\n    primary_db = Database(os.getenv(\"PRIMARY_DB_URL\"))\n    await primary_db.connect()\n\n    # 2. Configure read replicas\n    replica_urls = os.getenv(\"REPLICA_DB_URLS\", \"\").split(\",\")\n    replica_dbs = [Database(url) for url in replica_urls if url]\n    for db in replica_dbs:\n        await db.connect()\n\n    # 3. Setup router\n    from .router import ReadWriteRouter\n    db_router = ReadWriteRouter(primary_db, replica_dbs)\n\n    # 4. Register models with router\n    for model in [Post, Comment, User]:\n        model.set_db_router(db_router)\n\n    # 5. Run migrations on primary\n    await run_migrations(primary_db)\n\n    # 6. Setup caching\n    from .cache import setup_cache\n    await setup_cache()\n\n    # 7. Start monitoring\n    asyncio.create_task(monitor_replication_lag(primary_db, replica_dbs))\n\n    return db_router\n\nasync def monitor_replication_lag(primary_db, replica_dbs):\n    \"\"\"Monitor replication lag between primary and replicas\"\"\"\n    while True:\n        try:\n            # PostgreSQL specific replication monitoring\n            primary_position = await primary_db.connection.fetch_val(\n                \"SELECT pg_current_wal_lsn()\"\n            )\n\n            for i, replica_db in enumerate(replica_dbs):\n                replica_position = await replica_db.connection.fetch_val(\n                    \"SELECT pg_last_wal_replay_lsn()\"\n                )\n                lag = await primary_db.connection.fetch_val(\n                    \"SELECT pg_wal_lsn_diff($1, $2)\", \n                    (primary_position, replica_position)\n                )\n\n                if lag &gt; 1024 * 1024:  # 1MB lag\n                    logger.warning(\n                        f\"Replication lag on replica {i} is {lag} bytes\"\n                    )\n\n                # Trigger alert if lag is too high\n                if lag &gt; 100 * 1024 * 1024:  # 100MB lag\n                    send_replication_lag_alert(i, lag)\n        except Exception as e:\n            logger.error(f\"Error monitoring replication lag: {e}\")\n\n        # Check every 30 seconds\n        await asyncio.sleep(30)\n</code></pre>"},{"location":"advanced/deployment/#best-practices-for-production-deployment","title":"Best Practices for Production Deployment","text":""},{"location":"advanced/deployment/#use-infrastructure-as-code","title":"Use Infrastructure as Code","text":"<p>Manage your deployment with IaC tools:</p> <pre><code># terraform/main.tf\nresource \"aws_rds_cluster\" \"main\" {\n  cluster_identifier      = \"ormax-production\"\n  engine                  = \"aurora-postgresql\"\n  engine_version          = \"13.4\"\n  database_name           = \"maindb\"\n  master_username         = var.db_username\n  master_password         = var.db_password\n  backup_retention_period = 7\n  preferred_backup_window = \"07:00-09:00\"\n}\n\nresource \"aws_rds_cluster_instance\" \"instance\" {\n  count              = 2\n  cluster_identifier = aws_rds_cluster.main.id\n  instance_class     = \"db.r6g.large\"\n}\n</code></pre>"},{"location":"advanced/deployment/#implement-blue-green-deployments","title":"Implement Blue-Green Deployments","text":"<p>Minimize downtime with blue-green deployments:</p> <ol> <li>Deploy new version to \"green\" environment</li> <li>Run database migrations in a backward-compatible way</li> <li>Test green environment thoroughly</li> <li>Switch traffic from blue to green</li> <li>Decommission blue environment</li> </ol>"},{"location":"advanced/deployment/#automate-everything","title":"Automate Everything","text":"<p>Automate your deployment process:</p> <pre><code>#!/bin/bash\n# deploy.sh\nset -e\n\n# Load environment variables\nsource .env.production\n\n# Run tests\npytest tests/\n\n# Apply database migrations\npython manage.py migrate\n\n# Build and push Docker image\ndocker build -t myapp:$GIT_COMMIT .\ndocker push myapp:$GIT_COMMIT\n\n# Deploy to Kubernetes\nkubectl set image deployment/myapp myapp=myapp:$GIT_COMMIT\n\n# Verify deployment\nsleep 30\ncurl -s http://myapp.example.com/health | grep \"OK\"\n\n# Switch traffic\nkubectl apply -f canary-routing.yaml\n\necho \"Deployment successful!\"\n</code></pre>"},{"location":"advanced/deployment/#maintain-proper-backups","title":"Maintain Proper Backups","text":"<p>Ensure you have reliable backups:</p> <pre><code>async def create_backup():\n    \"\"\"Create database backup\"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    backup_file = f\"/backups/myapp_{timestamp}.sql\"\n\n    # Use pg_dump for PostgreSQL\n    if \"postgresql\" in db.connection_string:\n        await run_command(\n            f\"pg_dump -Fc {db.connection_string} -f {backup_file}\"\n        )\n    # Use mysqldump for MySQL\n    elif \"mysql\" in db.connection_string:\n        await run_command(\n            f\"mysqldump --single-transaction {db.connection_string} &gt; {backup_file}\"\n        )\n\n    # Upload to cloud storage\n    await upload_to_s3(backup_file)\n\n    # Verify backup integrity\n    if not await verify_backup(backup_file):\n        raise Exception(\"Backup verification failed\")\n\n    # Clean up old backups\n    await cleanup_old_backups()\n</code></pre>"},{"location":"advanced/deployment/#troubleshooting-deployment-issues","title":"Troubleshooting Deployment Issues","text":""},{"location":"advanced/deployment/#database-connection-failures","title":"Database Connection Failures","text":"<p>Diagnosis: - Check database URL format - Verify network connectivity - Check firewall rules - Test credentials separately</p> <p>Solution: - Use connection string validator - Implement connection retry logic - Verify DNS resolution - Check database server status</p>"},{"location":"advanced/deployment/#migration-failures","title":"Migration Failures","text":"<p>Diagnosis: - Review migration script - Check database schema - Verify permissions - Examine error logs</p> <p>Solution: - Implement transactional migrations - Add rollback procedures - Test migrations in staging - Use idempotent migration patterns</p>"},{"location":"advanced/deployment/#performance-degradation-after-deployment","title":"Performance Degradation After Deployment","text":"<p>Diagnosis: - Compare query execution plans - Check for missing indexes - Monitor resource usage - Analyze slow query logs</p> <p>Solution: - Roll back problematic changes - Add necessary indexes - Optimize slow queries - Adjust connection pool settings</p>"},{"location":"advanced/performance-monitoring/","title":"Performance Monitoring in Ormax","text":"<p>Effective performance monitoring is crucial for maintaining a responsive and scalable application. Ormax provides comprehensive tools and techniques to monitor database performance, identify bottlenecks, and optimize your application's database interactions.</p>"},{"location":"advanced/performance-monitoring/#understanding-performance-metrics","title":"Understanding Performance Metrics","text":"<p>Before diving into monitoring, it's essential to understand key performance metrics for database operations:</p>"},{"location":"advanced/performance-monitoring/#key-performance-indicators","title":"Key Performance Indicators","text":"Metric Description Target Value Query Execution Time Time taken to execute a query &lt; 100ms (simple), &lt; 500ms (complex) Connection Pool Usage Percentage of available connections in use &lt; 80% consistently Transaction Duration Time spent in a transaction As short as possible Query Count per Request Number of queries executed per HTTP request Minimize (N+1 queries are a common issue) Error Rate Percentage of failed queries &lt; 0.1%"},{"location":"advanced/performance-monitoring/#built-in-logging-system","title":"Built-in Logging System","text":"<p>Ormax includes a robust logging system that provides detailed insights into database operations.</p>"},{"location":"advanced/performance-monitoring/#configuring-logging","title":"Configuring Logging","text":"<pre><code>import logging\nfrom ormax.utils import setup_logging\n\n# Configure logging with default settings\nlogger = setup_logging(level=\"INFO\")\n\n# Or customize logging configuration\nlogger = logging.getLogger(\"ormax\")\nlogger.setLevel(logging.DEBUG)\nhandler = logging.FileHandler(\"ormax_performance.log\")\nformatter = logging.Formatter(\n    \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\n</code></pre>"},{"location":"advanced/performance-monitoring/#log-levels-and-their-meaning","title":"Log Levels and Their Meaning","text":"Level When to Use Example Use Case <code>DEBUG</code> Development and detailed diagnostics Tracking individual query execution <code>INFO</code> General operational information Connection pool status, migrations <code>WARNING</code> Potential issues Slow queries, connection timeouts <code>ERROR</code> Recoverable errors Query failures, connection issues <code>CRITICAL</code> Severe errors Database unavailable, critical failures"},{"location":"advanced/performance-monitoring/#sample-log-output","title":"Sample Log Output","text":"<pre><code>2023-10-15 14:30:45 - ormax - INFO - Database connected successfully\n2023-10-15 14:30:46 - ormax - DEBUG - Table books created without foreign keys\n2023-10-15 14:30:47 - ormax - WARNING - Slow query detected (245ms): SELECT * FROM books WHERE pages &gt; 300\n2023-10-15 14:30:48 - ormax - ERROR - Failed to add foreign key constraint fk_books_author_id: column \"author_id\" is not in the table\n</code></pre>"},{"location":"advanced/performance-monitoring/#query-performance-monitoring","title":"Query Performance Monitoring","text":""},{"location":"advanced/performance-monitoring/#tracking-query-execution-time","title":"Tracking Query Execution Time","text":"<p>Ormax provides multiple ways to track query execution time:</p>"},{"location":"advanced/performance-monitoring/#using-the-measure_time_async-decorator","title":"Using the <code>measure_time_async</code> Decorator","text":"<pre><code>from ormax.utils import measure_time_async\n\n@measure_time_async\nasync def get_popular_books():\n    return await Book.objects().filter(pages__gt=300).all()\n</code></pre> <p>This will log the execution time of the function:</p> <pre><code>2023-10-15 14:45:22 - ormax - INFO - get_popular_books executed in 187.45ms\n</code></pre>"},{"location":"advanced/performance-monitoring/#using-context-managers","title":"Using Context Managers","text":"<pre><code>from ormax.utils import Timer\n\nasync def process_books():\n    with Timer(\"Book Processing\"):\n        books = await Book.objects().all()\n        # Process books\n</code></pre>"},{"location":"advanced/performance-monitoring/#manual-timing","title":"Manual Timing","text":"<pre><code>from datetime import datetime\n\nstart_time = datetime.now()\nbooks = await Book.objects().all()\nduration = (datetime.now() - start_time).total_seconds()\nlogger.info(f\"Query executed in {duration:.4f} seconds\")\n</code></pre>"},{"location":"advanced/performance-monitoring/#identifying-slow-queries","title":"Identifying Slow Queries","text":"<p>Ormax can automatically flag slow queries:</p> <pre><code>from ormax.utils import performance_monitor\n\ndef monitor_slow_queries(func):\n    @wraps(func)\n    async def wrapper(*args, **kwargs):\n        start = datetime.now()\n        result = await func(*args, **kwargs)\n        duration = (datetime.now() - start).total_seconds()\n\n        # Record for performance analysis\n        performance_monitor.record(f\"{func.__name__}\", duration)\n\n        # Log slow queries\n        if duration &gt; 0.5:  # 500ms threshold\n            logger.warning(\n                f\"Slow query in {func.__name__}: {duration:.2f} seconds\"\n            )\n        return result\n    return wrapper\n\n@monitor_slow_queries\nasync def get_complex_data():\n    return await Book.objects().filter(pages__gt=300).all()\n</code></pre>"},{"location":"advanced/performance-monitoring/#connection-pool-monitoring","title":"Connection Pool Monitoring","text":"<p>Connection pooling is critical for database performance. Ormax provides tools to monitor and tune your connection pool.</p>"},{"location":"advanced/performance-monitoring/#viewing-connection-pool-status","title":"Viewing Connection Pool Status","text":"<pre><code>async def log_connection_pool_status():\n    \"\"\"Log current connection pool status\"\"\"\n    if hasattr(db, 'connection') and hasattr(db.connection, 'pool'):\n        if hasattr(db.connection.pool, 'size'):\n            logger.info(\n                f\"Connection pool: {db.connection.pool.size()} active, \"\n                f\"{db.connection.pool.freesize()} free, \"\n                f\"{db.connection.pool.size() - db.connection.pool.freesize()} used\"\n            )\n        elif hasattr(db.connection.pool, '_maxsize'):\n            # Different implementations have different APIs\n            logger.info(\n                f\"Connection pool: max={db.connection.pool._maxsize}, \"\n                f\"current={len(db.connection.pool._queue._get_heap())}\"\n            )\n</code></pre>"},{"location":"advanced/performance-monitoring/#monitoring-connection-usage-over-time","title":"Monitoring Connection Usage Over Time","text":"<pre><code>from ormax.utils import Timer, LRUCache\n\n# Cache to track connection usage\nconnection_usage = LRUCache(maxsize=1000)\n\nasync def monitor_connection_usage():\n    \"\"\"Monitor connection usage patterns\"\"\"\n    while True:\n        try:\n            if hasattr(db, 'connection') and hasattr(db.connection, 'pool'):\n                used = 0\n                total = 0\n\n                # Handle different pool implementations\n                if hasattr(db.connection.pool, 'size'):\n                    total = db.connection.pool.size()\n                    used = total - db.connection.pool.freesize()\n                elif hasattr(db.connection.pool, '_maxsize'):\n                    total = db.connection.pool._maxsize\n                    used = total - len(db.connection.pool._queue._get_heap())\n\n                # Record usage\n                timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                connection_usage.put(timestamp, {\"used\": used, \"total\": total})\n\n                # Log high usage\n                if used &gt; total * 0.8:  # 80% threshold\n                    logger.warning(\n                        f\"High connection usage: {used}/{total} ({used/total:.1%})\"\n                    )\n        except Exception as e:\n            logger.error(f\"Error monitoring connection pool: {e}\")\n\n        # Check every 5 seconds\n        await asyncio.sleep(5)\n</code></pre>"},{"location":"advanced/performance-monitoring/#connection-pool-tuning-parameters","title":"Connection Pool Tuning Parameters","text":"Parameter Description Default Tuning Guidance <code>min_size</code> Minimum connections in pool 5 Increase for consistent high load <code>max_size</code> Maximum connections in pool 20 Match to database server limits <code>pool_recycle</code> Time before recycling connections (seconds) 3600 Shorter for unstable networks <code>command_timeout</code> Query timeout (seconds) 60 Adjust based on query complexity <code>connect_timeout</code> Connection timeout (seconds) 10 Adjust for network conditions"},{"location":"advanced/performance-monitoring/#query-analysis-tools","title":"Query Analysis Tools","text":""},{"location":"advanced/performance-monitoring/#explain-plans","title":"EXPLAIN Plans","text":"<p>Use the <code>explain()</code> method to understand how your queries execute:</p> <pre><code># Get the execution plan for a query\nplan = await Book.objects().filter(pages__gt=300).explain()\nprint(plan)\n\n# For more detailed analysis\ndetailed_plan = await Book.objects().filter(pages__gt=300).explain(analyze=True)\n</code></pre>"},{"location":"advanced/performance-monitoring/#interpreting-explain-output","title":"Interpreting EXPLAIN Output","text":"<ul> <li>Seq Scan: Sequential table scan (potentially slow)</li> <li>Index Scan: Using an index (usually faster)</li> <li>Hash Join: Efficient join method</li> <li>Nested Loop: Can be slow with large datasets</li> <li>Sort: Sorting operation (can be expensive)</li> </ul>"},{"location":"advanced/performance-monitoring/#query-profiling","title":"Query Profiling","text":"<p>Enable query profiling for detailed timing information:</p> <pre><code># Enable query profiling\nawait db.connection.execute(\"SET SESSION STATISTICS_TIMING = ON\")\n\n# Run your queries\nbooks = await Book.objects().filter(pages__gt=300).all()\n\n# Get profiling information\nprofiles = await db.connection.fetch_all(\"SELECT * FROM pg_stat_statements\")\n</code></pre>"},{"location":"advanced/performance-monitoring/#tracking-query-counts","title":"Tracking Query Counts","text":"<p>Monitor the number of queries per operation:</p> <pre><code>from ormax.utils import setup_logging\nimport logging\n\n# Configure a special logger for query counting\nquery_logger = logging.getLogger(\"ormax.queries\")\nquery_logger.setLevel(logging.INFO)\nquery_logger.addHandler(logging.FileHandler(\"query_count.log\"))\n\ndef count_queries(func):\n    \"\"\"Decorator to count queries in a function\"\"\"\n    @wraps(func)\n    async def wrapper(*args, **kwargs):\n        original_level = logging.getLogger(\"ormax\").getEffectiveLevel()\n        logging.getLogger(\"ormax\").setLevel(logging.DEBUG)\n\n        # Clear previous logs\n        import io\n        log_stream = io.StringIO()\n        handler = logging.StreamHandler(log_stream)\n        logging.getLogger(\"ormax\").addHandler(handler)\n\n        try:\n            result = await func(*args, **kwargs)\n\n            # Count queries from logs\n            log_content = log_stream.getvalue()\n            query_count = log_content.count(\"Executing query\")\n\n            # Log the count\n            query_logger.info(\n                f\"{func.__name__}: {query_count} queries executed\"\n            )\n            return result\n        finally:\n            logging.getLogger(\"ormax\").setLevel(original_level)\n            logging.getLogger(\"ormax\").removeHandler(handler)\n\n    return wrapper\n\n@count_queries\nasync def process_books():\n    # Function that executes multiple queries\n    books = await Book.objects().all()\n    for book in books:\n        author = await book.author  # This would trigger N+1 queries\n</code></pre>"},{"location":"advanced/performance-monitoring/#monitoring-dashboard","title":"Monitoring Dashboard","text":"<p>Create a real-time monitoring dashboard for your database performance:</p> <pre><code>from fastapi import APIRouter, Depends\nfrom ormax.utils import performance_monitor\n\nrouter = APIRouter()\n\n@router.get(\"/monitoring/db\")\nasync def get_db_monitoring():\n    \"\"\"Get database performance metrics\"\"\"\n    # Connection pool status\n    pool_status = {\"max\": 0, \"used\": 0, \"free\": 0}\n    if hasattr(db, 'connection') and hasattr(db.connection, 'pool'):\n        if hasattr(db.connection.pool, 'size'):\n            pool_status[\"max\"] = db.connection.pool.size()\n            pool_status[\"used\"] = pool_status[\"max\"] - db.connection.pool.freesize()\n            pool_status[\"free\"] = db.connection.pool.freesize()\n        elif hasattr(db.connection.pool, '_maxsize'):\n            pool_status[\"max\"] = db.connection.pool._maxsize\n            pool_status[\"used\"] = len(db.connection.pool._queue._get_heap())\n            pool_status[\"free\"] = pool_status[\"max\"] - pool_status[\"used\"]\n\n    # Query performance stats\n    query_stats = {}\n    for operation in performance_monitor.metrics:\n        stats = performance_monitor.get_stats(operation)\n        if stats:\n            query_stats[operation] = {\n                \"count\": stats[\"count\"],\n                \"avg_time\": f\"{stats['average']:.4f}\",\n                \"max_time\": f\"{stats['max']:.4f}\"\n            }\n\n    # Recent slow queries\n    slow_queries = []\n    for record in logging.getLogger(\"ormax\").handlers[0].logs:\n        if \"Slow query\" in record.getMessage():\n            slow_queries.append(record.getMessage())\n\n    return {\n        \"connection_pool\": pool_status,\n        \"query_performance\": query_stats,\n        \"slow_queries\": slow_queries[-10:],  # Last 10 slow queries\n        \"timestamp\": datetime.now().isoformat()\n    }\n</code></pre>"},{"location":"advanced/performance-monitoring/#database-specific-monitoring","title":"Database-Specific Monitoring","text":""},{"location":"advanced/performance-monitoring/#postgresql-monitoring","title":"PostgreSQL Monitoring","text":"<p>PostgreSQL offers advanced monitoring capabilities:</p> <pre><code>async def monitor_postgresql():\n    \"\"\"Monitor PostgreSQL-specific metrics\"\"\"\n    # Track long-running queries\n    long_queries = await db.connection.fetch_all(\"\"\"\n        SELECT pid, now() - query_start AS duration, query \n        FROM pg_stat_activity \n        WHERE state = 'active' AND now() - query_start &gt; interval '5 seconds'\n        ORDER BY duration DESC\n    \"\"\")\n\n    if long_queries:\n        for query in long_queries:\n            logger.warning(\n                f\"Long-running query (pid={query['pid']}): \"\n                f\"{query['duration']} - {query['query'][:100]}...\"\n            )\n\n    # Track lock contention\n    locks = await db.connection.fetch_all(\"\"\"\n        SELECT \n            blocked_locks.pid AS blocked_pid,\n            blocking_locks.pid AS blocking_pid,\n            blocked_activity.query AS blocked_query,\n            blocking_activity.query AS blocking_query\n        FROM pg_catalog.pg_locks blocked_locks\n        JOIN pg_catalog.pg_stat_activity blocked_activity \n            ON blocked_activity.pid = blocked_locks.pid\n        JOIN pg_catalog.pg_locks blocking_locks \n            ON blocking_locks.locktype = blocked_locks.locktype\n            AND blocking_locks.DATABASE IS NOT DISTINCT FROM blocked_locks.DATABASE\n            AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation\n            AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page\n            AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple\n            AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid\n            AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid\n            AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid\n            AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid\n            AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid\n            AND blocking_locks.pid != blocked_locks.pid\n        JOIN pg_catalog.pg_stat_activity blocking_activity \n            ON blocking_activity.pid = blocking_locks.pid\n        WHERE NOT blocked_locks.GRANTED\n    \"\"\")\n\n    if locks:\n        for lock in locks:\n            logger.error(\n                f\"Deadlock detected: PID {lock['blocking_pid']} blocking \"\n                f\"PID {lock['blocked_pid']}\"\n            )\n</code></pre>"},{"location":"advanced/performance-monitoring/#mysql-monitoring","title":"MySQL Monitoring","text":"<p>MySQL-specific monitoring queries:</p> <pre><code>async def monitor_mysql():\n    \"\"\"Monitor MySQL-specific metrics\"\"\"\n    # Track slow queries\n    slow_queries = await db.connection.fetch_all(\"\"\"\n        SELECT \n            id, time, info \n        FROM information_schema.processlist \n        WHERE time &gt; 5\n        ORDER BY time DESC\n    \"\"\")\n\n    if slow_queries:\n        for query in slow_queries:\n            logger.warning(\n                f\"Slow MySQL query (id={query['id']}): \"\n                f\"{query['time']}s - {query['info'][:100]}...\"\n            )\n\n    # Track connection usage\n    connections = await db.connection.fetch_one(\"\"\"\n        SHOW STATUS LIKE 'Threads_connected'\n    \"\"\")\n    max_connections = await db.connection.fetch_one(\"\"\"\n        SHOW VARIABLES LIKE 'max_connections'\n    \"\"\")\n\n    if connections and max_connections:\n        usage = int(connections['Value']) / int(max_connections['Value'])\n        if usage &gt; 0.8:\n            logger.warning(\n                f\"High MySQL connection usage: \"\n                f\"{connections['Value']}/{max_connections['Value']} ({usage:.1%})\"\n            )\n</code></pre>"},{"location":"advanced/performance-monitoring/#sqlite-monitoring","title":"SQLite Monitoring","text":"<p>SQLite-specific monitoring (more limited):</p> <pre><code>async def monitor_sqlite():\n    \"\"\"Monitor SQLite-specific metrics\"\"\"\n    # Track database size\n    try:\n        import os\n        db_path = db.connection_string.replace(\"sqlite:///\", \"\")\n        size = os.path.getsize(db_path)\n        size_mb = size / (1024 * 1024)\n\n        if size_mb &gt; 100:\n            logger.warning(f\"SQLite database size: {size_mb:.2f} MB\")\n\n        # Track WAL file size\n        wal_path = db_path + \"-wal\"\n        if os.path.exists(wal_path):\n            wal_size = os.path.getsize(wal_path)\n            wal_size_mb = wal_size / (1024 * 1024)\n            if wal_size_mb &gt; 50:\n                logger.warning(f\"SQLite WAL file size: {wal_size_mb:.2f} MB\")\n    except Exception as e:\n        logger.error(f\"Error monitoring SQLite: {e}\")\n</code></pre>"},{"location":"advanced/performance-monitoring/#performance-analysis-techniques","title":"Performance Analysis Techniques","text":""},{"location":"advanced/performance-monitoring/#identifying-n1-query-problems","title":"Identifying N+1 Query Problems","text":"<p>N+1 queries are a common performance issue where a single query triggers N additional queries:</p> <pre><code>async def detect_n_plus_one():\n    \"\"\"Detect potential N+1 query patterns\"\"\"\n    # Track query patterns\n    query_patterns = {}\n\n    # Configure logger to capture queries\n    log_stream = io.StringIO()\n    handler = logging.StreamHandler(log_stream)\n    logger = logging.getLogger(\"ormax\")\n    logger.addHandler(handler)\n\n    try:\n        # Run common operations\n        books = await Book.objects().all()\n        for book in books:\n            await book.author  # This would trigger N+1 queries\n\n        # Analyze logs\n        log_content = log_stream.getvalue()\n        queries = [line for line in log_content.split('\\n') \n                  if \"Executing query\" in line]\n\n        # Look for similar queries with different parameters\n        pattern_counts = {}\n        for query in queries:\n            # Extract the base query without parameters\n            base_query = re.sub(r'\\?.*?FROM', ' ? FROM', query)\n            base_query = re.sub(r'WHERE .*? = ', 'WHERE ? = ', base_query)\n\n            pattern_counts[base_query] = pattern_counts.get(base_query, 0) + 1\n\n        # Flag potential N+1 queries\n        for pattern, count in pattern_counts.items():\n            if count &gt; 10:  # Threshold for potential N+1\n                logger.warning(\n                    f\"Potential N+1 query pattern detected ({count} occurrences): {pattern}\"\n                )\n    finally:\n        logger.removeHandler(handler)\n</code></pre>"},{"location":"advanced/performance-monitoring/#memory-usage-monitoring","title":"Memory Usage Monitoring","text":"<p>Track memory usage related to database operations:</p> <pre><code>import tracemalloc\nfrom ormax.utils import Timer\n\nasync def monitor_memory_usage():\n    \"\"\"Monitor memory usage of database operations\"\"\"\n    tracemalloc.start()\n\n    # Take initial snapshot\n    initial_snapshot = tracemalloc.take_snapshot()\n\n    # Run operation to monitor\n    with Timer(\"Memory Test\"):\n        books = await Book.objects().all()\n        # Process books to simulate real usage\n        processed = [book.title.upper() for book in books]\n\n    # Take final snapshot\n    final_snapshot = tracemalloc.take_snapshot()\n\n    # Compare snapshots\n    top_stats = final_snapshot.compare_to(initial_snapshot, 'lineno')\n\n    # Log top memory consumers\n    for stat in top_stats[:5]:\n        logger.info(f\"Memory usage: {stat}\")\n\n    # Check for memory leaks\n    if len(top_stats) &gt; 0:\n        largest_diff = top_stats[0]\n        if largest_diff.size_diff &gt; 1024 * 1024:  # 1MB threshold\n            logger.warning(\n                f\"Potential memory issue: {largest_diff.size_diff / 1024:.2f} KB \"\n                f\"in {largest_diff.traceback.format()}\"\n            )\n\n    tracemalloc.stop()\n</code></pre>"},{"location":"advanced/performance-monitoring/#alerting-and-notification-systems","title":"Alerting and Notification Systems","text":""},{"location":"advanced/performance-monitoring/#setting-up-performance-alerts","title":"Setting Up Performance Alerts","text":"<p>Create alerts for critical performance issues:</p> <pre><code>from ormax.utils import Timer\nimport smtplib\nfrom email.mime.text import MIMEText\n\nclass PerformanceMonitor:\n    \"\"\"Monitor performance and send alerts when thresholds are exceeded\"\"\"\n\n    def __init__(self):\n        self.thresholds = {\n            \"query_time\": 1.0,  # 1 second\n            \"connection_usage\": 0.9,  # 90%\n            \"error_rate\": 0.05  # 5%\n        }\n        self.alert_history = set()\n\n    def check_query_time(self, func_name, duration):\n        \"\"\"Check if query time exceeds threshold\"\"\"\n        if duration &gt; self.thresholds[\"query_time\"]:\n            alert_key = f\"query_time_{func_name}\"\n            if alert_key not in self.alert_history:\n                self.send_alert(\n                    \"High Query Time\",\n                    f\"Query {func_name} took {duration:.2f}s (threshold: {self.thresholds['query_time']}s)\"\n                )\n                self.alert_history.add(alert_key)\n                # Reset alert after 5 minutes\n                asyncio.create_task(self.reset_alert(alert_key, 300))\n\n    def check_connection_usage(self, used, total):\n        \"\"\"Check if connection usage exceeds threshold\"\"\"\n        usage = used / total if total &gt; 0 else 0\n        if usage &gt; self.thresholds[\"connection_usage\"]:\n            alert_key = \"connection_usage\"\n            if alert_key not in self.alert_history:\n                self.send_alert(\n                    \"High Connection Usage\",\n                    f\"Database connection usage: {usage:.1%} (threshold: {self.thresholds['connection_usage']:.0%})\"\n                )\n                self.alert_history.add(alert_key)\n                asyncio.create_task(self.reset_alert(alert_key, 300))\n\n    async def reset_alert(self, alert_key, delay):\n        \"\"\"Reset alert after specified delay\"\"\"\n        await asyncio.sleep(delay)\n        self.alert_history.discard(alert_key)\n\n    def send_alert(self, subject, message):\n        \"\"\"Send alert via email\"\"\"\n        try:\n            msg = MIMEText(message)\n            msg['Subject'] = f\"[ALERT] {subject}\"\n            msg['From'] = \"monitoring@yourapp.com\"\n            msg['To'] = \"admin@yourapp.com\"\n\n            with smtplib.SMTP('localhost') as server:\n                server.send_message(msg)\n            logger.info(f\"Sent alert: {subject}\")\n        except Exception as e:\n            logger.error(f\"Failed to send alert: {e}\")\n\n# Usage\nmonitor = PerformanceMonitor()\n\n@measure_time_async\nasync def get_popular_books():\n    result = await Book.objects().filter(pages__gt=300).all()\n    # Check performance after execution\n    if hasattr(get_popular_books, 'execution_time'):\n        monitor.check_query_time(\"get_popular_books\", get_popular_books.execution_time)\n    return result\n</code></pre>"},{"location":"advanced/performance-monitoring/#integrating-with-external-monitoring-services","title":"Integrating with External Monitoring Services","text":"<p>Connect to popular monitoring services:</p> <pre><code># Datadog integration\ntry:\n    from datadog import initialize, statsd\n    initialize(statsd_host=\"localhost\", statsd_port=8125)\n\n    def track_performance(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            start = time.time()\n            try:\n                return await func(*args, **kwargs)\n            finally:\n                duration = time.time() - start\n                statsd.timing(f\"ormax.query.{func.__name__}\", duration * 1000)\n                statsd.increment(f\"ormax.query.{func.__name__}.count\")\n        return wrapper\n\nexcept ImportError:\n    def track_performance(func):\n        return func\n\n# Usage\n@track_performance\nasync def get_books():\n    return await Book.objects().all()\n</code></pre>"},{"location":"advanced/performance-monitoring/#performance-optimization-checklist","title":"Performance Optimization Checklist","text":"<p>Use this checklist to maintain optimal performance:</p>"},{"location":"advanced/performance-monitoring/#daily-checks","title":"Daily Checks","text":"<ul> <li>[ ] Review slow query logs</li> <li>[ ] Check connection pool usage</li> <li>[ ] Verify no new N+1 query patterns</li> <li>[ ] Monitor error rates</li> </ul>"},{"location":"advanced/performance-monitoring/#weekly-checks","title":"Weekly Checks","text":"<ul> <li>[ ] Analyze query execution plans for critical queries</li> <li>[ ] Review index usage and effectiveness</li> <li>[ ] Check for memory usage trends</li> <li>[ ] Validate connection pool settings</li> </ul>"},{"location":"advanced/performance-monitoring/#monthly-checks","title":"Monthly Checks","text":"<ul> <li>[ ] Benchmark critical operations</li> <li>[ ] Review and optimize expensive queries</li> <li>[ ] Check database statistics and run ANALYZE/VACUUM</li> <li>[ ] Validate monitoring thresholds</li> </ul>"},{"location":"advanced/performance-monitoring/#common-performance-issues-and-solutions","title":"Common Performance Issues and Solutions","text":""},{"location":"advanced/performance-monitoring/#issue-high-connection-usage","title":"Issue: High Connection Usage","text":"<p>Symptoms: - Frequent \"Too many connections\" errors - Slow response times during peak usage - Connection timeouts</p> <p>Solutions: - Increase <code>max_size</code> in connection pool configuration - Ensure transactions are kept short - Implement connection recycling - Identify and fix connection leaks</p>"},{"location":"advanced/performance-monitoring/#issue-slow-queries","title":"Issue: Slow Queries","text":"<p>Symptoms: - Individual queries taking &gt; 500ms - High CPU usage on database server - Slow application response times</p> <p>Solutions: - Add appropriate indexes - Optimize query patterns (use <code>select_related</code>/<code>prefetch_related</code>) - Break down complex queries - Consider denormalization for critical paths</p>"},{"location":"advanced/performance-monitoring/#issue-n1-query-problem","title":"Issue: N+1 Query Problem","text":"<p>Symptoms: - Many similar queries with different parameters - Performance degrades with larger datasets - High query count per request</p> <p>Solutions: - Use <code>select_related</code> for single-valued relationships - Use <code>prefetch_related</code> for multi-valued relationships - Consider using <code>values()</code> or <code>values_list()</code> for simpler data needs - Implement batch loading patterns</p>"},{"location":"advanced/performance-monitoring/#issue-memory-pressure","title":"Issue: Memory Pressure","text":"<p>Symptoms: - High memory usage during query execution - Process restarts due to OOM (Out of Memory) - Slow garbage collection</p> <p>Solutions: - Use pagination for large result sets - Process data in batches - Use <code>only()</code> or <code>defer()</code> to load only necessary fields - Consider streaming results for very large datasets</p>"},{"location":"advanced/performance-monitoring/#best-practices-for-performance-monitoring","title":"Best Practices for Performance Monitoring","text":""},{"location":"advanced/performance-monitoring/#set-realistic-thresholds","title":"Set Realistic Thresholds","text":"<ul> <li>Base thresholds on your specific workload and requirements</li> <li>Start with conservative thresholds and adjust based on historical data</li> <li>Differentiate between development, staging, and production environments</li> </ul>"},{"location":"advanced/performance-monitoring/#monitor-in-production","title":"Monitor in Production","text":"<ul> <li>Development environment performance often differs significantly from production</li> <li>Implement monitoring early in the development cycle</li> <li>Use staging environments to simulate production loads</li> </ul>"},{"location":"advanced/performance-monitoring/#correlate-metrics","title":"Correlate Metrics","text":"<ul> <li>Don't look at metrics in isolation (e.g., query time + connection usage)</li> <li>Correlate database metrics with application metrics</li> <li>Consider the full request lifecycle</li> </ul>"},{"location":"advanced/performance-monitoring/#proactive-monitoring","title":"Proactive Monitoring","text":"<ul> <li>Set up alerts for early warning signs, not just critical issues</li> <li>Monitor trends over time to identify gradual degradation</li> <li>Implement synthetic transactions to verify performance</li> </ul>"},{"location":"advanced/performance-monitoring/#real-world-monitoring-examples","title":"Real-World Monitoring Examples","text":""},{"location":"advanced/performance-monitoring/#e-commerce-platform-monitoring","title":"E-commerce Platform Monitoring","text":"<pre><code>async def monitor_ecommerce_platform():\n    \"\"\"Comprehensive monitoring for e-commerce platform\"\"\"\n    # Check critical operations\n    checks = {\n        \"product_search\": lambda: Product.objects().search(\"laptop\"),\n        \"cart_processing\": lambda: Cart.process_open_carts(),\n        \"order_creation\": lambda: Order.create_sample_order(),\n        \"inventory_update\": lambda: Inventory.update_stock_levels()\n    }\n\n    results = {}\n    for name, func in checks.items():\n        try:\n            start = datetime.now()\n            await func()\n            duration = (datetime.now() - start).total_seconds()\n            results[name] = {\"status\": \"OK\", \"duration\": duration}\n\n            # Record for performance monitoring\n            performance_monitor.record(name, duration)\n\n            # Alert on slow performance\n            if duration &gt; 2.0:  # 2 seconds threshold\n                logger.warning(f\"Slow {name} operation: {duration:.2f}s\")\n        except Exception as e:\n            results[name] = {\"status\": \"ERROR\", \"error\": str(e)}\n            logger.error(f\"{name} check failed: {e}\")\n\n    # Check database health\n    db_health = await check_database_health()\n\n    # Generate summary\n    summary = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"operation_status\": results,\n        \"database_health\": db_health,\n        \"connection_pool\": get_connection_pool_status()\n    }\n\n    # Log summary\n    logger.info(f\"Platform health check: {json.dumps(summary)}\")\n\n    # Send alerts for critical issues\n    if any(status[\"status\"] == \"ERROR\" for status in results.values()):\n        send_critical_alert(\"E-commerce platform health check failure\", summary)\n\n    return summary\n\nasync def check_database_health():\n    \"\"\"Check database-specific health metrics\"\"\"\n    # Check replication lag (PostgreSQL)\n    if \"postgresql\" in db.connection_string:\n        lag = await db.connection.fetch_one(\n            \"SELECT pg_wal_lsn_diff(pg_current_wal_lsn(), replay_lsn) AS lag \"\n            \"FROM pg_stat_replication\"\n        )\n        return {\"replication_lag\": lag[\"lag\"] if lag else 0}\n\n    # Check other database-specific metrics\n    # ...\n\n    return {\"status\": \"OK\"}\n</code></pre>"},{"location":"advanced/performance-monitoring/#analytics-dashboard-monitoring","title":"Analytics Dashboard Monitoring","text":"<pre><code>async def monitor_analytics_dashboard():\n    \"\"\"Monitor performance of analytics dashboard queries\"\"\"\n    # Define critical analytics queries\n    analytics_queries = [\n        (\"monthly_sales\", \"SELECT * FROM monthly_sales\"),\n        (\"user_engagement\", \"SELECT * FROM user_engagement\"),\n        (\"product_trends\", \"SELECT * FROM product_trends\"),\n        (\"geographic_data\", \"SELECT * FROM geographic_data\")\n    ]\n\n    results = {}\n    for name, query in analytics_queries:\n        try:\n            start = datetime.now()\n            await db.connection.execute(query)\n            duration = (datetime.now() - start).total_seconds()\n            results[name] = {\"status\": \"OK\", \"duration\": duration}\n\n            # Record for performance monitoring\n            performance_monitor.record(f\"analytics.{name}\", duration)\n\n            # Alert on slow performance\n            if duration &gt; 5.0:  # 5 seconds threshold for analytics\n                logger.warning(f\"Slow analytics query {name}: {duration:.2f}s\")\n        except Exception as e:\n            results[name] = {\"status\": \"ERROR\", \"error\": str(e)}\n            logger.error(f\"Analytics query {name} failed: {e}\")\n\n    # Check for data freshness\n    last_update = await db.connection.fetch_one(\n        \"SELECT MAX(updated_at) as last_update FROM analytics_data\"\n    )\n    if last_update and last_update[\"last_update\"]:\n        age = (datetime.now() - last_update[\"last_update\"]).total_seconds()\n        if age &gt; 3600:  # 1 hour\n            logger.warning(f\"Analytics data is {age/60:.1f} minutes old\")\n\n    return {\n        \"timestamp\": datetime.now().isoformat(),\n        \"query_performance\": results,\n        \"data_freshness\": last_update[\"last_update\"] if last_update else None\n    }\n</code></pre>"},{"location":"advanced/performance-monitoring/#troubleshooting-performance-issues","title":"Troubleshooting Performance Issues","text":""},{"location":"advanced/performance-monitoring/#step-by-step-diagnosis","title":"Step-by-Step Diagnosis","text":"<p>When facing performance issues, follow this systematic approach:</p> <ol> <li>Identify the symptom</li> <li>Is it slow response times?</li> <li>High error rates?</li> <li> <p>Resource exhaustion?</p> </li> <li> <p>Narrow down the scope</p> </li> <li>Is it affecting all operations or specific ones?</li> <li>Is it database-specific or application-wide?</li> <li> <p>When did the issue start?</p> </li> <li> <p>Collect evidence</p> </li> <li>Query logs</li> <li>Connection pool statistics</li> <li>System resource usage (CPU, memory, I/O)</li> <li> <p>Database-specific metrics</p> </li> <li> <p>Formulate hypotheses</p> </li> <li>Missing index?</li> <li>Connection leak?</li> <li> <p>Inefficient query pattern?</p> </li> <li> <p>Test and validate</p> </li> <li>Implement potential fixes</li> <li>Measure impact</li> <li>Verify resolution</li> </ol>"},{"location":"advanced/performance-monitoring/#common-diagnostic-commands","title":"Common Diagnostic Commands","text":""},{"location":"advanced/performance-monitoring/#postgresql","title":"PostgreSQL","text":"<pre><code>-- Long-running queries\nSELECT pid, now() - query_start AS duration, query \nFROM pg_stat_activity \nWHERE state = 'active' AND now() - query_start &gt; interval '5 seconds';\n\n-- Lock contention\nSELECT * FROM pg_locks WHERE granted = false;\n\n-- Index usage\nSELECT * FROM pg_stat_user_indexes;\n</code></pre>"},{"location":"advanced/performance-monitoring/#mysql","title":"MySQL","text":"<pre><code>-- Slow queries\nSHOW FULL PROCESSLIST;\n\n-- Connection usage\nSHOW STATUS LIKE 'Threads_connected';\nSHOW VARIABLES LIKE 'max_connections';\n\n-- Query execution plans\nEXPLAIN SELECT * FROM books WHERE pages &gt; 300;\n</code></pre>"},{"location":"advanced/performance-monitoring/#sqlite","title":"SQLite","text":"<pre><code>-- Database statistics\nPRAGMA stats;\nPRAGMA index_list('books');\nPRAGMA index_info('books_title_idx');\n</code></pre>"},{"location":"advanced/query-optimization/","title":"Query Optimization in Ormax","text":"<p>Efficient database queries are critical for application performance. This guide covers advanced techniques to optimize your Ormax queries, reduce database load, and improve response times.</p>"},{"location":"advanced/query-optimization/#understanding-query-performance","title":"Understanding Query Performance","text":"<p>Before optimizing, it's essential to understand how queries perform and where bottlenecks occur.</p>"},{"location":"advanced/query-optimization/#measuring-query-performance","title":"Measuring Query Performance","text":"<p>Ormax provides built-in tools to measure query execution time:</p> <pre><code>from ormax.utils import measure_time_async\n\n@measure_time_async\nasync def get_popular_books():\n    return await Book.objects().filter(pages__gt=300).all()\n</code></pre> <p>This decorator will log the execution time of your function, helping you identify slow queries.</p>"},{"location":"advanced/query-optimization/#viewing-sql-queries","title":"Viewing SQL Queries","text":"<p>To see the actual SQL being executed:</p> <pre><code># Enable query logging\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Or configure specifically for Ormax\nlogger = logging.getLogger(\"ormax\")\nlogger.setLevel(logging.DEBUG)\n</code></pre> <p>With debug logging enabled, Ormax will print all executed queries to the console.</p>"},{"location":"advanced/query-optimization/#understanding-query-execution-plans","title":"Understanding Query Execution Plans","text":"<p>Different databases provide tools to analyze query execution:</p> <pre><code># Get the execution plan for a query\nplan = await Book.objects().filter(pages__gt=300).explain()\nprint(plan)\n</code></pre> <p>Note: The <code>explain()</code> method is database-specific and may return different formats depending on your database backend.</p>"},{"location":"advanced/query-optimization/#indexing-strategies","title":"Indexing Strategies","text":"<p>Proper indexing is the most effective way to improve query performance.</p>"},{"location":"advanced/query-optimization/#when-to-create-indexes","title":"When to Create Indexes","text":"<p>Create indexes for: - Fields used in WHERE clauses - Fields used in ORDER BY - Foreign key fields - Fields used in JOIN conditions - Fields frequently used in search operations</p>"},{"location":"advanced/query-optimization/#creating-indexes-in-ormax","title":"Creating Indexes in Ormax","text":"<p>You can define indexes in your model's <code>_meta</code>:</p> <pre><code>class Book(ormax.Model):\n    title = CharField(max_length=200)\n    pages = IntegerField()\n\n    _meta = {\n        \"indexes\": [\n            {\"fields\": [\"title\"], \"unique\": True},\n            {\"fields\": [\"pages\"], \"name\": \"pages_idx\"},\n            {\"fields\": [\"title\", \"pages\"], \"name\": \"title_pages_idx\"}\n        ]\n    }\n</code></pre>"},{"location":"advanced/query-optimization/#database-specific-index-types","title":"Database-Specific Index Types","text":"<p>Different databases support specialized index types:</p> <pre><code>class Product(ormax.Model):\n    name = CharField(max_length=200)\n    description = TextField()\n\n    _meta = {\n        \"indexes\": [\n            # PostgreSQL full-text index\n            {\"fields\": [\"name\", \"description\"], \"type\": \"GIN\", \"name\": \"product_search_idx\"}\n        ]\n    }\n</code></pre>"},{"location":"advanced/query-optimization/#managing-indexes","title":"Managing Indexes","text":"<p>Ormax provides methods to manage indexes:</p> <pre><code># Create all indexes for a model\nawait Book.create_indexes()\n\n# Create a specific index\nawait Book.create_index(\"title_idx\")\n\n# Drop an index\nawait Book.drop_index(\"title_idx\")\n</code></pre>"},{"location":"advanced/query-optimization/#optimizing-query-execution","title":"Optimizing Query Execution","text":""},{"location":"advanced/query-optimization/#using-select_related-effectively","title":"Using select_related Effectively","text":"<p><code>select_related</code> follows foreign key relationships in a single query:</p> <pre><code># BAD: N+1 queries\nbooks = await Book.objects().all()\nfor book in books:\n    print(book.author.name)  # Triggers a query for each book\n\n# GOOD: One query with JOIN\nbooks = await Book.objects().select_related(\"author\").all()\nfor book in books:\n    print(book.author.name)  # No additional queries\n</code></pre>"},{"location":"advanced/query-optimization/#best-practices-for-select_related","title":"Best Practices for select_related","text":"<ul> <li>Use for single-valued relationships (ForeignKey, OneToOne)</li> <li>Don't chain too many levels (2-3 levels max)</li> <li>Be specific about which relationships to follow:   <pre><code>books = await Book.objects().select_related(\"author__publisher\").all()\n</code></pre></li> </ul>"},{"location":"advanced/query-optimization/#using-prefetch_related-effectively","title":"Using prefetch_related Effectively","text":"<p><code>prefetch_related</code> fetches related objects in separate queries:</p> <pre><code># BAD: N+1 queries\nauthors = await Author.objects().all()\nfor author in authors:\n    for book in author.books.all():  # Triggers a query per author\n        print(book.title)\n\n# GOOD: Two queries total\nauthors = await Author.objects().prefetch_related(\"books\").all()\nfor author in authors:\n    for book in author.books.all():  # No additional queries\n        print(book.title)\n</code></pre>"},{"location":"advanced/query-optimization/#best-practices-for-prefetch_related","title":"Best Practices for prefetch_related","text":"<ul> <li>Use for multi-valued relationships (reverse ForeignKey)</li> <li>Combine with filtering:   <pre><code>authors = await Author.objects().prefetch_related(\n    Prefetch(\"books\", queryset=Book.objects().filter(pages__gt=300))\n).all()\n</code></pre></li> <li>Avoid nested prefetching unless absolutely necessary</li> </ul>"},{"location":"advanced/query-optimization/#using-only-and-defer","title":"Using only() and defer()","text":"<p>Load only the fields you need:</p> <pre><code># Load only title and pages\nbooks = await Book.objects().only(\"title\", \"pages\").all()\n\n# Accessing other fields will return None\nfor book in books:\n    print(book.title)  # Works\n    print(book.author)  # Returns None\n</code></pre> <pre><code># Load all fields except bio\nauthors = await Author.objects().defer(\"bio\").all()\n\n# Accessing bio will trigger an additional query\nfor author in authors:\n    print(author.name)  # Works\n    print(author.bio)   # Triggers additional query\n</code></pre>"},{"location":"advanced/query-optimization/#best-practices-for-only-and-defer","title":"Best Practices for only() and defer()","text":"<ul> <li>Use <code>only()</code> when you need just a few fields from a model with many fields</li> <li>Use <code>defer()</code> when you need most fields except one or two large ones</li> <li>Never use these methods with <code>create()</code> or <code>update()</code></li> </ul>"},{"location":"advanced/query-optimization/#batch-processing-large-result-sets","title":"Batch Processing Large Result Sets","text":"<p>For processing large datasets:</p> <pre><code>async def process_large_dataset():\n    # Process in batches of 100\n    async for batch in Book.objects().batch(100):\n        for book in batch:\n            # Process book\n            await process_book(book)\n</code></pre> <p>Or with manual batching:</p> <pre><code>async def process_in_batches(queryset, batch_size=100):\n    offset = 0\n    while True:\n        batch = await queryset.limit(batch_size).offset(offset).all()\n        if not batch:\n            break\n        for item in batch:\n            # Process item\n            await process_item(item)\n        offset += batch_size\n</code></pre>"},{"location":"advanced/query-optimization/#using-values-and-values_list","title":"Using values() and values_list()","text":"<p>When you only need specific field values:</p> <pre><code># Get titles and pages as dictionaries\nbook_data = await Book.objects().values(\"title\", \"pages\").all()\n\n# Get only titles as a flat list\ntitles = await Book.objects().values_list(\"title\", flat=True).all()\n</code></pre>"},{"location":"advanced/query-optimization/#performance-benefits","title":"Performance Benefits","text":"<ul> <li>Reduced memory usage</li> <li>Faster serialization</li> <li>Less network overhead</li> <li>No model instantiation overhead</li> </ul>"},{"location":"advanced/query-optimization/#advanced-query-optimization-techniques","title":"Advanced Query Optimization Techniques","text":""},{"location":"advanced/query-optimization/#query-caching","title":"Query Caching","text":"<p>Ormax provides caching utilities:</p> <pre><code>from ormax.utils import memoize_async\n\n@memoize_async(maxsize=100)\nasync def get_popular_books():\n    return await Book.objects().filter(pages__gt=300).all()\n</code></pre>"},{"location":"advanced/query-optimization/#cache-invalidation-strategies","title":"Cache Invalidation Strategies","text":"<ul> <li>Time-based expiration</li> <li>Manual invalidation on data changes</li> <li>Version-based caching</li> </ul>"},{"location":"advanced/query-optimization/#using-raw-sql-for-complex-queries","title":"Using Raw SQL for Complex Queries","text":"<p>For queries that can't be expressed with the QuerySet API:</p> <pre><code># Complex analytical query\nresults = await Book.objects().raw(\n    \"\"\"\n    SELECT author_id, AVG(pages) as avg_pages\n    FROM book\n    GROUP BY author_id\n    HAVING AVG(pages) &gt; ?\n    \"\"\",\n    (300,)\n).execute()\n</code></pre>"},{"location":"advanced/query-optimization/#optimizing-aggregation-queries","title":"Optimizing Aggregation Queries","text":"<pre><code># Efficient count\ntotal = await Book.objects().filter(pages__gt=300).count()\n\n# Optimized aggregation\nfrom ormax.query import Aggregation\navg_pages = await Aggregation.avg(Book.objects().filter(pages__gt=300), \"pages\")\n</code></pre>"},{"location":"advanced/query-optimization/#using-database-specific-features","title":"Using Database-Specific Features","text":""},{"location":"advanced/query-optimization/#postgresql-jsonb-optimization","title":"PostgreSQL JSONB Optimization","text":"<pre><code># Index JSONB field\nclass Product(ormax.Model):\n    data = JSONField()\n\n    _meta = {\n        \"indexes\": [\n            {\"fields\": [\"data\"], \"type\": \"GIN\", \"name\": \"data_idx\"}\n        ]\n    }\n\n# Query JSONB field efficiently\nproducts = await Product.objects().filter(\n    data__contains={\"color\": \"red\"}\n).all()\n</code></pre>"},{"location":"advanced/query-optimization/#mysql-full-text-search","title":"MySQL Full-Text Search","text":"<pre><code># Create full-text index\nclass Article(ormax.Model):\n    title = CharField(max_length=200)\n    content = TextField()\n\n    _meta = {\n        \"indexes\": [\n            {\"fields\": [\"title\", \"content\"], \"type\": \"FULLTEXT\", \"name\": \"search_idx\"}\n        ]\n    }\n\n# Use MATCH AGAINST for full-text search\narticles = await Article.objects().raw(\n    \"SELECT * FROM article WHERE MATCH(title, content) AGAINST (?)\",\n    (\"search term\",)\n).execute()\n</code></pre>"},{"location":"advanced/query-optimization/#common-query-performance-issues","title":"Common Query Performance Issues","text":""},{"location":"advanced/query-optimization/#the-n1-query-problem","title":"The N+1 Query Problem","text":"<p>Symptoms: - Application makes many similar queries - Performance degrades as dataset grows - Logs show repeated queries</p> <p>Solution: - Use <code>select_related</code> for single-valued relationships - Use <code>prefetch_related</code> for multi-valued relationships</p>"},{"location":"advanced/query-optimization/#unindexed-foreign-keys","title":"Unindexed Foreign Keys","text":"<p>Symptoms: - Slow JOIN operations - Slow filter operations on foreign key fields</p> <p>Solution: - Ensure all foreign key fields are indexed - Consider composite indexes for common query patterns</p>"},{"location":"advanced/query-optimization/#loading-unnecessary-data","title":"Loading Unnecessary Data","text":"<p>Symptoms: - High memory usage - Slow serialization - Large network payloads</p> <p>Solution: - Use <code>only()</code> or <code>values()</code> to load only needed fields - Avoid fetching entire related objects when not needed</p>"},{"location":"advanced/query-optimization/#inefficient-aggregation","title":"Inefficient Aggregation","text":"<p>Symptoms: - Slow count operations on large tables - Slow AVG, SUM, MAX, MIN operations</p> <p>Solution: - Use database-level aggregation instead of Python-level - Consider materialized views for frequently used aggregations</p>"},{"location":"advanced/query-optimization/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"advanced/query-optimization/#tracking-query-statistics","title":"Tracking Query Statistics","text":"<p>Ormax includes a performance monitor:</p> <pre><code>from ormax.utils import performance_monitor\n\n# Record query execution time\nstart = datetime.now()\nresults = await Book.objects().all()\nduration = (datetime.now() - start).total_seconds()\nperformance_monitor.record(\"book_query\", duration)\n\n# Get statistics\nstats = performance_monitor.get_stats(\"book_query\")\nprint(f\"Average query time: {stats['average']:.4f} seconds\")\n</code></pre>"},{"location":"advanced/query-optimization/#monitoring-long-running-queries","title":"Monitoring Long-Running Queries","text":"<p>Set up alerts for slow queries:</p> <pre><code>from ormax.utils import measure_time_async\n\ndef monitor_slow_queries(func):\n    @measure_time_async\n    async def wrapper(*args, **kwargs):\n        result = await func(*args, **kwargs)\n        # Log queries taking more than 500ms\n        if wrapper.execution_time &gt; 0.5:\n            logger.warning(\n                f\"Slow query in {func.__name__}: {wrapper.execution_time:.2f} seconds\"\n            )\n        return result\n    return wrapper\n\n@monitor_slow_queries\nasync def get_complex_data():\n    # Complex query operations\n    return await Book.objects().filter(pages__gt=300).all()\n</code></pre>"},{"location":"advanced/query-optimization/#query-optimization-checklist","title":"Query Optimization Checklist","text":"<p>Before deploying to production, verify these optimizations:</p> <ul> <li>[ ] All foreign key fields have indexes</li> <li>[ ] Frequently filtered fields have indexes</li> <li>[ ] N+1 query issues are resolved with select_related/prefetch_related</li> <li>[ ] Only necessary fields are loaded (using only()/values())</li> <li>[ ] Large datasets are processed in batches</li> <li>[ ] Complex aggregations use database-level operations</li> <li>[ ] Query execution plans have been reviewed</li> <li>[ ] Slow queries have been identified and optimized</li> </ul>"},{"location":"advanced/query-optimization/#real-world-optimization-examples","title":"Real-World Optimization Examples","text":""},{"location":"advanced/query-optimization/#e-commerce-product-listing","title":"E-commerce Product Listing","text":"<pre><code>async def get_product_list(category_id, page=1, per_page=20):\n    \"\"\"Optimized product listing with proper indexing and field selection\"\"\"\n    # Start timer for monitoring\n    start_time = datetime.now()\n\n    # Get category with minimal data\n    category = await Category.objects().only(\"id\", \"name\").get(id=category_id)\n\n    # Prefetch related data in efficient batches\n    products = await Product.objects() \\\n        .filter(category=category_id) \\\n        .select_related(\"brand\") \\\n        .prefetch_related(Prefetch(\n            \"reviews\", \n            queryset=Review.objects().order_by(\"-created_at\").limit(3)\n        )) \\\n        .only(\"id\", \"name\", \"price\", \"image_url\", \"brand_id\") \\\n        .order_by(\"-featured\", \"-created_at\") \\\n        .limit(per_page) \\\n        .offset((page-1) * per_page) \\\n        .all()\n\n    # Get total count efficiently\n    total = await Product.objects().filter(category=category_id).count()\n\n    # Record performance\n    duration = (datetime.now() - start_time).total_seconds()\n    performance_monitor.record(\"product_list\", duration)\n\n    return {\n        \"category\": category,\n        \"products\": products,\n        \"total\": total,\n        \"page\": page,\n        \"pages\": (total + per_page - 1) // per_page\n    }\n</code></pre>"},{"location":"advanced/query-optimization/#analytics-dashboard","title":"Analytics Dashboard","text":"<pre><code>async def get_dashboard_analytics():\n    \"\"\"Optimized analytics queries using database-level aggregation\"\"\"\n    # Use database aggregation for counts\n    total_books = await Book.objects().count()\n    total_authors = await Author.objects().count()\n\n    # Use raw SQL for complex analytics\n    monthly_stats = await Book.objects().raw(\n        \"\"\"\n        SELECT \n            DATE_TRUNC('month', published_date) as month,\n            COUNT(*) as book_count,\n            AVG(pages) as avg_pages\n        FROM book\n        WHERE published_date &gt;= CURRENT_DATE - INTERVAL '1 year'\n        GROUP BY month\n        ORDER BY month\n        \"\"\"\n    ).execute()\n\n    # Get popular authors with efficient query\n    popular_authors = await Author.objects() \\\n        .annotate(book_count=\"COUNT(books)\") \\\n        .order_by(\"-book_count\") \\\n        .limit(10) \\\n        .values(\"id\", \"name\", \"book_count\") \\\n        .all()\n\n    # Get category distribution\n    category_stats = await Book.objects() \\\n        .values(\"category__name\") \\\n        .annotate(count=\"COUNT(*)\") \\\n        .all()\n\n    return {\n        \"total_books\": total_books,\n        \"total_authors\": total_authors,\n        \"monthly_stats\": monthly_stats,\n        \"popular_authors\": popular_authors,\n        \"category_stats\": category_stats\n    }\n</code></pre>"},{"location":"advanced/query-optimization/#database-specific-optimization-tips","title":"Database-Specific Optimization Tips","text":""},{"location":"advanced/query-optimization/#postgresql-optimization","title":"PostgreSQL Optimization","text":"<ul> <li>Use <code>psycopg2.extras.DictCursor</code> for faster result processing</li> <li>Enable connection pooling with appropriate settings</li> <li>Use <code>pg_stat_statements</code> to identify slow queries</li> <li>Consider partial indexes for common filter patterns</li> </ul>"},{"location":"advanced/query-optimization/#mysql-optimization","title":"MySQL Optimization","text":"<ul> <li>Use <code>utf8mb4</code> character set for full Unicode support</li> <li>Configure <code>innodb_buffer_pool_size</code> appropriately</li> <li>Use <code>EXPLAIN</code> to analyze query plans</li> <li>Consider query caching for frequently executed queries</li> </ul>"},{"location":"advanced/query-optimization/#sqlite-optimization","title":"SQLite Optimization","text":"<ul> <li>Use <code>PRAGMA journal_mode=WAL</code> for better concurrency</li> <li>Use <code>PRAGMA synchronous=NORMAL</code> for better performance</li> <li>Create indexes before inserting large amounts of data</li> <li>Use transactions for bulk operations</li> </ul>"},{"location":"advanced/query-optimization/#best-practices","title":"Best Practices","text":""},{"location":"advanced/query-optimization/#write-efficient-queries-from-the-start","title":"Write Efficient Queries from the Start","text":"<ul> <li>Think about how your query will be translated to SQL</li> <li>Consider the data volume you're working with</li> <li>Profile queries during development, not just in production</li> </ul>"},{"location":"advanced/query-optimization/#balance-database-load-and-application-complexity","title":"Balance Database Load and Application Complexity","text":"<ul> <li>Sometimes it's better to do multiple simple queries than one complex one</li> <li>Consider the trade-off between query complexity and application logic</li> <li>Don't over-optimize queries that are rarely executed</li> </ul>"},{"location":"advanced/query-optimization/#monitor-in-production","title":"Monitor in Production","text":"<ul> <li>Set up query logging in production (at a reasonable level)</li> <li>Monitor slow query logs</li> <li>Use APM tools to track database performance</li> </ul>"},{"location":"advanced/query-optimization/#troubleshooting-performance-issues","title":"Troubleshooting Performance Issues","text":""},{"location":"advanced/query-optimization/#identifying-slow-queries","title":"Identifying Slow Queries","text":"<ol> <li>Enable query logging</li> <li>Look for repeated patterns</li> <li>Check for missing indexes</li> <li>Analyze query execution plans</li> </ol>"},{"location":"advanced/query-optimization/#fixing-n1-query-issues","title":"Fixing N+1 Query Issues","text":"<ol> <li>Identify the relationship causing the issue</li> <li>Determine if it's a single-valued (use select_related) or multi-valued (use prefetch_related) relationship</li> <li>Implement the appropriate optimization</li> <li>Verify with logging that the issue is resolved</li> </ol>"},{"location":"advanced/query-optimization/#resolving-lock-contention","title":"Resolving Lock Contention","text":"<ul> <li>Keep transactions short</li> <li>Access tables in consistent order</li> <li>Use appropriate isolation levels</li> <li>Implement retry logic for deadlock-prone operations</li> </ul>"},{"location":"advanced/testing/","title":"Testing Strategies for Ormax Applications","text":"<p>Thorough testing is essential for building reliable applications with Ormax ORM. This guide covers comprehensive testing strategies, patterns, and tools specifically designed for Ormax-based applications.</p>"},{"location":"advanced/testing/#testing-philosophy-with-ormax","title":"Testing Philosophy with Ormax","text":"<p>Testing Ormax applications requires a layered approach that addresses:</p> <ul> <li>Unit tests: Isolated tests of model methods and business logic</li> <li>Integration tests: Tests of database interactions and queries</li> <li>End-to-end tests: Tests of complete application workflows</li> <li>Performance tests: Tests of query efficiency and scalability</li> </ul>"},{"location":"advanced/testing/#key-testing-principles","title":"Key Testing Principles","text":"<ol> <li>Test behavior, not implementation: Focus on what your code does, not how it does it</li> <li>Test one thing at a time: Each test should verify a single behavior</li> <li>Keep tests independent: Tests should not depend on each other</li> <li>Use appropriate test doubles: Choose between mocks, stubs, and fakes based on context</li> <li>Test edge cases: Don't just test the happy path</li> </ol>"},{"location":"advanced/testing/#setting-up-a-testing-environment","title":"Setting Up a Testing Environment","text":""},{"location":"advanced/testing/#test-database-configuration","title":"Test Database Configuration","text":"<p>Always use a separate database for testing:</p> <pre><code># test_config.py\nimport os\nfrom ormax import Database\n\ndef get_test_db():\n    \"\"\"Get a database connection for testing\"\"\"\n    # Use in-memory SQLite for fast tests\n    if os.getenv(\"TEST_DB\") == \"sqlite\":\n        return Database(\"sqlite:///:memory:\")\n\n    # Use dedicated test database for PostgreSQL\n    elif os.getenv(\"TEST_DB\") == \"postgres\":\n        return Database(\"postgresql://test_user:test_password@localhost:5432/test_db\")\n\n    # Default to SQLite in-memory\n    return Database(\"sqlite:///:memory:\")\n</code></pre>"},{"location":"advanced/testing/#test-setup-and-teardown","title":"Test Setup and Teardown","text":"<p>Proper setup and cleanup is critical for reliable tests:</p> <pre><code>import pytest\nfrom ormax import Database\nfrom app.models import Book, Author\n\n@pytest.fixture\nasync def db():\n    \"\"\"Test fixture for database setup and teardown\"\"\"\n    # Create test database\n    db = get_test_db()\n    await db.connect()\n\n    # Register models\n    db.register_model(Book)\n    db.register_model(Author)\n\n    # Create tables\n    await db.create_tables()\n\n    try:\n        yield db\n    finally:\n        # Clean up\n        await db.drop_tables()\n        await db.disconnect()\n</code></pre>"},{"location":"advanced/testing/#using-transactions-for-test-isolation","title":"Using Transactions for Test Isolation","text":"<p>Use transactions to keep tests isolated and fast:</p> <pre><code>@pytest.fixture\nasync def transaction(db):\n    \"\"\"Fixture to wrap tests in a transaction that gets rolled back\"\"\"\n    async with db.transaction():\n        yield\n        # Transaction automatically rolls back when context exits\n</code></pre>"},{"location":"advanced/testing/#unit-testing-models","title":"Unit Testing Models","text":""},{"location":"advanced/testing/#testing-model-validation","title":"Testing Model Validation","text":"<pre><code>import pytest\nfrom ormax.exceptions import ValidationError\nfrom app.models import Book\n\n@pytest.mark.asyncio\nasync def test_book_validation(db):\n    \"\"\"Test book model validation\"\"\"\n    # Valid book\n    book = Book(title=\"Valid Book\", pages=200)\n    await book.save()\n\n    # Invalid page count\n    with pytest.raises(ValidationError):\n        Book(title=\"Invalid Pages\", pages=-10)\n\n    # Too long title\n    with pytest.raises(ValidationError):\n        Book(title=\"A\" * 201, pages=100)\n</code></pre>"},{"location":"advanced/testing/#testing-model-methods","title":"Testing Model Methods","text":"<pre><code>@pytest.mark.asyncio\nasync def test_book_methods(db):\n    \"\"\"Test custom model methods\"\"\"\n    book = await Book.create(title=\"Test Book\", pages=350)\n\n    # Test is_long() method\n    assert book.is_long() is True\n    book.pages = 100\n    assert book.is_long() is False\n\n    # Test days_since_publication()\n    book.published_date = datetime.now() - timedelta(days=10)\n    assert book.days_since_publication() == 10\n</code></pre>"},{"location":"advanced/testing/#testing-model-relationships","title":"Testing Model Relationships","text":"<pre><code>@pytest.mark.asyncio\nasync def test_book_author_relationship(db):\n    \"\"\"Test book-author relationship\"\"\"\n    # Create author\n    author = await Author.create(name=\"Test Author\")\n\n    # Create book with author\n    book = await Book.create(title=\"Test Book\", pages=200, author=author)\n\n    # Test forward relationship\n    assert await book.author == author\n\n    # Test reverse relationship\n    books = await author.books.all()\n    assert len(books) == 1\n    assert books[0] == book\n\n    # Test adding and removing relationships\n    new_author = await Author.create(name=\"New Author\")\n    await book.author.set(new_author)\n    assert await book.author == new_author\n</code></pre>"},{"location":"advanced/testing/#integration-testing-database-operations","title":"Integration Testing Database Operations","text":""},{"location":"advanced/testing/#testing-queryset-operations","title":"Testing QuerySet Operations","text":"<pre><code>@pytest.mark.asyncio\nasync def test_queryset_filters(db):\n    \"\"\"Test QuerySet filtering operations\"\"\"\n    # Create test data\n    await Book.create(title=\"Book 1\", pages=100)\n    await Book.create(title=\"Book 2\", pages=200)\n    await Book.create(title=\"Book 3\", pages=300)\n\n    # Test exact filter\n    books = await Book.objects().filter(pages=200).all()\n    assert len(books) == 1\n    assert books[0].title == \"Book 2\"\n\n    # Test greater than filter\n    books = await Book.objects().filter(pages__gt=250).all()\n    assert len(books) == 1\n    assert books[0].title == \"Book 3\"\n\n    # Test contains filter\n    books = await Book.objects().filter(title__contains=\"Book\").all()\n    assert len(books) == 3\n</code></pre>"},{"location":"advanced/testing/#testing-transactions","title":"Testing Transactions","text":"<pre><code>@pytest.mark.asyncio\nasync def test_transaction_rollback(db):\n    \"\"\"Test transaction rollback behavior\"\"\"\n    # Track if transaction was entered\n    transaction_entered = False\n\n    try:\n        async with db.transaction():\n            transaction_entered = True\n            await Book.create(title=\"Transaction Book\", pages=200)\n            raise ValueError(\"Force rollback\")\n    except ValueError:\n        pass\n\n    # Verify transaction rolled back\n    assert transaction_entered is True\n    books = await Book.objects().all()\n    assert len(books) == 0  # Should be empty after rollback\n\n@pytest.mark.asyncio\nasync def test_nested_transactions(db):\n    \"\"\"Test nested transaction behavior\"\"\"\n    async with db.transaction():\n        await Book.create(title=\"Outer Book\", pages=100)\n\n        try:\n            async with db.transaction():\n                await Book.create(title=\"Inner Book\", pages=200)\n                raise ValueError(\"Force inner rollback\")\n        except ValueError:\n            pass\n\n        # Outer transaction should still succeed\n        await Book.create(title=\"Another Outer Book\", pages=300)\n\n    # Verify results\n    books = await Book.objects().all()\n    assert len(books) == 2\n    assert books[0].title == \"Outer Book\"\n    assert books[1].title == \"Another Outer Book\"\n</code></pre>"},{"location":"advanced/testing/#testing-bulk-operations","title":"Testing Bulk Operations","text":"<pre><code>@pytest.mark.asyncio\nasync def test_bulk_create(db):\n    \"\"\"Test bulk create operations\"\"\"\n    # Create test data\n    books_data = [\n        {\"title\": f\"Book {i}\", \"pages\": 100 + i} for i in range(100)\n    ]\n\n    # Perform bulk create\n    created_books = await Book.bulk_create(books_data)\n\n    # Verify results\n    assert len(created_books) == 100\n    for i, book in enumerate(created_books):\n        assert book.title == f\"Book {i}\"\n        assert book.pages == 100 + i\n\n    # Verify database count\n    all_books = await Book.objects().all()\n    assert len(all_books) == 100\n</code></pre>"},{"location":"advanced/testing/#mocking-database-dependencies","title":"Mocking Database Dependencies","text":""},{"location":"advanced/testing/#mocking-at-the-connection-level","title":"Mocking at the Connection Level","text":"<pre><code>import pytest\nfrom unittest.mock import AsyncMock, MagicMock\nfrom ormax import Database\nfrom ormax.connection import DatabaseConnection\n\n@pytest.mark.asyncio\nasync def test_query_mocking():\n    \"\"\"Test queries with connection mocking\"\"\"\n    # Create a mock connection\n    mock_connection = MagicMock(spec=DatabaseConnection)\n    mock_connection.fetch_all = AsyncMock(return_value=[\n        {\"id\": 1, \"title\": \"Mock Book\", \"pages\": 200}\n    ])\n\n    # Create a mock database\n    mock_db = MagicMock()\n    mock_db.connection = mock_connection\n\n    # Test query execution\n    books = await Book.objects(db=mock_db).all()\n\n    # Verify query was executed\n    mock_connection.fetch_all.assert_called_once()\n    assert len(books) == 1\n    assert books[0].title == \"Mock Book\"\n</code></pre>"},{"location":"advanced/testing/#using-a-test-database-adapter","title":"Using a Test Database Adapter","text":"<p>Create a lightweight test adapter for faster tests:</p> <pre><code>from ormax.connection import DatabaseConnection\n\nclass TestConnection(DatabaseConnection):\n    \"\"\"In-memory database connection for testing\"\"\"\n    def __init__(self):\n        super().__init__(\"test://\")\n        self.data = {}\n        self.queries = []\n        self._placeholder = \"?\"\n\n    async def connect(self):\n        pass\n\n    async def disconnect(self):\n        pass\n\n    async def execute(self, query, params=None):\n        self.queries.append((query, params))\n        if query.strip().upper().startswith(\"INSERT\"):\n            return 1  # Simulate lastrowid\n        return 1  # Simulate rowcount\n\n    async def fetch_one(self, query, params=None):\n        self.queries.append((query, params))\n        # Simplified query parsing for testing\n        if \"WHERE\" in query:\n            where_clause = query.split(\"WHERE\")[1].strip()\n            field, value = where_clause.split(\"=\")\n            field = field.strip()\n            value = value.strip().strip(\"'\")\n            # Find matching record\n            for table_data in self.data.values():\n                for record in table_data:\n                    if str(record.get(field)) == value:\n                        return record\n        return None\n\n    async def fetch_all(self, query, params=None):\n        self.queries.append((query, params))\n        # Return all data for the first table mentioned\n        table_name = query.split()[3]  # Crude parsing for testing\n        return self.data.get(table_name, [])\n\n    def get_placeholder(self) -&gt; str:\n        return self._placeholder\n\n# Usage in tests\n@pytest.fixture\ndef test_db():\n    db = Database(\"test://\")\n    db.connection = TestConnection()\n    return db\n\n@pytest.mark.asyncio\nasync def test_with_test_adapter(test_db):\n    \"\"\"Test using the in-memory test adapter\"\"\"\n    # Setup test data\n    test_db.connection.data = {\n        \"book\": [\n            {\"id\": 1, \"title\": \"Test Book\", \"pages\": 200}\n        ]\n    }\n\n    # Run query\n    book = await Book.objects(db=test_db).get(id=1)\n\n    # Verify results\n    assert book.title == \"Test Book\"\n    assert book.pages == 200\n</code></pre>"},{"location":"advanced/testing/#testing-advanced-orm-features","title":"Testing Advanced ORM Features","text":""},{"location":"advanced/testing/#testing-custom-managers","title":"Testing Custom Managers","text":"<pre><code>@pytest.mark.asyncio\nasync def test_published_book_manager(db):\n    \"\"\"Test custom manager functionality\"\"\"\n    # Create test data\n    await Book.create(title=\"Published Book\", pages=200, published_date=datetime.now())\n    await Book.create(title=\"Unpublished Book\", pages=100, published_date=None)\n\n    # Test published manager\n    published_books = await Book.published.all()\n    assert len(published_books) == 1\n    assert published_books[0].title == \"Published Book\"\n\n    # Test recently published\n    recent_books = await Book.published.recently_published(days=1)\n    assert len(recent_books) == 1\n</code></pre>"},{"location":"advanced/testing/#testing-signals","title":"Testing Signals","text":"<pre><code>import pytest\nfrom ormax.signals import pre_save, post_save\n\n@pytest.mark.asyncio\nasync def test_book_signals(db):\n    \"\"\"Test signal functionality\"\"\"\n    # Track signal calls\n    pre_save_called = False\n    post_save_called = False\n    created_value = False\n\n    # Connect to signals\n    @pre_save.connect\n    async def book_pre_save(sender, instance, **kwargs):\n        nonlocal pre_save_called\n        pre_save_called = True\n\n    @post_save.connect\n    async def book_post_save(sender, instance, created, **kwargs):\n        nonlocal post_save_called, created_value\n        post_save_called = True\n        created_value = created\n\n    # Create a book (should trigger signals)\n    book = await Book.create(title=\"Signal Test\", pages=100)\n\n    # Verify signals were called\n    assert pre_save_called is True\n    assert post_save_called is True\n    assert created_value is True\n\n    # Update the book (should trigger post_save with created=False)\n    pre_save_called = False\n    post_save_called = False\n    book.pages = 200\n    await book.save()\n\n    # Verify signals were called\n    assert pre_save_called is True\n    assert post_save_called is True\n    assert created_value is False\n</code></pre>"},{"location":"advanced/testing/#testing-custom-querysets","title":"Testing Custom QuerySets","text":"<pre><code>@pytest.mark.asyncio\nasync def test_custom_queryset(db):\n    \"\"\"Test custom QuerySet methods\"\"\"\n    # Create test data\n    await Book.create(title=\"Book 1\", pages=100)\n    await Book.create(title=\"Book 2\", pages=200)\n    await Book.create(title=\"Book 3\", pages=300)\n\n    # Test custom methods\n    recent_books = await Book.objects().recent(days=7).all()\n    assert len(recent_books) == 3  # All books are recent in this test\n\n    top_books = await Book.objects().top_rated(limit=2)\n    assert len(top_books) == 2\n</code></pre>"},{"location":"advanced/testing/#performance-testing-orm-operations","title":"Performance Testing ORM Operations","text":""},{"location":"advanced/testing/#benchmarking-queries","title":"Benchmarking Queries","text":"<pre><code>import time\nimport pytest\nfrom ormax.utils import Timer\n\n@pytest.mark.asyncio\nasync def test_query_performance(db):\n    \"\"\"Test query performance under load\"\"\"\n    # Create large dataset\n    books_data = [\n        {\"title\": f\"Book {i}\", \"pages\": 100 + (i % 200)} \n        for i in range(1000)\n    ]\n    await Book.bulk_create(books_data)\n\n    # Test simple query performance\n    with Timer(\"Simple query\"):\n        books = await Book.objects().filter(pages__gt=300).all()\n    assert len(books) &lt; 1000  # Some books should match\n\n    # Test complex query performance\n    with Timer(\"Complex query\"):\n        books = await Book.objects() \\\n            .filter(pages__gt=150) \\\n            .order_by(\"-pages\") \\\n            .limit(50) \\\n            .all()\n    assert len(books) &lt;= 50\n\n    # Verify performance targets\n    assert Timer.get_duration(\"Simple query\") &lt; 0.1  # 100ms target\n    assert Timer.get_duration(\"Complex query\") &lt; 0.2  # 200ms target\n</code></pre>"},{"location":"advanced/testing/#testing-n1-query-prevention","title":"Testing N+1 Query Prevention","text":"<pre><code>import pytest\nfrom io import StringIO\nimport logging\n\n@pytest.mark.asyncio\nasync def test_n_plus_one_prevention(db):\n    \"\"\"Test N+1 query prevention techniques\"\"\"\n    # Setup test data\n    author = await Author.create(name=\"Test Author\")\n    for i in range(10):\n        await Book.create(title=f\"Book {i}\", pages=100, author=author)\n\n    # Configure logger to capture queries\n    log_stream = StringIO()\n    handler = logging.StreamHandler(log_stream)\n    logger = logging.getLogger(\"ormax\")\n    logger.addHandler(handler)\n\n    try:\n        # Test without select_related (should trigger N+1)\n        books = await Book.objects().all()\n        for book in books:\n            await book.author  # This would trigger N+1 queries\n\n        # Count queries\n        log_content = log_stream.getvalue()\n        query_count = log_content.count(\"Executing query\")\n\n        # Should have more than 10 queries (1 + 10)\n        assert query_count &gt; 10\n\n        # Reset logger\n        log_stream = StringIO()\n\n        # Test with select_related (should prevent N+1)\n        books = await Book.objects().select_related(\"author\").all()\n        for book in books:\n            await book.author  # No additional query\n\n        # Count queries\n        log_content = log_stream.getvalue()\n        query_count = log_content.count(\"Executing query\")\n\n        # Should have only 1 query\n        assert query_count == 1\n    finally:\n        logger.removeHandler(handler)\n</code></pre>"},{"location":"advanced/testing/#testing-database-migrations","title":"Testing Database Migrations","text":""},{"location":"advanced/testing/#testing-migration-scripts","title":"Testing Migration Scripts","text":"<pre><code>import pytest\nfrom app.migrations import run_migrations, get_applied_migrations\n\n@pytest.mark.asyncio\nasync def test_migration_application(db):\n    \"\"\"Test migration application process\"\"\"\n    # Clear applied migrations\n    async with db.transaction():\n        await db.connection.execute(\"DELETE FROM migrations\")\n\n    # Run migrations\n    await run_migrations()\n\n    # Verify migrations were applied\n    applied = await get_applied_migrations()\n    assert len(applied) &gt; 0\n\n    # Verify schema changes\n    # Check for expected columns\n    columns = await db.connection.fetch_all(\n        \"PRAGMA table_info(books)\"\n    )\n    column_names = [col[\"name\"] for col in columns]\n    assert \"pages\" in column_names\n    assert \"published_date\" in column_names\n\n@pytest.mark.asyncio\nasync def test_migration_rollback(db):\n    \"\"\"Test migration rollback process\"\"\"\n    # Run migrations\n    await run_migrations()\n\n    # Get the last migration\n    applied = await get_applied_migrations()\n    last_migration = applied[-1]\n\n    # Create a rollback script\n    rollback_script = f\"\"\"\n    async def down(db):\n        await db.connection.execute(\"ALTER TABLE books DROP COLUMN pages\")\n    \"\"\"\n\n    # Save rollback script\n    with open(f\"migrations/{last_migration}_rollback.py\", \"w\") as f:\n        f.write(rollback_script)\n\n    # Run rollback\n    from app.migrations import run_rollback\n    await run_rollback(last_migration)\n\n    # Verify column was dropped\n    columns = await db.connection.fetch_all(\n        \"PRAGMA table_info(books)\"\n    )\n    column_names = [col[\"name\"] for col in columns]\n    assert \"pages\" not in column_names\n</code></pre>"},{"location":"advanced/testing/#testing-with-multiple-database-backends","title":"Testing with Multiple Database Backends","text":""},{"location":"advanced/testing/#parameterized-database-tests","title":"Parameterized Database Tests","text":"<pre><code>import pytest\nfrom app.models import Book\nfrom app.config import get_db\n\n# List of database configurations to test\nDB_BACKENDS = [\n    {\"name\": \"SQLite\", \"url\": \"sqlite:///:memory:\"},\n    {\"name\": \"PostgreSQL\", \"url\": \"postgresql://test:test@localhost:5432/test\"},\n]\n\n@pytest.mark.parametrize(\"db_config\", DB_BACKENDS, ids=lambda x: x[\"name\"])\n@pytest.mark.asyncio\nasync def test_cross_database(db_config):\n    \"\"\"Test functionality across different database backends\"\"\"\n    # Setup database\n    db = get_db(db_config[\"url\"])\n    await db.connect()\n    db.register_model(Book)\n    await db.create_tables()\n\n    try:\n        # Run identical tests on each database\n        await Book.create(title=\"Cross-DB Test\", pages=200)\n        book = await Book.objects().get(title=\"Cross-DB Test\")\n        assert book.pages == 200\n\n        # Test database-specific features\n        if \"sqlite\" in db_config[\"url\"]:\n            # SQLite-specific test\n            result = await db.connection.fetch_one(\n                \"PRAGMA foreign_keys\"\n            )\n            assert result[\"foreign_keys\"] == 1\n        elif \"postgresql\" in db_config[\"url\"]:\n            # PostgreSQL-specific test\n            result = await db.connection.fetch_one(\n                \"SHOW server_version\"\n            )\n            assert \"PostgreSQL\" in result[\"server_version\"]\n    finally:\n        await db.drop_tables()\n        await db.disconnect()\n</code></pre>"},{"location":"advanced/testing/#best-practices-for-testing-ormax-applications","title":"Best Practices for Testing Ormax Applications","text":""},{"location":"advanced/testing/#use-fixtures-for-common-setup","title":"Use Fixtures for Common Setup","text":"<pre><code>@pytest.fixture\nasync def author(db):\n    \"\"\"Fixture to create a test author\"\"\"\n    return await Author.create(name=\"Test Author\", bio=\"Test bio\")\n\n@pytest.fixture\nasync def book(db, author):\n    \"\"\"Fixture to create a test book with author\"\"\"\n    return await Book.create(\n        title=\"Test Book\", \n        pages=200, \n        author=author,\n        published_date=datetime.now()\n    )\n\n# Usage in tests\n@pytest.mark.asyncio\nasync def test_book_with_fixtures(book):\n    assert book.title == \"Test Book\"\n    assert await book.author.name == \"Test Author\"\n</code></pre>"},{"location":"advanced/testing/#test-edge-cases","title":"Test Edge Cases","text":"<pre><code>@pytest.mark.asyncio\nasync def test_edge_cases(db):\n    \"\"\"Test various edge cases\"\"\"\n    # Empty string\n    with pytest.raises(ValidationError):\n        Book(title=\"\", pages=100)\n\n    # Maximum length\n    book = Book(title=\"A\" * 200, pages=100)\n    await book.save()\n\n    # Null values where allowed\n    book = Book(title=\"Null Test\", pages=None)\n    await book.save()\n\n    # Boundary values\n    book = Book(title=\"Boundary\", pages=1)\n    await book.save()\n    book = Book(title=\"Boundary\", pages=100000)\n    await book.save()\n</code></pre>"},{"location":"advanced/testing/#test-error-handling","title":"Test Error Handling","text":"<pre><code>@pytest.mark.asyncio\nasync def test_error_handling(db):\n    \"\"\"Test proper error handling\"\"\"\n    # Non-existent record\n    with pytest.raises(Book.DoesNotExist):\n        await Book.objects().get(id=99999)\n\n    # Multiple objects returned\n    await Book.create(title=\"Duplicate\", pages=100)\n    await Book.create(title=\"Duplicate\", pages=200)\n    with pytest.raises(DatabaseError):\n        await Book.objects().get(title=\"Duplicate\")\n\n    # Database constraint violation\n    # (Assuming unique constraint on title)\n    await Book.create(title=\"Unique\", pages=100)\n    with pytest.raises(DatabaseError):\n        await Book.create(title=\"Unique\", pages=200)\n</code></pre>"},{"location":"advanced/testing/#testing-anti-patterns-to-avoid","title":"Testing Anti-Patterns to Avoid","text":""},{"location":"advanced/testing/#avoid-testing-implementation-details","title":"Avoid Testing Implementation Details","text":"<p>BAD: Testing specific SQL queries</p> <pre><code># BAD: Testing specific SQL generation\ndef test_book_query():\n    qs = Book.objects().filter(title=\"Test\")\n    assert \"WHERE title = ?\" in qs._build_select_query()\n</code></pre> <p>GOOD: Testing behavior instead</p> <pre><code># GOOD: Testing behavior\n@pytest.mark.asyncio\nasync def test_book_filtering(db):\n    await Book.create(title=\"Test Book\", pages=100)\n    books = await Book.objects().filter(title=\"Test Book\").all()\n    assert len(books) == 1\n</code></pre>"},{"location":"advanced/testing/#avoid-over-mocking","title":"Avoid Over-Mocking","text":"<p>BAD: Mocking too much of the ORM</p> <pre><code># BAD: Over-mocking\ndef test_book_creation():\n    with patch(\"ormax.Model.save\", return_value=None):\n        with patch(\"ormax.Model.__init__\", return_value=None):\n            book = Book(title=\"Test\", pages=100)\n            assert book.title == \"Test\"\n</code></pre> <p>GOOD: Testing with a real database connection</p> <pre><code># GOOD: Using a real test database\n@pytest.mark.asyncio\nasync def test_book_creation(db):\n    book = await Book.create(title=\"Test\", pages=100)\n    assert book.id is not None\n    assert book.title == \"Test\"\n</code></pre>"},{"location":"advanced/testing/#avoid-slow-tests","title":"Avoid Slow Tests","text":"<p>BAD: Creating large datasets in every test</p> <pre><code># BAD: Creating 10,000 records in every test\n@pytest.mark.asyncio\nasync def test_book_search(db):\n    # Creates 10,000 records - too slow for unit test\n    for i in range(10000):\n        await Book.create(title=f\"Book {i}\", pages=i)\n    # Test logic...\n</code></pre> <p>GOOD: Using minimal test data</p> <pre><code># GOOD: Using minimal test data\n@pytest.mark.asyncio\nasync def test_book_search(db):\n    # Create just enough data for the test\n    await Book.create(title=\"Searchable Book\", pages=100)\n    await Book.create(title=\"Another Book\", pages=200)\n\n    # Test search\n    results = await Book.objects().filter(title__contains=\"Searchable\").all()\n    assert len(results) == 1\n</code></pre>"},{"location":"advanced/testing/#testing-pyramid-for-ormax-applications","title":"Testing Pyramid for Ormax Applications","text":"<p>A well-structured testing strategy follows the testing pyramid:</p> <pre><code>          Performance Tests (Few)\n                /    \\\n               /      \\\nIntegration Tests (Medium)\n             |\n             |\n        Unit Tests (Many)\n</code></pre>"},{"location":"advanced/testing/#unit-tests-70-of-tests","title":"Unit Tests (70% of tests)","text":"<ul> <li>Test individual model methods</li> <li>Test validation logic</li> <li>Test simple business rules</li> <li>Fast, isolated, no database required (or in-memory)</li> </ul>"},{"location":"advanced/testing/#integration-tests-25-of-tests","title":"Integration Tests (25% of tests)","text":"<ul> <li>Test database queries</li> <li>Test relationships</li> <li>Test transactions</li> <li>Test with real database connection</li> </ul>"},{"location":"advanced/testing/#performance-tests-5-of-tests","title":"Performance Tests (5% of tests)","text":"<ul> <li>Test query performance</li> <li>Test bulk operations</li> <li>Test under load</li> <li>Measure execution time and resource usage</li> </ul>"},{"location":"advanced/testing/#real-world-testing-examples","title":"Real-World Testing Examples","text":""},{"location":"advanced/testing/#e-commerce-product-testing","title":"E-commerce Product Testing","text":"<pre><code>import pytest\nfrom decimal import Decimal\nfrom app.models import Product, Category, Inventory\n\n@pytest.mark.asyncio\nasync def test_product_inventory_management(db):\n    \"\"\"Test product inventory management flow\"\"\"\n    # Create test data\n    category = await Category.create(name=\"Electronics\")\n    product = await Product.create(\n        name=\"Smartphone\",\n        price=Decimal(\"599.99\"),\n        stock=100,\n        category=category\n    )\n\n    # Test inventory reduction\n    assert await product.reserve_stock(10) is True\n    assert product.stock == 90\n\n    # Test insufficient inventory\n    assert await product.reserve_stock(100) is False\n    assert product.stock == 90\n\n    # Test inventory restoration\n    await product.restore_stock(5)\n    assert product.stock == 95\n\n    # Test concurrent inventory updates\n    async def reduce_stock():\n        await product.reserve_stock(1)\n\n    # Run multiple concurrent updates\n    tasks = [reduce_stock() for _ in range(10)]\n    await asyncio.gather(*tasks)\n\n    # Verify final stock count\n    assert product.stock == 85\n\n@pytest.mark.asyncio\nasync def test_product_search(db):\n    \"\"\"Test product search functionality\"\"\"\n    # Create test data\n    await Product.create(name=\"iPhone 13\", price=Decimal(\"999.99\"), stock=10)\n    await Product.create(name=\"Samsung Galaxy\", price=Decimal(\"899.99\"), stock=5)\n    await Product.create(name=\"Google Pixel\", price=Decimal(\"699.99\"), stock=15)\n\n    # Test basic search\n    results = await Product.objects().search(\"phone\").all()\n    assert len(results) == 3\n\n    # Test price range filter\n    results = await Product.objects().filter(\n        price__gte=Decimal(\"700.00\"),\n        price__lte=Decimal(\"1000.00\")\n    ).all()\n    assert len(results) == 2\n\n    # Test category filter\n    electronics = await Category.create(name=\"Electronics\")\n    for product in results:\n        product.category = electronics\n        await product.save()\n\n    results = await Product.objects().filter(category=electronics).all()\n    assert len(results) == 2\n</code></pre>"},{"location":"advanced/testing/#blog-system-testing","title":"Blog System Testing","text":"<pre><code>import pytest\nfrom app.models import Post, Comment, User\n\n@pytest.mark.asyncio\nasync def test_post_comment_workflow(db):\n    \"\"\"Test complete post and comment workflow\"\"\"\n    # Create test data\n    user = await User.create(username=\"testuser\", email=\"test@example.com\")\n    post = await Post.create(\n        title=\"Test Post\",\n        content=\"This is a test post\",\n        author=user\n    )\n\n    # Test comment creation\n    comment = await Comment.create(\n        post=post,\n        author=user,\n        content=\"This is a test comment\"\n    )\n\n    # Test nested comments\n    reply = await Comment.create(\n        post=post,\n        author=user,\n        content=\"This is a reply\",\n        parent=comment\n    )\n\n    # Test comment retrieval\n    comments = await post.comments.all()\n    assert len(comments) == 1\n    assert comments[0].content == \"This is a test comment\"\n\n    # Test nested comment retrieval\n    nested_comments = await comments[0].replies.all()\n    assert len(nested_comments) == 1\n    assert nested_comments[0].content == \"This is a reply\"\n\n@pytest.mark.asyncio\nasync def test_post_publishing_workflow(db):\n    \"\"\"Test post publishing workflow with signals\"\"\"\n    # Track signal calls\n    post_published_called = False\n\n    # Connect to custom signal\n    @post_published.connect\n    async def handle_post_published(sender, instance, **kwargs):\n        nonlocal post_published_called\n        post_published_called = True\n\n    # Create draft post\n    user = await User.create(username=\"testuser\", email=\"test@example.com\")\n    post = await Post.create(\n        title=\"Draft Post\",\n        content=\"This is a draft post\",\n        author=user,\n        is_published=False\n    )\n\n    # Publish the post\n    post.is_published = True\n    await post.save()\n\n    # Verify signal was called\n    assert post_published_called is True\n\n    # Verify post is now published\n    published_post = await Post.objects().get(id=post.id)\n    assert published_post.is_published is True\n</code></pre>"},{"location":"advanced/testing/#continuous-integration-setup","title":"Continuous Integration Setup","text":""},{"location":"advanced/testing/#github-actions-example","title":"GitHub Actions Example","text":"<pre><code># .github/workflows/test.yml\nname: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    services:\n      postgres:\n        image: postgres:13\n        env:\n          POSTGRES_USER: test\n          POSTGRES_PASSWORD: test\n          POSTGRES_DB: test_db\n        ports:\n          - 5432:5432\n        options: &gt;-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n\n    steps:\n    - uses: actions/checkout@v2\n\n    - name: Set up Python\n      uses: actions/setup-python@v2\n      with:\n        python-version: '3.9'\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n        pip install -r requirements-dev.txt\n\n    - name: Run tests with SQLite\n      run: pytest tests/ --cov=app\n\n    - name: Run tests with PostgreSQL\n      env:\n        DATABASE_URL: postgresql://test:test@localhost:5432/test_db\n      run: pytest tests/ --db=postgres\n\n    - name: Upload coverage\n      uses: codecov/codecov-action@v2\n</code></pre>"},{"location":"advanced/testing/#docker-based-testing","title":"Docker-Based Testing","text":"<pre><code>#!/bin/bash\n# test.sh\nset -e\n\n# Start database containers\ndocker-compose up -d db-postgres db-mysql\n\n# Run tests against PostgreSQL\nDATABASE_URL=\"postgresql://test:test@localhost:5432/test_db\" pytest tests/ --db=postgres\n\n# Run tests against MySQL\nDATABASE_URL=\"mysql://test:test@localhost:3306/test_db\" pytest tests/ --db=mysql\n\n# Run performance tests\npytest tests/performance/ --performance\n\n# Stop containers\ndocker-compose down\n</code></pre>"},{"location":"advanced/testing/#troubleshooting-common-testing-issues","title":"Troubleshooting Common Testing Issues","text":""},{"location":"advanced/testing/#issue-tests-fail-with-database-connection-errors","title":"Issue: Tests Fail with Database Connection Errors","text":"<p>Symptoms: - <code>DatabaseError: Failed to connect to database</code> - Connection timeout errors - Authentication failures</p> <p>Solutions: - Verify test database is running - Check database credentials in test configuration - Ensure database container is ready before tests start - Add retry logic for database connections</p>"},{"location":"advanced/testing/#issue-tests-fail-due-to-foreign-key-constraints","title":"Issue: Tests Fail Due to Foreign Key Constraints","text":"<p>Symptoms: - <code>IntegrityError: FOREIGN KEY constraint failed</code> - Tests fail when deleting objects</p> <p>Solutions: - Use transactions that roll back after each test - Delete objects in the correct order (child before parent) - Disable foreign key checks during teardown (database-specific)</p>"},{"location":"advanced/testing/#issue-tests-are-too-slow","title":"Issue: Tests Are Too Slow","text":"<p>Symptoms: - Long test execution time - CI pipeline takes too long - Developers avoid running tests</p> <p>Solutions: - Use in-memory SQLite for unit tests - Minimize test data size - Avoid heavy setup in fixtures - Use mocking for external dependencies - Run tests in parallel</p>"},{"location":"advanced/testing/#issue-flaky-tests","title":"Issue: Flaky Tests","text":"<p>Symptoms: - Tests pass sometimes and fail other times - Failures seem random - Hard to reproduce locally</p> <p>Solutions: - Check for race conditions in async code - Ensure proper isolation between tests - Avoid shared state between tests - Add proper cleanup in teardown - Increase timeouts for slow operations</p>"},{"location":"advanced/transactions/","title":"Transaction Management in Ormax","text":"<p>Transactions are essential for maintaining data integrity in database applications. Ormax provides robust transaction management with support for nested transactions, savepoints, and multiple database backends.</p>"},{"location":"advanced/transactions/#understanding-transactions","title":"Understanding Transactions","text":"<p>A database transaction is a sequence of operations performed as a single logical unit of work. Transactions ensure that either all operations succeed or none do, maintaining database consistency.</p>"},{"location":"advanced/transactions/#acid-properties","title":"ACID Properties","text":"<p>Ormax transactions maintain the ACID properties:</p> <ul> <li>Atomicity: All operations in a transaction succeed or fail together</li> <li>Consistency: Transactions bring the database from one valid state to another</li> <li>Isolation: Concurrent transactions don't interfere with each other</li> <li>Durability: Once committed, changes persist even after system failure</li> </ul>"},{"location":"advanced/transactions/#basic-transaction-usage","title":"Basic Transaction Usage","text":"<p>The simplest way to use transactions in Ormax is with the <code>transaction()</code> context manager:</p> <pre><code>async with db.transaction():\n    # All operations within this block are part of a transaction\n    book = await Book.create(title=\"Transaction Book\", pages=200)\n    author = await Author.create(name=\"Transaction Author\")\n\n    # If an exception occurs, the transaction will be rolled back automatically\n</code></pre>"},{"location":"advanced/transactions/#why-use-transactions","title":"Why Use Transactions?","text":"<p>Without transactions:</p> <pre><code># Without transaction\nbook = await Book.create(title=\"Book 1\", pages=200)\n# If exception occurs here, book is created but author isn't\nauthor = await Author.create(name=\"Author 1\")\n</code></pre> <p>With transactions:</p> <pre><code>async with db.transaction():\n    book = await Book.create(title=\"Book 1\", pages=200)\n    author = await Author.create(name=\"Author 1\")\n    # Either both succeed or both fail\n</code></pre>"},{"location":"advanced/transactions/#nested-transactions-with-savepoints","title":"Nested Transactions with Savepoints","text":"<p>Ormax supports nested transactions using savepoints, which is particularly useful for complex business logic.</p>"},{"location":"advanced/transactions/#how-nested-transactions-work","title":"How Nested Transactions Work","text":"<pre><code>async with db.transaction():\n    # Outer transaction\n    await Book.create(title=\"Outer Book\", pages=100)\n\n    try:\n        async with db.transaction():\n            # Inner transaction (savepoint)\n            await Book.create(title=\"Inner Book\", pages=200)\n            # This will roll back only the inner transaction\n            raise ValueError(\"Something went wrong\")\n    except ValueError:\n        pass\n\n    # Outer transaction still succeeds\n    await Book.create(title=\"Another Outer Book\", pages=300)\n</code></pre>"},{"location":"advanced/transactions/#savepoint-behavior-by-database","title":"Savepoint Behavior by Database","text":"Database Nested Transaction Support Savepoint Syntax PostgreSQL Full support <code>SAVEPOINT name</code>, <code>RELEASE SAVEPOINT name</code>, <code>ROLLBACK TO name</code> MySQL Full support <code>SAVEPOINT name</code>, <code>RELEASE SAVEPOINT name</code>, <code>ROLLBACK TO name</code> SQLite Full support <code>SAVEPOINT name</code>, <code>RELEASE name</code>, <code>ROLLBACK TO name</code> MSSQL Full support <code>SAVE TRANSACTION name</code>, <code>ROLLBACK TRANSACTION name</code> Oracle Limited support <code>SAVEPOINT name</code>, <code>ROLLBACK TO name</code> Aurora Full support (MySQL-compatible) Same as MySQL"},{"location":"advanced/transactions/#manual-transaction-control","title":"Manual Transaction Control","text":"<p>For more complex scenarios, you can manage transactions manually:</p> <pre><code># Start transaction manually\ntransaction = await db.transaction().__aenter__()\n\ntry:\n    await Book.create(title=\"Manual Transaction Book\", pages=250)\n    # Commit if everything is good\n    await transaction.__aexit__(None, None, None)\nexcept Exception as e:\n    # Rollback on error\n    await transaction.__aexit__(type(e), e, None)\n    raise\n</code></pre>"},{"location":"advanced/transactions/#transaction-state-checking","title":"Transaction State Checking","text":"<p>You can check if you're currently in a transaction:</p> <pre><code>is_in_transaction = db._current_transaction is not None\ntransaction_level = getattr(db, '_transaction_level', 0)\n</code></pre>"},{"location":"advanced/transactions/#transaction-isolation-levels","title":"Transaction Isolation Levels","text":"<p>Ormax allows you to set transaction isolation levels for specific needs:</p> <pre><code>async with db.transaction(isolation_level=\"SERIALIZABLE\"):\n    # Highly isolated transaction\n    await process_critical_data()\n</code></pre>"},{"location":"advanced/transactions/#supported-isolation-levels","title":"Supported Isolation Levels","text":"Level Description Use Case <code>READ UNCOMMITTED</code> Can see uncommitted changes from other transactions Rarely used, highest performance <code>READ COMMITTED</code> Can only see committed changes (default for most databases) General purpose <code>REPEATABLE READ</code> Ensures consistent reads within a transaction When consistent reads are critical <code>SERIALIZABLE</code> Complete isolation, prevents phantom reads Critical financial operations <p>Note: Not all databases support all isolation levels. Ormax will use the closest available level for your database.</p>"},{"location":"advanced/transactions/#advanced-transaction-patterns","title":"Advanced Transaction Patterns","text":""},{"location":"advanced/transactions/#transaction-with-retry-logic","title":"Transaction with Retry Logic","text":"<p>For handling transient errors like deadlocks:</p> <pre><code>from ormax.utils import retry_async\n\n@retry_async(max_attempts=3, delay=0.1, backoff=2.0)\nasync def create_order_with_retry(user, items):\n    async with db.transaction():\n        # Create order and process items\n        order = await Order.create(user=user)\n        for item in items:\n            await OrderItem.create(order=order, item=item, quantity=item.quantity)\n        return order\n</code></pre>"},{"location":"advanced/transactions/#transaction-with-time-limit","title":"Transaction with Time Limit","text":"<p>Set a timeout for transactions to prevent long-running operations:</p> <pre><code>from ormax.utils import timeout_async\n\n@timeout_async(seconds=5)\nasync def process_payment():\n    async with db.transaction():\n        # Payment processing logic\n        await Payment.process()\n</code></pre>"},{"location":"advanced/transactions/#savepoint-based-partial-rollbacks","title":"Savepoint-Based Partial Rollbacks","text":"<p>Use savepoints for granular control within a transaction:</p> <pre><code>async with db.transaction():\n    # Initial operation\n    await create_invoice()\n\n    # Create a savepoint\n    sp1 = await db.transaction().__aenter__()\n\n    try:\n        # Critical operation that might fail\n        await process_payment()\n    except PaymentError:\n        # Roll back just the payment processing\n        await sp1.__aexit__(Exception, None, None)\n        # Continue with alternative payment method\n        await process_alternative_payment()\n\n    # Continue with the rest of the transaction\n    await send_confirmation_email()\n</code></pre>"},{"location":"advanced/transactions/#working-with-model-transactions","title":"Working with Model Transactions","text":""},{"location":"advanced/transactions/#transaction-aware-model-methods","title":"Transaction-Aware Model Methods","text":"<p>Some model methods automatically participate in transactions:</p> <pre><code># These operations will be part of the current transaction\nasync with db.transaction():\n    book = await Book.create(title=\"Transactional Book\", pages=300)\n    await book.update(pages=350)\n    await book.delete()\n</code></pre>"},{"location":"advanced/transactions/#bulk-operations-in-transactions","title":"Bulk Operations in Transactions","text":"<p>Perform bulk operations within a single transaction for performance:</p> <pre><code>async with db.transaction():\n    books_data = [\n        {\"title\": f\"Book {i}\", \"pages\": 100 + i} for i in range(1000)\n    ]\n    await Book.bulk_create(books_data)\n</code></pre>"},{"location":"advanced/transactions/#database-specific-transaction-features","title":"Database-Specific Transaction Features","text":""},{"location":"advanced/transactions/#postgresql-advisory-locks","title":"PostgreSQL Advisory Locks","text":"<p>Use PostgreSQL's advisory locks within transactions:</p> <pre><code>async with db.transaction():\n    # Acquire an advisory lock\n    await db.connection.execute(\"SELECT pg_advisory_xact_lock(12345)\")\n    # Perform operations that require exclusive access\n    await process_exclusive_operation()\n</code></pre>"},{"location":"advanced/transactions/#mysql-locking-reads","title":"MySQL Locking Reads","text":"<p>Use MySQL's locking reads within transactions:</p> <pre><code>async with db.transaction():\n    # Lock rows for update\n    books = await Book.objects().filter(pages__gt=300).for_update().all()\n    # Now update these books without interference\n    for book in books:\n        book.pages -= 10\n        await book.save()\n</code></pre>"},{"location":"advanced/transactions/#error-handling-and-recovery","title":"Error Handling and Recovery","text":""},{"location":"advanced/transactions/#handling-transaction-errors","title":"Handling Transaction Errors","text":"<p>Proper error handling is critical for transaction management:</p> <pre><code>try:\n    async with db.transaction():\n        # Transaction operations\n        await process_data()\nexcept DatabaseError as e:\n    # Handle specific database errors\n    logger.error(f\"Database error in transaction: {e}\")\n    # The transaction is already rolled back\nexcept CustomApplicationError as e:\n    # Handle application-specific errors\n    logger.error(f\"Application error: {e}\")\n    # The transaction is already rolled back\n</code></pre>"},{"location":"advanced/transactions/#common-transaction-errors","title":"Common Transaction Errors","text":"Error Cause Solution <code>TransactionRollbackError</code> Deadlock or serialization failure Implement retry logic <code>IntegrityError</code> Constraint violation Fix data or adjust constraints <code>OperationalError</code> Connection issues Reconnect and retry <code>ProgrammingError</code> SQL syntax error Fix the query"},{"location":"advanced/transactions/#deadlock-prevention-strategies","title":"Deadlock Prevention Strategies","text":"<ol> <li>Access tables in consistent order across your application</li> <li>Keep transactions short - do only what's necessary</li> <li>Use appropriate isolation levels - don't use SERIALIZABLE unless needed</li> <li>Implement retry logic for deadlock-prone operations</li> <li>Set timeouts to prevent long-running transactions</li> </ol>"},{"location":"advanced/transactions/#performance-considerations","title":"Performance Considerations","text":""},{"location":"advanced/transactions/#transaction-size","title":"Transaction Size","text":"<ul> <li>Small transactions: Better concurrency, less locking</li> <li>Large transactions: Fewer round-trips, but more locking</li> </ul> <p>Best Practice: Find the right balance for your application.</p>"},{"location":"advanced/transactions/#batch-processing-in-transactions","title":"Batch Processing in Transactions","text":"<p>For large data operations, process in batches within transactions:</p> <pre><code>async def process_in_batches(items, batch_size=100):\n    for i in range(0, len(items), batch_size):\n        batch = items[i:i+batch_size]\n        async with db.transaction():\n            for item in batch:\n                await process_item(item)\n</code></pre>"},{"location":"advanced/transactions/#read-only-transactions","title":"Read-Only Transactions","text":"<p>Mark transactions as read-only when possible for better performance:</p> <pre><code>async with db.transaction(read_only=True):\n    # Only read operations here\n    books = await Book.objects().all()\n</code></pre> <p>Note: PostgreSQL and Oracle support explicit read-only transactions; other databases ignore this setting.</p>"},{"location":"advanced/transactions/#real-world-examples","title":"Real-World Examples","text":""},{"location":"advanced/transactions/#e-commerce-order-processing","title":"E-commerce Order Processing","text":"<pre><code>async def process_order(user, cart_items):\n    \"\"\"\n    Process an e-commerce order with proper transaction management\n    \"\"\"\n    async with db.transaction():\n        # Create order\n        order = await Order.create(\n            user=user,\n            status=\"PROCESSING\",\n            total_amount=calculate_total(cart_items)\n        )\n\n        # Process each item\n        for item in cart_items:\n            # Check inventory\n            product = await Product.get(id=item.product_id)\n            if product.inventory &lt; item.quantity:\n                raise InsufficientInventoryError(\n                    f\"Not enough {product.name} in stock\"\n                )\n\n            # Reserve inventory\n            product.inventory -= item.quantity\n            await product.save()\n\n            # Create order item\n            await OrderItem.create(\n                order=order,\n                product=product,\n                quantity=item.quantity,\n                price=product.price\n            )\n\n        # Update order status\n        order.status = \"COMPLETED\"\n        await order.save()\n\n        return order\n</code></pre>"},{"location":"advanced/transactions/#banking-transaction-system","title":"Banking Transaction System","text":"<pre><code>async def transfer_funds(from_account, to_account, amount):\n    \"\"\"\n    Transfer funds between accounts with transaction safety\n    \"\"\"\n    # Validate inputs\n    if amount &lt;= 0:\n        raise ValueError(\"Transfer amount must be positive\")\n\n    async with db.transaction():\n        # Get accounts with row locking to prevent race conditions\n        from_acc = await Account.get(id=from_account, for_update=True)\n        to_acc = await Account.get(id=to_account, for_update=True)\n\n        # Check sufficient funds\n        if from_acc.balance &lt; amount:\n            raise InsufficientFundsError(\n                f\"Account {from_account} has insufficient funds\"\n            )\n\n        # Perform transfer\n        from_acc.balance -= amount\n        to_acc.balance += amount\n\n        # Save changes\n        await from_acc.save()\n        await to_acc.save()\n\n        # Create transaction records\n        await Transaction.create(\n            account_id=from_account,\n            amount=-amount,\n            description=f\"Transfer to {to_account}\"\n        )\n        await Transaction.create(\n            account_id=to_account,\n            amount=amount,\n            description=f\"Transfer from {from_account}\"\n        )\n\n        return {\n            \"from_balance\": from_acc.balance,\n            \"to_balance\": to_acc.balance\n        }\n</code></pre>"},{"location":"advanced/transactions/#testing-transactions","title":"Testing Transactions","text":""},{"location":"advanced/transactions/#unit-testing-transactions","title":"Unit Testing Transactions","text":"<p>When testing transactional code, use in-memory SQLite for speed:</p> <pre><code>import pytest\nfrom unittest.mock import AsyncMock\n\n@pytest.mark.asyncio\nasync def test_order_processing():\n    # Setup test database\n    db = Database(\"sqlite:///:memory:\")\n    db.register_model(Order)\n    db.register_model(OrderItem)\n    await db.create_tables()\n\n    # Test transactional function\n    user = await User.create(name=\"Test User\")\n    cart_items = [{\"product_id\": 1, \"quantity\": 2}]\n\n    # Should complete without errors\n    order = await process_order(user.id, cart_items)\n    assert order.status == \"COMPLETED\"\n\n    # Verify inventory was updated\n    product = await Product.get(id=1)\n    assert product.inventory == INITIAL_INVENTORY - 2\n</code></pre>"},{"location":"advanced/transactions/#simulating-transaction-failures","title":"Simulating Transaction Failures","text":"<p>Test your error handling by simulating failures:</p> <pre><code>@pytest.mark.asyncio\nasync def test_order_processing_insufficient_inventory():\n    # Setup test database\n    db = Database(\"sqlite:///:memory:\")\n    # ... setup models and data ...\n\n    # Create product with limited inventory\n    product = await Product.create(name=\"Test Product\", inventory=1)\n\n    # Attempt to order more than available\n    cart_items = [{\"product_id\": product.id, \"quantity\": 2}]\n\n    # Should raise InsufficientInventoryError\n    with pytest.raises(InsufficientInventoryError):\n        await process_order(user.id, cart_items)\n\n    # Verify no changes were made\n    product = await Product.get(id=product.id)\n    assert product.inventory == 1  # No change\n</code></pre>"},{"location":"advanced/transactions/#best-practices","title":"Best Practices","text":""},{"location":"advanced/transactions/#keep-transactions-short","title":"Keep Transactions Short","text":"<ul> <li>Do as little work as possible within a transaction</li> <li>Move non-database operations outside transactions</li> <li>Process data before entering the transaction</li> </ul>"},{"location":"advanced/transactions/#avoid-user-interaction-in-transactions","title":"Avoid User Interaction in Transactions","text":"<ul> <li>Never wait for user input inside a transaction</li> <li>Complete all business logic before starting a transaction</li> </ul>"},{"location":"advanced/transactions/#use-appropriate-isolation-levels","title":"Use Appropriate Isolation Levels","text":"<ul> <li>Use the lowest isolation level that meets your needs</li> <li>Don't use SERIALIZABLE unless absolutely necessary</li> </ul>"},{"location":"advanced/transactions/#implement-retry-logic","title":"Implement Retry Logic","text":"<ul> <li>For operations prone to deadlocks or serialization failures</li> <li>Use exponential backoff for retries</li> </ul>"},{"location":"advanced/transactions/#monitor-long-running-transactions","title":"Monitor Long-Running Transactions","text":"<ul> <li>Set up monitoring for transactions exceeding expected duration</li> <li>Implement timeouts to prevent runaway transactions</li> </ul>"},{"location":"advanced/transactions/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"advanced/transactions/#stale-data-after-transaction","title":"Stale Data After Transaction","text":"<p>Issue: Reading data that appears unchanged after a transaction</p> <p>Solution: - Verify you're not in a nested transaction that hasn't committed - Check if you're using the correct isolation level - Ensure you're not caching results outside the transaction</p>"},{"location":"advanced/transactions/#deadlocks","title":"Deadlocks","text":"<p>Issue: <code>TransactionRollbackError</code> due to deadlocks</p> <p>Solution: - Access tables in consistent order across your application - Keep transactions as short as possible - Implement retry logic for deadlock-prone operations - Consider using lower isolation levels</p>"},{"location":"advanced/transactions/#transaction-not-rolling-back","title":"Transaction Not Rolling Back","text":"<p>Issue: Changes persist after an exception within a transaction</p> <p>Solution: - Ensure you're using the async context manager correctly - Verify you're not catching and ignoring exceptions - Check if you're using manual transaction control correctly</p>"},{"location":"advanced/transactions/#savepoint-errors-in-nested-transactions","title":"Savepoint Errors in Nested Transactions","text":"<p>Issue: <code>ProgrammingError</code> related to savepoints in nested transactions</p> <p>Solution: - Verify your database supports nested transactions - Check if you're properly exiting inner transaction contexts - Ensure you're not mixing manual and context manager transaction styles</p>"},{"location":"advanced/transactions/#performance-tuning","title":"Performance Tuning","text":""},{"location":"advanced/transactions/#connection-pool-configuration","title":"Connection Pool Configuration","text":"<p>Optimize connection pool settings for transaction-heavy workloads:</p> <pre><code>db = Database(\n    \"postgresql://user:password@localhost:5432/mydb\",\n    min_size=10,      # Increase for high transaction volume\n    max_size=50,      # Higher for more concurrent transactions\n    command_timeout=30  # Adjust based on transaction complexity\n)\n</code></pre>"},{"location":"advanced/transactions/#batch-size-optimization","title":"Batch Size Optimization","text":"<p>Find the optimal batch size for bulk operations:</p> <pre><code># Experiment with different batch sizes\nBATCH_SIZES = [50, 100, 200, 500]\n\nfor batch_size in BATCH_SIZES:\n    start = time.time()\n    async with db.transaction():\n        await process_in_batches(items, batch_size)\n    duration = time.time() - start\n    print(f\"Batch size {batch_size}: {duration:.2f} seconds\")\n</code></pre>"},{"location":"advanced/transactions/#monitoring-long-transactions","title":"Monitoring Long Transactions","text":"<p>Implement monitoring for long-running transactions:</p> <pre><code>from ormax.utils import Timer\n\nasync def monitored_transaction():\n    with Timer(\"Order Processing\"):\n        async with db.transaction():\n            # Transaction operations\n            await process_order()\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>Welcome to the Ormax ORM installation guide! This document will help you set up Ormax in your Python project, connect to your database, and verify that everything is working correctly.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing Ormax, ensure you have:</p> <ul> <li>Python 3.7 or higher</li> <li>A supported database system (SQLite, PostgreSQL, MySQL, MSSQL, Oracle, or Amazon Aurora)</li> <li>Basic knowledge of Python and asynchronous programming</li> </ul>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"<p>Install Ormax using pip:</p> <pre><code>pip install ormax\n</code></pre> <p>This will install the core Ormax library. Depending on which database you plan to use, you'll need additional drivers.</p>"},{"location":"getting-started/installation/#database-drivers","title":"Database Drivers","text":"<p>Ormax supports multiple database backends, but you'll need to install the appropriate driver for your database:</p>"},{"location":"getting-started/installation/#sqlite","title":"SQLite","text":"<pre><code>pip install aiosqlite\n</code></pre> <ul> <li>Best for: Development, testing, and small applications</li> <li>Advantages: No server required, zero configuration</li> <li>Limitations: Not suitable for high-concurrency production environments</li> </ul>"},{"location":"getting-started/installation/#postgresql","title":"PostgreSQL","text":"<pre><code>pip install asyncpg\n</code></pre> <ul> <li>Best for: Production applications requiring robustness and advanced features</li> <li>Advantages: Excellent performance, full ACID compliance, JSONB support</li> <li>Required version: PostgreSQL 9.6 or higher</li> </ul>"},{"location":"getting-started/installation/#mysql","title":"MySQL","text":"<pre><code>pip install aiomysql\n</code></pre> <ul> <li>Best for: Applications already using MySQL</li> <li>Advantages: Good performance, widely used</li> <li>Required version: MySQL 5.7 or higher, or MariaDB 10.2 or higher</li> </ul>"},{"location":"getting-started/installation/#microsoft-sql-server","title":"Microsoft SQL Server","text":"<pre><code>pip install aioodbc\n</code></pre> <ul> <li>Best for: Windows-based enterprise applications</li> <li>Advantages: Integration with Microsoft ecosystem</li> <li>Required version: SQL Server 2016 or higher</li> </ul>"},{"location":"getting-started/installation/#oracle","title":"Oracle","text":"<pre><code>pip install async-oracledb\n</code></pre> <ul> <li>Best for: Enterprise applications using Oracle databases</li> <li>Advantages: Full Oracle feature support</li> <li>Required version: Oracle Database 12c Release 2 or higher</li> </ul>"},{"location":"getting-started/installation/#amazon-aurora","title":"Amazon Aurora","text":"<pre><code>pip install aiomysql\n</code></pre> <ul> <li>Best for: Cloud applications using AWS</li> <li>Advantages: MySQL/PostgreSQL compatibility with cloud scalability</li> <li>Note: Use the same driver as for MySQL or PostgreSQL depending on your Aurora version</li> </ul>"},{"location":"getting-started/installation/#installation-options","title":"Installation Options","text":""},{"location":"getting-started/installation/#install-with-specific-database-support","title":"Install with Specific Database Support","text":"<p>To install Ormax with support for a specific database:</p> <pre><code># For PostgreSQL\npip install \"ormax[postgresql]\"\n\n# For MySQL\npip install \"ormax[mysql]\"\n\n# For SQLite\npip install \"ormax[sqlite]\"\n\n# For all databases\npip install \"ormax[all]\"\n</code></pre>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<p>If you want the latest development version:</p> <pre><code>git clone https://github.com/yourusername/ormax.git\ncd ormax\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#connection-setup","title":"Connection Setup","text":""},{"location":"getting-started/installation/#creating-a-database-connection","title":"Creating a Database Connection","text":"<p>After installing the necessary drivers, create a connection to your database:</p> <pre><code>import ormax\n\n# SQLite (file-based)\ndb = ormax.Database(\"sqlite:///mydatabase.db\")\n\n# PostgreSQL\ndb = ormax.Database(\"postgresql://user:password@localhost:5432/mydb\")\n\n# MySQL\ndb = ormax.Database(\"mysql://user:password@localhost:3306/mydb\")\n\n# Microsoft SQL Server\ndb = ormax.Database(\"mssql://user:password@localhost:1433/mydb\")\n\n# Oracle\ndb = ormax.Database(\"oracle://user:password@localhost:1521/mydb\")\n\n# Amazon Aurora (MySQL-compatible)\ndb = ormax.Database(\"aurora://user:password@cluster-endpoint:3306/mydb\")\n</code></pre>"},{"location":"getting-started/installation/#connection-parameters","title":"Connection Parameters","text":"<p>You can include additional parameters in your connection string:</p> <pre><code># With additional parameters\ndb = ormax.Database(\n    \"postgresql://user:password@localhost:5432/mydb?\"\n    \"sslmode=require&amp;\"\n    \"connect_timeout=10&amp;\"\n    \"application_name=myapp\"\n)\n</code></pre>"},{"location":"getting-started/installation/#async-connection-management","title":"Async Connection Management","text":"<p>Ormax uses async/await for connection management:</p> <pre><code>import asyncio\n\nasync def setup_database():\n    # Connect to database\n    await db.connect()\n\n    # Your database operations here\n\n    # Disconnect when done\n    await db.disconnect()\n\n# Run the setup\nasyncio.run(setup_database())\n</code></pre>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>Let's verify that Ormax is working correctly with a simple test:</p> <pre><code>import asyncio\nimport ormax\nfrom ormax.fields import CharField, IntegerField\n\n# Create database connection\ndb = ormax.Database(\"sqlite:///:memory:\")  # In-memory SQLite for testing\n\n# Define a simple model\nclass Book(ormax.Model):\n    title = CharField(max_length=200)\n    pages = IntegerField()\n\n# Register model with database\ndb.register_model(Book)\n\nasync def test_ormax():\n    # Create tables\n    await db.create_tables()\n\n    # Create a book\n    book = await Book.create(title=\"Test Book\", pages=100)\n\n    # Verify creation\n    assert book.id is not None\n    assert book.title == \"Test Book\"\n    assert book.pages == 100\n\n    # Query the book\n    all_books = await Book.objects().all()\n    assert len(all_books) == 1\n\n    print(\"Ormax installation verified successfully!\")\n\n# Run the test\nasyncio.run(test_ormax())\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-installation-issues","title":"Common Installation Issues","text":""},{"location":"getting-started/installation/#driver-not-found","title":"Driver Not Found","text":"<p>Error: <code>DatabaseError: asyncpg is not installed</code></p> <p>Solution: Install the required driver: <pre><code>pip install asyncpg\n</code></pre></p>"},{"location":"getting-started/installation/#connection-refused","title":"Connection Refused","text":"<p>Error: <code>OperationalError: could not connect to server: Connection refused</code></p> <p>Solution: 1. Verify your database server is running 2. Check your connection string for correct host, port, username, and password 3. Ensure your database accepts connections from your IP address</p>"},{"location":"getting-started/installation/#ssl-connection-issues","title":"SSL Connection Issues","text":"<p>Error: <code>ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed</code></p> <p>Solution: - For development: Add <code>?sslmode=disable</code> to your connection string - For production: Configure proper SSL certificates</p>"},{"location":"getting-started/installation/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"getting-started/installation/#connection-pooling","title":"Connection Pooling","text":"<p>Ormax automatically configures connection pooling, but you can customize it:</p> <pre><code>db = ormax.Database(\n    \"postgresql://user:password@localhost:5432/mydb\",\n    min_size=5,      # Minimum connections in pool\n    max_size=20,     # Maximum connections in pool\n    timeout=30.0     # Connection timeout in seconds\n)\n</code></pre>"},{"location":"getting-started/installation/#custom-connection-parameters","title":"Custom Connection Parameters","text":"<p>For database-specific parameters:</p> <pre><code># PostgreSQL with custom parameters\ndb = ormax.Database(\n    \"postgresql://user:password@localhost:5432/mydb?\"\n    \"application_name=myapp&amp;\"\n    \"statement_timeout=5000\"  # 5 seconds\n)\n\n# MySQL with custom parameters\ndb = ormax.Database(\n    \"mysql://user:password@localhost:3306/mydb?\"\n    \"charset=utf8mb4&amp;\"\n    \"connect_timeout=10\"\n)\n</code></pre>"},{"location":"getting-started/models/","title":"Basic Model Definition","text":"<p>Models are the core of any ORM system. In Ormax, models represent database tables and provide an intuitive Python interface to interact with your data. This guide will show you how to define and work with models in Ormax.</p>"},{"location":"getting-started/models/#defining-your-first-model","title":"Defining Your First Model","text":"<p>Creating a model in Ormax is as simple as subclassing <code>ormax.Model</code> and declaring your fields:</p> <pre><code>import ormax\nfrom ormax.fields import CharField, IntegerField\n\nclass Book(ormax.Model):\n    title = CharField(max_length=200)\n    pages = IntegerField()\n</code></pre> <p>This creates a model with two fields: - <code>title</code>: A string field with maximum length of 200 characters - <code>pages</code>: An integer field for page count</p>"},{"location":"getting-started/models/#registering-models-with-database","title":"Registering Models with Database","text":"<p>Before you can use your model, you need to register it with your database instance:</p> <pre><code># Create database connection\ndb = ormax.Database(\"sqlite:///library.db\")\n\n# Register the model\ndb.register_model(Book)\n</code></pre>"},{"location":"getting-started/models/#creating-database-tables","title":"Creating Database Tables","text":"<p>After registering your models, you need to create the corresponding database tables:</p> <pre><code># Create all registered tables\nawait db.create_tables()\n\n# Or create a specific table\nawait db.create_table(Book)\n</code></pre>"},{"location":"getting-started/models/#field-types","title":"Field Types","text":"<p>Ormax provides a rich set of field types for different data needs. Each field type corresponds to an appropriate database column type.</p>"},{"location":"getting-started/models/#string-fields","title":"String Fields","text":""},{"location":"getting-started/models/#charfield","title":"CharField","text":"<pre><code>name = CharField(max_length=100, min_length=2)\n</code></pre> <ul> <li>Stores strings with a fixed maximum length</li> <li>Automatically optimized for database-specific constraints (e.g., MySQL's 191 character limit for unique indexes)</li> </ul>"},{"location":"getting-started/models/#textfield","title":"TextField","text":"<pre><code>content = TextField(max_length=5000, null=True)\n</code></pre> <ul> <li>For longer text content</li> <li>No maximum length by default (uses database's TEXT type)</li> </ul>"},{"location":"getting-started/models/#emailfield","title":"EmailField","text":"<pre><code>email = EmailField()\n</code></pre> <ul> <li>Validates email format</li> <li>Subclass of CharField with built-in validation</li> </ul>"},{"location":"getting-started/models/#urlfield","title":"URLField","text":"<pre><code>website = URLField()\n</code></pre> <ul> <li>Validates URL format</li> <li>Subclass of CharField with built-in validation</li> </ul>"},{"location":"getting-started/models/#slugfield","title":"SlugField","text":"<pre><code>slug = SlugField(max_length=50)\n</code></pre> <ul> <li>Stores URL-friendly strings</li> <li>Validates format (only letters, numbers, hyphens, and underscores)</li> </ul>"},{"location":"getting-started/models/#numeric-fields","title":"Numeric Fields","text":""},{"location":"getting-started/models/#integerfield","title":"IntegerField","text":"<pre><code>age = IntegerField(min_value=0, max_value=120)\n</code></pre> <ul> <li>Stores integer values</li> <li>Supports min/max validation</li> </ul>"},{"location":"getting-started/models/#bigintegerfield","title":"BigIntegerField","text":"<pre><code>population = BigIntegerField()\n</code></pre> <ul> <li>For very large integers</li> </ul>"},{"location":"getting-started/models/#smallintegerfield","title":"SmallIntegerField","text":"<pre><code>rating = SmallIntegerField(min_value=1, max_value=5)\n</code></pre> <ul> <li>For smaller integer values (16-bit)</li> </ul>"},{"location":"getting-started/models/#floatfield","title":"FloatField","text":"<pre><code>price = FloatField(min_value=0.0)\n</code></pre> <ul> <li>Stores floating-point numbers</li> </ul>"},{"location":"getting-started/models/#decimalfield","title":"DecimalField","text":"<pre><code>price = DecimalField(max_digits=10, decimal_places=2)\n</code></pre> <ul> <li>For precise decimal values</li> <li>Ideal for financial data</li> </ul>"},{"location":"getting-started/models/#date-and-time-fields","title":"Date and Time Fields","text":""},{"location":"getting-started/models/#datetimefield","title":"DateTimeField","text":"<pre><code>created_at = DateTimeField(auto_now_add=True, use_tz=True)\nupdated_at = DateTimeField(auto_now=True)\n</code></pre> <ul> <li>Comprehensive timezone support</li> <li><code>auto_now_add</code>: Set to current time when object is first created</li> <li><code>auto_now</code>: Set to current time on every save</li> <li><code>use_tz</code>: Whether to use timezone-aware datetime objects (default: True)</li> </ul>"},{"location":"getting-started/models/#datefield","title":"DateField","text":"<pre><code>birth_date = DateField()\n</code></pre> <ul> <li>Stores just dates (without time)</li> </ul>"},{"location":"getting-started/models/#timefield","title":"TimeField","text":"<pre><code>start_time = TimeField()\n</code></pre> <ul> <li>Stores just times (without date)</li> </ul>"},{"location":"getting-started/models/#special-fields","title":"Special Fields","text":""},{"location":"getting-started/models/#booleanfield","title":"BooleanField","text":"<pre><code>is_active = BooleanField(default=True)\n</code></pre> <ul> <li>Accepts various formats (True/False, 1/0, \"true\"/\"false\", etc.)</li> </ul>"},{"location":"getting-started/models/#jsonfield","title":"JSONField","text":"<pre><code>metadata = JSONField(null=True)\n</code></pre> <ul> <li>Stores JSON-serializable data</li> <li>Automatically converts between Python objects and JSON</li> </ul>"},{"location":"getting-started/models/#uuidfield","title":"UUIDField","text":"<pre><code>id = UUIDField(primary_key=True)\n</code></pre> <ul> <li>Stores UUID values</li> <li>Automatically converts between string and UUID objects</li> </ul>"},{"location":"getting-started/models/#ipaddressfield","title":"IPAddressField","text":"<pre><code>ip_address = IPAddressField(protocol=\"both\")  # \"ipv4\", \"ipv6\", or \"both\"\n</code></pre> <ul> <li>Validates and stores IP addresses</li> </ul>"},{"location":"getting-started/models/#auto-fields","title":"Auto Fields","text":""},{"location":"getting-started/models/#autofield","title":"AutoField","text":"<pre><code>id = AutoField(primary_key=True)\n</code></pre> <ul> <li>Auto-incrementing integer field</li> <li>Default primary key if none specified</li> </ul>"},{"location":"getting-started/models/#bigautofield","title":"BigAutoField","text":"<pre><code>id = BigAutoField(primary_key=True)\n</code></pre> <ul> <li>Auto-incrementing big integer field</li> </ul>"},{"location":"getting-started/models/#field-options","title":"Field Options","text":"<p>All fields accept these common options:</p> <ul> <li><code>primary_key</code>: Whether this field is the primary key (default: False)</li> <li><code>null</code>: Whether the field can be null (default: True)</li> <li><code>default</code>: Default value for the field</li> <li><code>unique</code>: Whether the field must be unique (default: False)</li> <li><code>index</code>: Whether to create an index for this field (default: False)</li> <li><code>help_text</code>: Documentation for the field</li> <li><code>db_column</code>: Custom column name in the database (default: field name)</li> </ul>"},{"location":"getting-started/models/#example-with-multiple-options","title":"Example with Multiple Options","text":"<pre><code>class Product(ormax.Model):\n    sku = CharField(\n        max_length=50,\n        primary_key=True,\n        help_text=\"Stock Keeping Unit - unique product identifier\"\n    )\n    name = CharField(max_length=200, unique=True)\n    description = TextField(null=True)\n    price = DecimalField(max_digits=10, decimal_places=2, default=0.0)\n    stock = IntegerField(default=0, min_value=0)\n    created_at = DateTimeField(auto_now_add=True)\n    is_active = BooleanField(default=True)\n</code></pre>"},{"location":"getting-started/models/#model-meta-options","title":"Model Meta Options","text":"<p>You can customize model behavior using the <code>_meta</code> attribute:</p> <pre><code>class Book(ormax.Model):\n    title = CharField(max_length=200)\n\n    _meta = {\n        \"table_name\": \"library_books\"  # Custom table name\n    }\n</code></pre>"},{"location":"getting-started/models/#available-meta-options","title":"Available Meta Options","text":"<ul> <li><code>table_name</code>: Custom name for the database table (default: model class name in lowercase)</li> <li><code>indexes</code>: List of custom indexes to create</li> </ul>"},{"location":"getting-started/models/#example-with-multiple-meta-options","title":"Example with Multiple Meta Options","text":"<pre><code>class User(ormax.Model):\n    username = CharField(max_length=50, unique=True)\n    email = CharField(max_length=100, unique=True)\n\n    _meta = {\n        \"table_name\": \"auth_users\",\n        \"indexes\": [\n            {\"fields\": [\"username\", \"email\"], \"unique\": True}\n        ]\n    }\n</code></pre>"},{"location":"getting-started/models/#working-with-models","title":"Working with Models","text":""},{"location":"getting-started/models/#creating-model-instances","title":"Creating Model Instances","text":""},{"location":"getting-started/models/#method-1-using-the-constructor","title":"Method 1: Using the constructor","text":"<pre><code># Create a new book instance\nbook = Book(title=\"\u0634\u06cc\u0631 \u0648 \u062e\u0648\u0631\u0634\u06cc\u062f\", pages=350)\n\n# Save to database\nawait book.save()\n</code></pre>"},{"location":"getting-started/models/#method-2-using-create-saves-automatically","title":"Method 2: Using create() (saves automatically)","text":"<pre><code># Create and save in one step\nbook = await Book.create(title=\"\u0647\u0648\u06cc\u062c\", pages=200)\n</code></pre>"},{"location":"getting-started/models/#accessing-field-values","title":"Accessing Field Values","text":"<pre><code># Get field values\nprint(book.title)  # \"\u0634\u06cc\u0631 \u0648 \u062e\u0648\u0631\u0634\u06cc\u062f\"\nprint(book.pages)  # 350\n\n# Set field values\nbook.pages = 360\nawait book.save()\n</code></pre>"},{"location":"getting-started/models/#deleting-models","title":"Deleting Models","text":"<pre><code># Delete a model instance\nawait book.delete()\n</code></pre>"},{"location":"getting-started/models/#converting-to-dictionary","title":"Converting to Dictionary","text":"<pre><code># Convert model to dictionary\nbook_dict = book.to_dict()\nprint(book_dict)\n# {'title': '\u0634\u06cc\u0631 \u0648 \u062e\u0648\u0631\u0634\u06cc\u062f', 'pages': 350, 'id': 1}\n\n# Create model from dictionary\nnew_book = Book.from_dict(book_dict)\nawait new_book.save()\n</code></pre>"},{"location":"getting-started/models/#advanced-model-features","title":"Advanced Model Features","text":""},{"location":"getting-started/models/#custom-primary-keys","title":"Custom Primary Keys","text":"<p>By default, Ormax adds an auto-incrementing integer primary key named <code>id</code>. You can override this:</p> <pre><code>class Product(ormax.Model):\n    sku = CharField(max_length=50, primary_key=True)\n    name = CharField(max_length=200)\n</code></pre>"},{"location":"getting-started/models/#default-values","title":"Default Values","text":"<pre><code>class BlogPost(ormax.Model):\n    title = CharField(max_length=200)\n    content = TextField()\n    created_at = DateTimeField(default=datetime.now)\n    is_published = BooleanField(default=False)\n</code></pre>"},{"location":"getting-started/models/#validation","title":"Validation","text":"<p>Ormax automatically validates data based on field types and constraints:</p> <pre><code># This will raise a ValidationError\nbook = Book(title=\"Very Long Title That Exceeds The Maximum Length\", pages=350)\nawait book.save()  # ValidationError: Value exceeds maximum length of 200\n</code></pre>"},{"location":"getting-started/models/#model-relationships","title":"Model Relationships","text":"<p>Ormax provides robust support for database relationships. Let's define a simple author-book relationship:</p>"},{"location":"getting-started/models/#foreignkeyfield","title":"ForeignKeyField","text":"<pre><code>from ormax.fields import ForeignKeyField\n\nclass Author(ormax.Model):\n    name = CharField(max_length=100)\n    bio = CharField(max_length=500, null=True)\n\nclass Book(ormax.Model):\n    title = CharField(max_length=200)\n    pages = IntegerField()\n    author = ForeignKeyField(Author, related_name=\"books\")\n</code></pre>"},{"location":"getting-started/models/#working-with-relationships","title":"Working with Relationships","text":""},{"location":"getting-started/models/#forward-relationship","title":"Forward Relationship","text":"<pre><code># Get book's author\nauthor = await book.author\n\n# Set book's author\nbook.author = new_author\nawait book.save()\n</code></pre>"},{"location":"getting-started/models/#reverse-relationship","title":"Reverse Relationship","text":"<pre><code># Get all books by an author\nbooks = await author.books.all()\n\n# Create a new book for the author\nnew_book = await author.books.create(\n    title=\"New Book\", \n    pages=200\n)\n\n# Add existing book to author\nawait author.books.add(existing_book)\n</code></pre>"},{"location":"getting-started/models/#complete-example","title":"Complete Example","text":"<p>Here's a complete example showing model definition and basic usage:</p> <pre><code>import ormax\nfrom ormax.fields import (\n    CharField, \n    IntegerField, \n    DateTimeField,\n    ForeignKeyField\n)\nimport asyncio\n\n# Create database connection\ndb = ormax.Database(\"sqlite:///library.db\")\n\n# Define models\nclass Author(ormax.Model):\n    name = CharField(max_length=100)\n    bio = CharField(max_length=500, null=True)\n    created_at = DateTimeField(auto_now_add=True)\n\nclass Book(ormax.Model):\n    title = CharField(max_length=200)\n    pages = IntegerField(min_value=1)\n    author = ForeignKeyField(Author, related_name=\"books\")\n    published_date = DateTimeField(null=True)\n\n    _meta = {\n        \"table_name\": \"library_books\"\n    }\n\n# Register models\ndb.register_model(Author)\ndb.register_model(Book)\n\nasync def main():\n    # Create tables\n    await db.create_tables()\n\n    # Create authors\n    ahmad = await Author.create(name=\"\u0627\u062d\u0645\u062f \u0645\u062d\u0645\u0648\u062f\u06cc\", bio=\"\u0646\u0648\u06cc\u0633\u0646\u062f\u0647 \u0631\u0645\u0627\u0646\u200c\u0647\u0627\u06cc \u0627\u062c\u062a\u0645\u0627\u0639\u06cc\")\n    saman = await Author.create(name=\"\u0633\u0627\u0645\u0627\u0646 \u0633\u0644\u0637\u0627\u0646\u06cc\", bio=\"\u0646\u0648\u06cc\u0633\u0646\u062f\u0647 \u06a9\u0648\u062f\u06a9 \u0648 \u0646\u0648\u062c\u0648\u0627\u0646\")\n\n    # Create books\n    await Book.create(title=\"\u0634\u06cc\u0631 \u0648 \u062e\u0648\u0631\u0634\u06cc\u062f\", pages=350, author=ahmad, published_date=\"2020-01-15\")\n    await Book.create(title=\"\u0647\u0648\u06cc\u062c\", pages=200, author=saman, published_date=\"2021-03-22\")\n\n    # Query books\n    all_books = await Book.objects().all()\n    print(f\"Total books: {len(all_books)}\")\n\n    # Filter books\n    ahmad_books = await Book.objects().filter(author=ahmad).all()\n    print(f\"Ahmad's books: {len(ahmad_books)}\")\n\n    # Get author's books using reverse relationship\n    ahmad = await Author.objects().get(name=\"\u0627\u062d\u0645\u062f \u0645\u062d\u0645\u0648\u062f\u06cc\")\n    ahmad_books = await ahmad.books.all()\n    print(f\"Ahmad's books via reverse relationship: {len(ahmad_books)}\")\n\n    # Update a book\n    book = ahmad_books[0]\n    book.pages = 360\n    await book.save()\n\n    # Delete a book\n    await ahmad_books[1].delete()\n\n# Run the example\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/models/#best-practices","title":"Best Practices","text":""},{"location":"getting-started/models/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Use singular model names (<code>Book</code> instead of <code>Books</code>)</li> <li>Use lowercase with underscores for field names (<code>first_name</code> not <code>firstName</code>)</li> </ul>"},{"location":"getting-started/models/#field-design","title":"Field Design","text":"<ul> <li>Be explicit about nullability (<code>null=True</code> or <code>null=False</code>)</li> <li>Use appropriate field types for your data</li> <li>Add validation constraints where needed</li> </ul>"},{"location":"getting-started/models/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Use <code>TextField</code> instead of <code>CharField</code> for long text</li> <li>Add indexes to frequently queried fields</li> <li>Consider using <code>defer()</code> or <code>only()</code> for large models</li> </ul>"},{"location":"getting-started/models/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/models/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/models/#field-not-found","title":"Field Not Found","text":"<p>Error: <code>DatabaseError: Field 'xyz' does not exist on model 'MyModel'</code></p> <p>Solution: Check that the field name is correctly spelled and registered in your model.</p>"},{"location":"getting-started/models/#invalid-value","title":"Invalid Value","text":"<p>Error: <code>ValidationError: Value exceeds maximum length of 100</code></p> <p>Solution: Ensure your data meets the field constraints (length, type, etc.).</p>"},{"location":"getting-started/models/#database-table-not-created","title":"Database Table Not Created","text":"<p>Error: <code>OperationalError: no such table: mymodel</code></p> <p>Solution: Make sure you've called <code>await db.create_tables()</code> after registering your models.</p>"},{"location":"querying/queryset/","title":"Querying Data with Ormax","text":"<p>Ormax provides a powerful, chainable QuerySet API for building database queries. This guide will show you how to effectively query your data using Ormax's intuitive syntax.</p>"},{"location":"querying/queryset/#understanding-queryset","title":"Understanding QuerySet","text":"<p>A <code>QuerySet</code> represents a collection of objects from your database. It has two key characteristics:</p> <ol> <li>Lazy evaluation: Queries aren't executed until you actually need the data</li> <li>Chainable: You can keep adding filters and operations to refine your query</li> </ol> <pre><code># This doesn't hit the database yet\nbooks = Book.objects().filter(pages__gt=300).order_by(\"-published_date\")\n\n# This executes the query\nawait books.all()\n</code></pre>"},{"location":"querying/queryset/#basic-query-operations","title":"Basic Query Operations","text":""},{"location":"querying/queryset/#retrieving-all-objects","title":"Retrieving All Objects","text":"<pre><code># Get all books\nall_books = await Book.objects().all()\n\n# Get the first book\nfirst_book = await Book.objects().first()\n\n# Get the last book (by primary key)\nlast_book = await Book.objects().last()\n</code></pre>"},{"location":"querying/queryset/#getting-a-single-object","title":"Getting a Single Object","text":"<pre><code># Get a book by primary key\nbook = await Book.objects().get(id=1)\n\n# Get by other fields\nauthor = await Author.objects().get(name=\"\u0627\u062d\u0645\u062f \u0645\u062d\u0645\u0648\u062f\u06cc\")\n\n# Using get() with filters\nbook = await Book.objects().get(title=\"\u0634\u06cc\u0631 \u0648 \u062e\u0648\u0631\u0634\u06cc\u062f\", pages=350)\n</code></pre> <p>Note: <code>get()</code> will raise <code>DoesNotExist</code> if no object is found, and <code>DatabaseError</code> if multiple objects match.</p>"},{"location":"querying/queryset/#filtering-results","title":"Filtering Results","text":"<pre><code># Exact match\nbooks = await Book.objects().filter(title=\"\u0634\u06cc\u0631 \u0648 \u062e\u0648\u0631\u0634\u06cc\u062f\").all()\n\n# Multiple filters (AND condition)\nbooks = await Book.objects().filter(\n    title=\"\u0634\u06cc\u0631 \u0648 \u062e\u0648\u0631\u0634\u06cc\u062f\", \n    pages=350\n).all()\n\n# Chained filters\nbooks = await Book.objects().filter(pages__gt=300).filter(title__contains=\"sun\").all()\n</code></pre>"},{"location":"querying/queryset/#excluding-results","title":"Excluding Results","text":"<pre><code># Exclude books with less than 200 pages\nbooks = await Book.objects().exclude(pages__lt=200).all()\n\n# Combine filter and exclude\nbooks = await Book.objects().filter(pages__gt=100).exclude(title__startswith=\"A\").all()\n</code></pre>"},{"location":"querying/queryset/#field-lookups","title":"Field Lookups","text":"<p>Ormax supports various field lookups to create more specific queries:</p>"},{"location":"querying/queryset/#basic-lookups","title":"Basic Lookups","text":"Lookup Description Example <code>__exact</code> Exact match (default) <code>title__exact=\"\u0634\u06cc\u0631 \u0648 \u062e\u0648\u0631\u0634\u06cc\u062f\"</code> <code>__iexact</code> Case-insensitive exact match <code>title__iexact=\"shir va khorshid\"</code> <code>__contains</code> Contains the value <code>title__contains=\"sun\"</code> <code>__icontains</code> Case-insensitive contains <code>title__icontains=\"sun\"</code> <code>__gt</code> Greater than <code>pages__gt=300</code> <code>__gte</code> Greater than or equal to <code>pages__gte=300</code> <code>__lt</code> Less than <code>pages__lt=100</code> <code>__lte</code> Less than or equal to <code>pages__lte=100</code> <code>__in</code> In a list of values <code>pages__in=[100, 200, 300]</code> <code>__isnull</code> Is null <code>published_date__isnull=True</code> <pre><code># Books with 100-300 pages\nbooks = await Book.objects().filter(\n    pages__gte=100, \n    pages__lte=300\n).all()\n\n# Books with specific page counts\nbooks = await Book.objects().filter(pages__in=[100, 200, 300]).all()\n\n# Books with titles containing \"sun\" (case-insensitive)\nbooks = await Book.objects().filter(title__icontains=\"sun\").all()\n</code></pre>"},{"location":"querying/queryset/#date-and-time-lookups","title":"Date and Time Lookups","text":"<p>For DateTimeField and DateField:</p> Lookup Description Example <code>__year</code> Match year <code>published_date__year=2020</code> <code>__month</code> Match month <code>published_date__month=5</code> <code>__day</code> Match day <code>published_date__day=15</code> <code>__week_day</code> Match weekday (1=Sunday) <code>published_date__week_day=1</code> <code>__hour</code> Match hour <code>created_at__hour=12</code> <code>__minute</code> Match minute <code>created_at__minute=30</code> <code>__second</code> Match second <code>created_at__second=45</code> <pre><code># Books published in 2020\nbooks = await Book.objects().filter(published_date__year=2020).all()\n\n# Books published in May\nbooks = await Book.objects().filter(published_date__month=5).all()\n</code></pre>"},{"location":"querying/queryset/#ordering-results","title":"Ordering Results","text":""},{"location":"querying/queryset/#basic-ordering","title":"Basic Ordering","text":"<pre><code># Order by title (ascending)\nbooks = await Book.objects().order_by(\"title\").all()\n\n# Order by pages descending\nbooks = await Book.objects().order_by(\"-pages\").all()\n\n# Multiple ordering\nbooks = await Book.objects().order_by(\"-published_date\", \"title\").all()\n</code></pre>"},{"location":"querying/queryset/#using-expressions-in-ordering","title":"Using Expressions in Ordering","text":"<pre><code># Order by calculated field\nbooks = await Book.objects().annotate(\n    page_ratio=\"pages / 10\"\n).order_by(\"page_ratio\").all()\n</code></pre>"},{"location":"querying/queryset/#limiting-and-pagination","title":"Limiting and Pagination","text":""},{"location":"querying/queryset/#limiting-results","title":"Limiting Results","text":"<pre><code># Get first 10 books\nbooks = await Book.objects().limit(10).all()\n\n# Get books 11-20 (for pagination)\nbooks = await Book.objects().limit(10).offset(10).all()\n</code></pre>"},{"location":"querying/queryset/#pagination-helper","title":"Pagination Helper","text":"<p>Ormax includes a utility function for pagination:</p> <pre><code>from ormax.utils import paginate_list\n\n# Get page 2 with 10 items per page\nbooks = await Book.objects().all()\npaginated_books, total_pages, total_items = paginate_list(books, page=2, per_page=10)\n</code></pre>"},{"location":"querying/queryset/#aggregation-and-statistics","title":"Aggregation and Statistics","text":""},{"location":"querying/queryset/#counting-records","title":"Counting Records","text":"<pre><code># Count all books\ntotal = await Book.objects().count()\n\n# Count filtered books\nlong_books = await Book.objects().filter(pages__gt=300).count()\n</code></pre>"},{"location":"querying/queryset/#checking-existence","title":"Checking Existence","text":"<pre><code># Check if any books exist\nexists = await Book.objects().exists()\n\n# Check if specific book exists\nexists = await Book.objects().filter(title=\"\u0634\u06cc\u0631 \u0648 \u062e\u0648\u0631\u0634\u06cc\u062f\").exists()\n</code></pre>"},{"location":"querying/queryset/#advanced-aggregation","title":"Advanced Aggregation","text":"<pre><code>from ormax.query import Aggregation\n\n# Get average number of pages\navg_pages = await Aggregation.avg(Book.objects(), \"pages\")\n\n# Get total pages\ntotal_pages = await Aggregation.sum(Book.objects(), \"pages\")\n\n# Get minimum/maximum\nmin_pages = await Aggregation.min(Book.objects(), \"pages\")\nmax_pages = await Aggregation.max(Book.objects(), \"pages\")\n</code></pre>"},{"location":"querying/queryset/#advanced-query-operations","title":"Advanced Query Operations","text":""},{"location":"querying/queryset/#distinct-results","title":"Distinct Results","text":"<pre><code># Get distinct authors\nauthors = await Book.objects().distinct().values_list(\"author\", flat=True)\n</code></pre>"},{"location":"querying/queryset/#grouping-results","title":"Grouping Results","text":"<pre><code># Group by author and count books\nauthor_counts = await Book.objects().values(\"author\").annotate(count=\"COUNT(*)\").all()\n\n# Group by publication year\nyear_counts = await Book.objects().annotate(\n    year=\"EXTRACT(YEAR FROM published_date)\"\n).values(\"year\").annotate(count=\"COUNT(*)\").all()\n</code></pre>"},{"location":"querying/queryset/#using-only-specific-fields","title":"Using Only Specific Fields","text":"<pre><code># Load only title and pages\nbooks = await Book.objects().only(\"title\", \"pages\").all()\n\n# Accessing other fields will return None\nfor book in books:\n    print(book.title)  # Works\n    print(book.author)  # Returns None\n</code></pre>"},{"location":"querying/queryset/#deferring-fields","title":"Deferring Fields","text":"<pre><code># Load all fields except bio\nauthors = await Author.objects().defer(\"bio\").all()\n\n# Accessing bio will trigger an additional query\nfor author in authors:\n    print(author.name)  # Works\n    print(author.bio)   # Triggers additional query\n</code></pre>"},{"location":"querying/queryset/#using-specific-indexes","title":"Using Specific Indexes","text":"<pre><code># Force using a specific index\nbooks = await Book.objects().using_index(\"title_idx\").filter(title__startswith=\"A\").all()\n</code></pre>"},{"location":"querying/queryset/#locking-records","title":"Locking Records","text":"<pre><code># Lock records for update\nbook = await Book.objects().filter(id=1).for_update().first()\n# This record is now locked until the transaction ends\nbook.pages += 1\nawait book.save()\n</code></pre>"},{"location":"querying/queryset/#query-timeout","title":"Query Timeout","text":"<pre><code># Set a timeout for the query\nbooks = await Book.objects().timeout(5).filter(pages__gt=500).all()\n# Will raise TimeoutError if query takes more than 5 seconds\n</code></pre>"},{"location":"querying/queryset/#raw-sql-queries","title":"Raw SQL Queries","text":"<p>For complex queries that can't be expressed with the QuerySet API:</p>"},{"location":"querying/queryset/#executing-raw-queries","title":"Executing Raw Queries","text":"<pre><code># Execute raw SQL\nresults = await Book.objects().raw(\n    \"SELECT * FROM book WHERE pages &gt; ?\", \n    (300,)\n).execute()\n\n# Get first result\nresult = await Book.objects().raw(\n    \"SELECT * FROM book WHERE id = ?\", \n    (1,)\n).fetch_one()\n</code></pre>"},{"location":"querying/queryset/#async-iteration","title":"Async Iteration","text":"<pre><code># Async iteration over results\nasync for row in Book.objects().raw(\"SELECT * FROM book\"):\n    print(row)\n</code></pre>"},{"location":"querying/queryset/#mapping-results-to-models","title":"Mapping Results to Models","text":"<pre><code># Map raw results to model instances\nbooks = await Book.objects().raw(\n    \"SELECT * FROM book WHERE pages &gt; ?\",\n    (300,)\n).map_to_model()\n</code></pre>"},{"location":"querying/queryset/#query-optimization","title":"Query Optimization","text":""},{"location":"querying/queryset/#select-related-join","title":"Select Related (JOIN)","text":"<p>Use <code>select_related</code> to follow foreign key relationships in a single query:</p> <pre><code># Get books with authors in one query\nbooks = await Book.objects().select_related(\"author\").all()\n\n# Now accessing book.author won't trigger additional queries\nfor book in books:\n    print(book.author.name)  # No additional query\n</code></pre>"},{"location":"querying/queryset/#prefetch-related-separate-queries","title":"Prefetch Related (Separate Queries)","text":"<p>Use <code>prefetch_related</code> to fetch related objects in separate queries:</p> <pre><code># Get authors with their books\nauthors = await Author.objects().prefetch_related(\"books\").all()\n\n# Now accessing author.books.all() won't trigger additional queries per author\nfor author in authors:\n    for book in author.books.all():  # No additional queries\n        print(book.title)\n</code></pre>"},{"location":"querying/queryset/#combining-both-techniques","title":"Combining Both Techniques","text":"<pre><code># Complex query with both select_related and prefetch_related\nbooks = await Book.objects().select_related(\"author\").prefetch_related(\"reviews\").all()\n</code></pre>"},{"location":"querying/queryset/#common-query-patterns","title":"Common Query Patterns","text":""},{"location":"querying/queryset/#getting-the-earliestlatest-records","title":"Getting the Earliest/Latest Records","text":"<pre><code># Get earliest book (by primary key or created_at)\nearliest = await Book.objects().earliest()\n\n# Get latest book\nlatest = await Book.objects().latest()\n\n# Specify field to use\nlatest = await Book.objects().latest(\"published_date\")\n</code></pre>"},{"location":"querying/queryset/#in-bulk-operations","title":"In Bulk Operations","text":"<pre><code># Get multiple objects by ID\nbooks = await Book.objects().in_bulk([1, 2, 3])\n\n# Get by custom field\nauthors = await Author.objects().in_bulk(\n    [\"ahmad\", \"saman\"], \n    field_name=\"username\"\n)\n</code></pre>"},{"location":"querying/queryset/#updating-multiple-records","title":"Updating Multiple Records","text":"<pre><code># Update all books with less than 200 pages\nawait Book.objects().filter(pages__lt=200).update(\n    pages=200\n)\n</code></pre>"},{"location":"querying/queryset/#deleting-multiple-records","title":"Deleting Multiple Records","text":"<pre><code># Delete all books with less than 100 pages\nawait Book.objects().filter(pages__lt=100).delete()\n</code></pre>"},{"location":"querying/queryset/#performance-considerations","title":"Performance Considerations","text":""},{"location":"querying/queryset/#avoiding-n1-query-problem","title":"Avoiding N+1 Query Problem","text":"<p>The N+1 query problem occurs when you trigger a new query for each item in a loop:</p> <pre><code># BAD: N+1 queries\nbooks = await Book.objects().all()\nfor book in books:\n    print(book.author.name)  # Triggers a query for each book\n\n# GOOD: Use select_related\nbooks = await Book.objects().select_related(\"author\").all()\nfor book in books:\n    print(book.author.name)  # No additional queries\n</code></pre>"},{"location":"querying/queryset/#using-values-and-values-list","title":"Using Values and Values List","text":"<p>When you only need specific fields, use <code>values()</code> or <code>values_list()</code>:</p> <pre><code># Get only titles and pages as dictionaries\nbook_data = await Book.objects().values(\"title\", \"pages\").all()\n\n# Get only titles as a flat list\ntitles = await Book.objects().values_list(\"title\", flat=True).all()\n</code></pre>"},{"location":"querying/queryset/#batch-processing","title":"Batch Processing","text":"<p>For large result sets, process in batches:</p> <pre><code>async def process_batch(batch):\n    for book in batch:\n        # Process book\n        await book.update(pages=book.pages + 1)\n\n# Process books in batches of 100\nasync for batch in Book.objects().batch(100):\n    await process_batch(batch)\n</code></pre>"},{"location":"querying/queryset/#complete-examples","title":"Complete Examples","text":""},{"location":"querying/queryset/#book-search-example","title":"Book Search Example","text":"<pre><code>async def search_books(\n    title=None, \n    min_pages=None, \n    max_pages=None, \n    author_name=None,\n    page=1,\n    per_page=10\n):\n    \"\"\"Search books with various filters\"\"\"\n    qs = Book.objects()\n\n    # Apply filters\n    if title:\n        qs = qs.filter(title__icontains=title)\n    if min_pages:\n        qs = qs.filter(pages__gte=min_pages)\n    if max_pages:\n        qs = qs.filter(pages__lte=max_pages)\n    if author_name:\n        qs = qs.filter(author__name__icontains=author_name)\n\n    # Apply ordering\n    qs = qs.order_by(\"-published_date\", \"title\")\n\n    # Apply pagination\n    total = await qs.count()\n    books = await qs.limit(per_page).offset((page-1)*per_page).all()\n\n    # Get authors with prefetch_related to avoid N+1\n    author_ids = list(set(book.author_id for book in books))\n    authors = await Author.objects().filter(id__in=author_ids).all()\n    author_map = {author.id: author for author in authors}\n\n    # Add authors to books\n    for book in books:\n        book.author = author_map.get(book.author_id)\n\n    return {\n        \"books\": books,\n        \"total\": total,\n        \"page\": page,\n        \"pages\": (total + per_page - 1) // per_page\n    }\n</code></pre>"},{"location":"querying/queryset/#dashboard-statistics-example","title":"Dashboard Statistics Example","text":"<pre><code>async def get_dashboard_stats():\n    \"\"\"Get statistics for dashboard\"\"\"\n    # Books stats\n    total_books = await Book.objects().count()\n    average_pages = await Aggregation.avg(Book.objects(), \"pages\")\n\n    # Authors stats\n    total_authors = await Author.objects().count()\n    authors_with_books = await Author.objects().filter(books__isnull=False).count()\n\n    # Books by year\n    books_by_year = await Book.objects().annotate(\n        year=\"EXTRACT(YEAR FROM published_date)\"\n    ).values(\"year\").annotate(count=\"COUNT(*)\").order_by(\"year\").all()\n\n    # Popular authors\n    popular_authors = await Author.objects().annotate(\n        book_count=\"COUNT(books)\"\n    ).order_by(\"-book_count\").limit(5).all()\n\n    return {\n        \"total_books\": total_books,\n        \"average_pages\": average_pages,\n        \"total_authors\": total_authors,\n        \"authors_with_books\": authors_with_books,\n        \"books_by_year\": books_by_year,\n        \"popular_authors\": popular_authors\n    }\n</code></pre>"},{"location":"querying/queryset/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"querying/queryset/#empty-queryset","title":"Empty QuerySet","text":"<p>Issue: <code>await Book.objects().filter(title=\"test\").all()</code> returns empty list</p> <p>Solution: - Check if records actually exist in the database - Verify the filter condition matches your data - Check for case sensitivity (use <code>__icontains</code> for case-insensitive matching)</p>"},{"location":"querying/queryset/#multipleobjectsreturned","title":"MultipleObjectsReturned","text":"<p>Issue: <code>await Book.objects().get(title=\"test\")</code> raises <code>DatabaseError: Multiple objects returned</code></p> <p>Solution: - Use <code>filter()</code> instead of <code>get()</code> when expecting multiple results - Add more specific filters to ensure only one object matches - Use <code>first()</code> if you only need one result: <code>await Book.objects().filter(title=\"test\").first()</code></p>"},{"location":"querying/queryset/#fielddoesnotexist","title":"FieldDoesNotExist","text":"<p>Issue: <code>DatabaseError: Field 'xyz' does not exist on model 'Book'</code></p> <p>Solution: - Check that the field name is correctly spelled - Verify the field is defined in your model - For related fields, ensure the relationship is properly defined</p>"},{"location":"querying/queryset/#performance-problems","title":"Performance Problems","text":"<p>Issue: Queries are slow with large datasets</p> <p>Solution: - Add indexes to frequently queried fields - Use <code>select_related</code> or <code>prefetch_related</code> to avoid N+1 queries - Use <code>only()</code> or <code>defer()</code> to load only necessary fields - Limit the number of results with <code>limit()</code></p>"},{"location":"querying/queryset/#best-practices","title":"Best Practices","text":""},{"location":"querying/queryset/#use-asynchronous-pattern-correctly","title":"Use Asynchronous Pattern Correctly","text":"<pre><code># CORRECT\nbooks = await Book.objects().filter(pages__gt=300).all()\n\n# INCORRECT (will return a coroutine object, not results)\nbooks = Book.objects().filter(pages__gt=300).all()\n</code></pre>"},{"location":"querying/queryset/#chain-queries-for-readability","title":"Chain Queries for Readability","text":"<pre><code># GOOD\nbooks = await Book.objects() \\\n    .filter(pages__gt=300) \\\n    .order_by(\"-published_date\") \\\n    .limit(10) \\\n    .all()\n\n# BAD (harder to read and modify)\nbooks = await Book.objects().filter(pages__gt=300).order_by(\"-published_date\").limit(10).all()\n</code></pre>"},{"location":"querying/queryset/#use-transactions-for-related-operations","title":"Use Transactions for Related Operations","text":"<pre><code>async with db.transaction():\n    # Create book\n    book = await Book.create(title=\"New Book\", pages=200)\n\n    # Update author's book count\n    author = await Author.get(id=book.author_id)\n    author.book_count += 1\n    await author.save()\n</code></pre>"},{"location":"relationships/overview/","title":"Relationships in Ormax","text":"<p>Relationships are a fundamental aspect of database modeling that allow you to connect different data entities. Ormax provides a comprehensive system for defining and working with relationships between models.</p>"},{"location":"relationships/overview/#understanding-relationships","title":"Understanding Relationships","text":"<p>In Ormax, relationships are primarily implemented through <code>ForeignKeyField</code>, which creates a link between two models. There are three main types of relationships:</p> <ol> <li>One-to-Many: One instance of Model A can be related to multiple instances of Model B</li> <li>Many-to-One: Multiple instances of Model A can be related to one instance of Model B</li> <li>Many-to-Many: Multiple instances of Model A can be related to multiple instances of Model B</li> </ol> <p>Note: Currently, Ormax fully supports One-to-Many and Many-to-One relationships through <code>ForeignKeyField</code>. Many-to-Many relationships are planned for future releases.</p>"},{"location":"relationships/overview/#defining-relationships","title":"Defining Relationships","text":""},{"location":"relationships/overview/#foreignkeyfield","title":"ForeignKeyField","text":"<p>The primary way to define relationships in Ormax is using <code>ForeignKeyField</code>:</p> <pre><code>from ormax.fields import ForeignKeyField\n\nclass Author(ormax.Model):\n    name = CharField(max_length=100)\n    bio = CharField(max_length=500, null=True)\n\nclass Book(ormax.Model):\n    title = CharField(max_length=200)\n    pages = IntegerField()\n    author = ForeignKeyField(Author, related_name=\"books\")\n</code></pre> <p>This creates a relationship where: - Each <code>Book</code> has one <code>Author</code> - Each <code>Author</code> can have multiple <code>Book</code> instances (accessed via <code>author.books</code>)</p>"},{"location":"relationships/overview/#foreignkeyfield-options","title":"ForeignKeyField Options","text":"Option Description Default <code>to</code> The related model class or name Required <code>related_name</code> Name for reverse relationship <code>modelname_set</code> <code>on_delete</code> Behavior when referenced object is deleted <code>CASCADE</code> <code>null</code> Whether the field can be null <code>True</code> <code>db_constraint</code> Whether to create database constraint <code>True</code>"},{"location":"relationships/overview/#on_delete-options","title":"on_delete Options","text":"Option Description <code>CASCADE</code> Delete the object when referenced object is deleted <code>SET_NULL</code> Set to NULL when referenced object is deleted (requires <code>null=True</code>) <code>RESTRICT</code> Prevent deletion of referenced object <code>DO_NOTHING</code> Take no action <pre><code># Example with custom options\nclass Book(ormax.Model):\n    title = CharField(max_length=200)\n    author = ForeignKeyField(\n        Author, \n        related_name=\"books\",\n        on_delete=\"SET_NULL\",\n        null=True\n    )\n</code></pre>"},{"location":"relationships/overview/#working-with-relationships","title":"Working with Relationships","text":""},{"location":"relationships/overview/#forward-relationships","title":"Forward Relationships","text":"<p>Forward relationships are accessed directly through the field:</p> <pre><code># Get a book\nbook = await Book.objects().get(id=1)\n\n# Access the author (forward relationship)\nauthor = await book.author  # Returns Author instance or None\n\n# Set a new author\nnew_author = await Author.get(name=\"New Author\")\nbook.author = new_author\nawait book.save()\n</code></pre>"},{"location":"relationships/overview/#reverse-relationships","title":"Reverse Relationships","text":"<p>Reverse relationships are accessed through the related manager:</p> <pre><code># Get an author\nauthor = await Author.objects().get(id=1)\n\n# Get all books by this author (reverse relationship)\nbooks = await author.books.all()\n\n# Create a new book for this author\nnew_book = await author.books.create(\n    title=\"New Book\", \n    pages=200\n)\n\n# Add an existing book to this author\nexisting_book = await Book.get(title=\"Existing Book\")\nawait author.books.add(existing_book)\n\n# Remove a book from this author\nawait author.books.remove(existing_book)\n\n# Clear all books from this author\nawait author.books.clear()\n</code></pre>"},{"location":"relationships/overview/#relationship-managers","title":"Relationship Managers","text":"<p>Ormax provides specialized managers for working with relationships:</p>"},{"location":"relationships/overview/#relatedmanager-forward","title":"RelatedManager (Forward)","text":"<p>Handles forward relationships (the <code>author</code> field in the <code>Book</code> model):</p> <ul> <li><code>get(instance)</code>: Get the related object</li> <li><code>set(instance, related_instance)</code>: Set the related object</li> </ul>"},{"location":"relationships/overview/#reverserelatedmanager-reverse","title":"ReverseRelatedManager (Reverse)","text":"<p>Handles reverse relationships (the <code>books</code> manager in the <code>Author</code> model):</p> <ul> <li><code>all(instance)</code>: Get all related objects</li> <li><code>create(instance, **kwargs)</code>: Create a new related object</li> <li><code>add(instance, *related_instances)</code>: Add existing objects</li> <li><code>remove(instance, *related_instances)</code>: Remove relationships</li> <li><code>clear(instance)</code>: Clear all relationships</li> </ul>"},{"location":"relationships/overview/#advanced-relationship-patterns","title":"Advanced Relationship Patterns","text":""},{"location":"relationships/overview/#self-referential-relationships","title":"Self-Referential Relationships","text":"<p>Models can have relationships to themselves:</p> <pre><code>class Category(ormax.Model):\n    name = CharField(max_length=100)\n    parent = ForeignKeyField(\n        \"Category\", \n        related_name=\"children\",\n        null=True\n    )\n</code></pre>"},{"location":"relationships/overview/#multiple-relationships-to-same-model","title":"Multiple Relationships to Same Model","text":"<p>You can have multiple relationships to the same model by using different <code>related_name</code> values:</p> <pre><code>class Book(ormax.Model):\n    title = CharField(max_length=200)\n    author = ForeignKeyField(Author, related_name=\"written_books\")\n    editor = ForeignKeyField(Author, related_name=\"edited_books\")\n</code></pre>"},{"location":"relationships/overview/#nullable-relationships","title":"Nullable Relationships","text":"<p>When a relationship is optional, set <code>null=True</code>:</p> <pre><code>class Book(ormax.Model):\n    title = CharField(max_length=200)\n    translator = ForeignKeyField(Author, related_name=\"translations\", null=True)\n</code></pre>"},{"location":"relationships/overview/#querying-across-relationships","title":"Querying Across Relationships","text":""},{"location":"relationships/overview/#filtering-by-related-fields","title":"Filtering by Related Fields","text":"<pre><code># Books by authors with \"Ahmad\" in their name\nbooks = await Book.objects().filter(author__name__contains=\"Ahmad\").all()\n\n# Books by a specific author\nbooks = await Book.objects().filter(author__id=1).all()\n</code></pre>"},{"location":"relationships/overview/#selecting-related-objects","title":"Selecting Related Objects","text":"<p>To avoid N+1 query problems, use <code>select_related</code>:</p> <pre><code># Get books with authors in one query\nbooks = await Book.objects().select_related(\"author\").all()\n\n# Now accessing book.author won't trigger additional queries\nfor book in books:\n    print(book.author.name)  # No additional query\n</code></pre>"},{"location":"relationships/overview/#prefetching-related-collections","title":"Prefetching Related Collections","text":"<p>For reverse relationships, use <code>prefetch_related</code>:</p> <pre><code># Get authors with their books\nauthors = await Author.objects().prefetch_related(\"books\").all()\n\n# Now accessing author.books.all() won't trigger additional queries per author\nfor author in authors:\n    for book in author.books.all():  # No additional queries\n        print(book.title)\n</code></pre>"},{"location":"relationships/overview/#combining-both-techniques","title":"Combining Both Techniques","text":"<pre><code># Complex query with both select_related and prefetch_related\nbooks = await Book.objects() \\\n    .select_related(\"author\") \\\n    .prefetch_related(\"reviews\") \\\n    .all()\n</code></pre>"},{"location":"relationships/overview/#relationship-examples","title":"Relationship Examples","text":""},{"location":"relationships/overview/#blog-post-and-author-example","title":"Blog Post and Author Example","text":"<pre><code>class Author(ormax.Model):\n    name = CharField(max_length=100)\n    email = EmailField(unique=True)\n    bio = TextField(null=True)\n\nclass BlogPost(ormax.Model):\n    title = CharField(max_length=200)\n    content = TextField()\n    author = ForeignKeyField(Author, related_name=\"posts\")\n    published = BooleanField(default=False)\n    published_date = DateTimeField(null=True)\n    created_at = DateTimeField(auto_now_add=True)\n\nasync def create_post():\n    # Create author\n    author = await Author.create(\n        name=\"Ahmad Mahmoudi\",\n        email=\"ahmad@example.com\",\n        bio=\"Technical writer and Python enthusiast\"\n    )\n\n    # Create post\n    post = await BlogPost.create(\n        title=\"Introduction to Ormax ORM\",\n        content=\"Ormax is a powerful async ORM for Python...\",\n        author=author\n    )\n\n    # Get author's posts\n    posts = await author.posts.all()\n    print(f\"{author.name} has {len(posts)} posts\")\n\n    # Update post\n    post.published = True\n    post.published_date = datetime.now()\n    await post.save()\n\n    # Get published posts\n    published_posts = await BlogPost.objects().filter(published=True).all()\n    print(f\"There are {len(published_posts)} published posts\")\n</code></pre>"},{"location":"relationships/overview/#e-commerce-product-and-category-example","title":"E-commerce Product and Category Example","text":"<pre><code>class Category(ormax.Model):\n    name = CharField(max_length=100, unique=True)\n    description = TextField(null=True)\n    parent = ForeignKeyField(\"Category\", related_name=\"children\", null=True)\n\nclass Product(ormax.Model):\n    name = CharField(max_length=200)\n    description = TextField(null=True)\n    price = DecimalField(max_digits=10, decimal_places=2)\n    stock = IntegerField(default=0)\n    category = ForeignKeyField(Category, related_name=\"products\")\n    created_at = DateTimeField(auto_now_add=True)\n\nasync def setup_store():\n    # Create categories\n    electronics = await Category.create(name=\"Electronics\")\n    phones = await Category.create(\n        name=\"Mobile Phones\", \n        parent=electronics\n    )\n    laptops = await Category.create(\n        name=\"Laptops\", \n        parent=electronics\n    )\n\n    # Create products\n    await Product.create(\n        name=\"Smartphone X\", \n        price=999.99, \n        stock=50,\n        category=phones\n    )\n    await Product.create(\n        name=\"Laptop Pro\", \n        price=1499.99, \n        stock=30,\n        category=laptops\n    )\n\n    # Get all electronics products\n    electronics_products = await Product.objects().filter(\n        category__in=[phones, laptops]\n    ).all()\n    print(f\"There are {len(electronics_products)} electronics products\")\n\n    # Get category hierarchy\n    root_categories = await Category.objects().filter(parent=None).all()\n    for category in root_categories:\n        print(f\"Category: {category.name}\")\n        children = await category.children.all()\n        for child in children:\n            print(f\"  Subcategory: {child.name}\")\n            products = await child.products.all()\n            for product in products:\n                print(f\"    Product: {product.name} (${product.price})\")\n</code></pre>"},{"location":"relationships/overview/#best-practices-for-relationships","title":"Best Practices for Relationships","text":""},{"location":"relationships/overview/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Use singular names for forward relationships (<code>author</code>, not <code>authors</code>)</li> <li>Use plural names for reverse relationships (<code>books</code>, not <code>book</code>)</li> </ul>"},{"location":"relationships/overview/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Always use <code>select_related</code> for single-valued relationships (ForeignKey)</li> <li>Use <code>prefetch_related</code> for multi-valued relationships (reverse ForeignKey)</li> <li>Be careful with nested prefetching as it can lead to complex queries</li> </ul>"},{"location":"relationships/overview/#database-design","title":"Database Design","text":"<ul> <li>Add indexes to foreign key fields for better performance</li> <li>Consider whether relationships should be nullable</li> <li>Think carefully about <code>on_delete</code> behavior based on your business logic</li> </ul>"},{"location":"relationships/overview/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"relationships/overview/#related-object-not-loading","title":"Related Object Not Loading","text":"<p>Issue: <code>book.author</code> returns a coroutine object instead of the author</p> <p>Solution: Remember to <code>await</code> the relationship: <pre><code>author = await book.author  # Correct\nauthor = book.author        # Incorrect - returns coroutine\n</code></pre></p>"},{"location":"relationships/overview/#reverse-relationship-name-conflict","title":"Reverse Relationship Name Conflict","text":"<p>Issue: <code>AttributeError: 'Author' object has no attribute 'book_set'</code></p> <p>Solution: Specify a custom <code>related_name</code>: <pre><code>class Book(ormax.Model):\n    author = ForeignKeyField(Author, related_name=\"books\")\n</code></pre></p>"},{"location":"relationships/overview/#multipleobjectsreturned-with-reverse-relationships","title":"MultipleObjectsReturned with Reverse Relationships","text":"<p>Issue: <code>await author.books.all()</code> returns more objects than expected</p> <p>Solution: Check your filters and ensure you're not accidentally creating duplicate relationships</p>"},{"location":"relationships/overview/#foreign-key-constraint-violation","title":"Foreign Key Constraint Violation","text":"<p>Issue: <code>IntegrityError: FOREIGN KEY constraint failed</code></p> <p>Solution: - Ensure the related object exists before creating the relationship - Verify you're using the correct primary key value - Check if the relationship should be nullable</p>"}]}